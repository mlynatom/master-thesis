{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare NLI Collection for Instruction Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Templates\n",
    "\n",
    "like in FLAN, 10 instruction templates to introduce diversity -> randomly select one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = {\n",
    "    \"anli\": [\n",
    "        (\"{premise}\\n\\nBased on the paragraph above can we conclude that \\\"{hypothesis}\\\"?\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"{premise}\\n\\nBased on that paragraph can we conclude that this sentence is true?\\n{hypothesis}\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"{premise}\\n\\nCan we draw the following conclusion?\\n{hypothesis}\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"{premise}\\nDoes this next sentence follow, given the preceding text?\\n{hypothesis}\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"{premise}\\nCan we infer the following?\\n{hypothesis}\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Read the following paragraph and determine if the hypothesis is true:\\n\\n{premise}\\n\\nHypothesis: {hypothesis}n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Read the text and determine if the sentence is true:\\n\\n{premise}\\n\\nSentence: {hypothesis}n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Can we draw the following hypothesis from the context? \\n\\nContext:\\n\\n{premise}\\n\\nHypothesis: {hypothesis}n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Determine if the sentence is true based on the text below:\\n{hypothesis}\\n\\n{premise}\\n{options_}\", \"{answer}\"),\n",
    "        (\"Generate a context and a hypothesis.\", \"Context: {premise}\\n\\nHypothesis: {hypothesis}\"),\n",
    "    ],\n",
    "    \"snli\": [\n",
    "        (\"If \\\"{premise}\\\", does this mean that \\\"{hypothesis}\\\"?\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"If \\\"{premise}\\\", can we conclude \\\"{hypothesis}\\\"?\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"If \\\"{premise}\\\", does it logically follow that \\\"{hypothesis}\\\"?\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Based on the sentence \\\"{premise}\\\", is the sentence \\\"{hypothesis}\\\" a true sentence?\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Premise: {premise}\\n\\nHypothesis: {hypothesis}\\n\\n.Can we conclude that the hypothesis is true if the premise is true?\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Premise: {premise}\\n\\nHypothesis: {hypothesis}\\n\\n.Given the premise, can we conclude the hypothesis?\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Here is a premise: \\\"{premise}\\\"\\n\\nHere is a hypothesis: \\\"{hypothesis}\\\"\\n\\n.Does the premise tell us whether the hypothesis is true?\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Is it possible to conclude that \\\"{premise}\\\" if \\\"{hypothesis}\\\"?\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Is the premise \\\"{premise}\\\" true if \\\"{hypothesis}\\\"?\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Write a brief sentence.\", \"{hypothesis}\"),\n",
    "    ],\n",
    "    \"fever\": [\n",
    "        (\"Assess the veracity of the claim based on the provided evidence.\\n\\nClaim: {hypothesis}\\n\\nEvidence: {premise}\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Evaluate the veracity of the claim: \\\"{hypothesis}\\\" based on the evidence: \\\"{premise}\\\".\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"{premise}\\n\\nBased on the evidence paragraph above what can we conclude about the claim \\\"{hypothesis}\\\"?\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"You are given claim {hypothesis} and evidence {premise}. Can you determine if the claim is supported, refuted, or if there is not enough information?\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"{hypothesis}\\n\\n Rate the veracity of the claim based on the evidence in the next paragraph.\\n\\n{premise}\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"What can you conclude about the claim \\\"{premise}\\\" if the evidence is \\\"{hypothesis}\\\"?\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Determine if the claim is supported, refuted, or if there is not enough information based on the evidence.\\n\\nClaim: {premise}\\n\\nEvidence: {hypothesis}\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"{premise}\\n\\nGenerate a claim based on the evidence above which would have veracity {answer}\", \"{hypothesis}\"),\n",
    "        (\"Write evidence for the claim: \\\"{hypothesis}\\\", which would make the veracity be {answer}\", \"{premise}\"),\n",
    "        (\"Create a short claim.\", \"{hypothesis}\"),\n",
    "    ],\n",
    "    \"averitec\": [\n",
    "        (\"Assess the veracity of the claim based on the provided evidence.\\n\\nClaim: {hypothesis}\\n\\nEvidence: {premise}\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Classify the veracity of the claim based on the provided question-answer pairs.\\n\\nClaim: {hypothesis}\\n\\nEvidence: {premise}\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"{premise}\\n\\nBased on the question-answer pairs above what can we conclude about the claim \\\"{hypothesis}\\\"?\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Claim: {hypothesis}\\n\\nEvidence: {premise}\\n\\nWhich of the following options is true?\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Determine the veracity of the claim \\\"{hypothesis}\\\" based on the evidence in form of question-answer pairs \\\"{premise}\\\".\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Based on the provided evidence, which option from the following: {options_} is true?\\n\\nClaim: {hypothesis}\\n\\nEvidence: {premise}\\n\\n\", \"{answer}\"),\n",
    "        (\"Determine the veracity of the claim \\\"{premise}\\\" based on the evidence in form of question-answer pairs \\\"{hypothesis}\\\".\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"{premise}\\n\\nWrite a claim that would have veracity {answer} based on the evidence above.\", \"{hypothesis}\"),\n",
    "        (\"Create evidence in form of questions and answers for the claim: \\\"{hypothesis}\\\", which would make the veracity be {answer}\", \"{premise}\"),\n",
    "        (\"Write a claim to be fact-checked.\", \"{hypothesis}\"),\n",
    "    ],\n",
    "    \"anli_cs\": [\n",
    "        (\"{premise}\\n\\nNa základě odstavce výše, můžeme dospět k závěru, že \\\"{hypothesis}\\\"?\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"{premise}\\n\\nNa základě tohoto odstavce, můžeme dospět k závěru, že tato věta je pravdivá?\\n{hypothesis}\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"{premise}\\n\\nMůžeme vyvodit následující závěr?\\n{hypothesis}\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"{premise}\\nNásleduje tato věta z předešlého textu?\\n{hypothesis}\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"{premise}\\nMůžeme z toho vyvodit následující?\\n{hypothesis}\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Přečtěte si následující odstavec a určete, zda je hypotéza pravdivá:\\n\\n{premise}\\n\\nHypotéza: {hypothesis}n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Přečtěte si text a určete, zda je věta pravdivá:\\n\\n{premise}\\n\\nVěta: {hypothesis}n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Můžeme z tohoto kontextu vyvodit následující hypotézu? \\n\\nKontext:\\n\\n{premise}\\n\\nHypotéza: {hypothesis}n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Určete, zda je věta pravdivá na základě níže uvedeného textu:\\n{hypothesis}\\n\\n{premise}\\n{options_}\", \"{answer}\"),\n",
    "        (\"Vytvořte kontext a hypotézu.\", \"Kontext: {premise}\\n\\nHypotéza: {hypothesis}\"),\n",
    "    ],\n",
    "    \"snli_cs\": [\n",
    "        (\"Pokud \\\"{premise}\\\", znamená to, že \\\"{hypothesis}\\\"?\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Pokud \\\"{premise}\\\", můžeme dospět k závěru \\\"{hypothesis}\\\"?\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Pokud \\\"{premise}\\\", logicky z toho vyplývá, že \\\"{hypothesis}\\\"?\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Na základě věty \\\"{premise}\\\", je věta \\\"{hypothesis}\\\" pravdivá?\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Premisa: {premise}\\n\\nHypotéza: {hypothesis}\\n\\n.Můžeme dospět k závěru, že hypotéza je pravdivá, pokud je premisa pravdivá?\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Premisa: {premise}\\n\\nHypotéza: {hypothesis}\\n\\n.Na základě premisy, můžeme dospět k závěru o hypotéze?\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Zde je premisa: \\\"{premise}\\\"\\n\\nZde je hypotéza: \\\"{hypothesis}\\\"\\n\\n.Ukazuje premisa, zda je hypotéza pravdivá?\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Je možné dospět k závěru, že \\\"{premise}\\\", pokud \\\"{hypothesis}\\\"?\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Je premisa \\\"{premise}\\\" pravdivá, pokud \\\"{hypothesis}\\\"?\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Napiš stručnou větu.\", \"{hypothesis}\"),\n",
    "    ],\n",
    "    \"csfever_nli\": [\n",
    "        (\"Posuď pravdivost tvrzení na základě poskytnutých důkazů.\\n\\nTvrzení: {hypothesis}\\n\\nDůkazy: {premise}\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Zhodnoť pravdivost tvrzení: „{hypothesis}“ na základě důkazů: „{premise}“.\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"{premise}\\n\\nNa základě výše uvedeného odstavce s důkazy, co můžeme vyvodit o tvrzení „{hypothesis}“?\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Je dáno tvrzení {hypothesis} a důkazy {premise}. Dokážeš určit, zda je tvrzení podpořeno, vyvráceno, nebo zda je nedostatek informací?\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"{hypothesis}\\n\\nOhodnoť pravdivost tvrzení na základě důkazů v následujícím odstavci.\\n\\n{premise}\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Co můžeme vyvodit o tvrzení „{premise}“, pokud jsou důkazy „{hypothesis}“?\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Urči, zda je tvrzení podpořeno, vyvráceno, nebo zda není dostatek informací na základě důkazů.\\n\\nTvrzení: {premise}\\n\\nDůkazy: {hypothesis}\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"{premise}\\n\\nVytvořte tvrzení na základě výše uvedených důkazů, které bude mít pravdivost {answer}\", \"{hypothesis}\"),\n",
    "        (\"Napiš evidenci pro tvrzení: „{hypothesis}“, které by způsobily, že pravdivost bude {answer}\", \"{premise}\"),\n",
    "        (\"Vytvořte krátké tvrzení.\", \"{hypothesis}\"),\n",
    "    ],\n",
    "    \"ctkfacts_nli\": [\n",
    "        (\"Ohodnoť pravdivost tvrzení na základě poskytnutých evidencí.\\n\\nTvrzení: {hypothesis}\\n\\nDůkazy: {premise}\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Zhodnoť pravdivost tvrzení: „{hypothesis}“ na základě důkazů: „{premise}“.\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"{premise}\\n\\nNa základě výše uvedeného odstavce s důkazy, co můžeme vyvodit o tvrzení „{hypothesis}“?\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Je dáno tvrzení {hypothesis} a důkazy {premise}. Dokážeš určit, zda je tvrzení podpořeno, vyvráceno, nebo zda není dostatek informací?\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"{hypothesis}\\n\\nVyhodnoť pravdivost tvrzení na základě důkazů v následujícím odstavci.\\n\\n{premise}\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Co můžeme vyvodit o tvrzení „{premise}“, pokud jsou důkazy „{hypothesis}“?\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"Urči, zda je tvrzení podpořeno, vyvráceno, nebo zda není dostatek informací na základě důkazů.\\n\\nTvrzení: {premise}\\n\\nDůkazy: {hypothesis}\\n\\n{options_}\", \"{answer}\"),\n",
    "        (\"{premise}\\n\\nVytvořte tvrzení na základě výše uvedených důkazů, které bude mít pravdivost {answer}\", \"{hypothesis}\"),\n",
    "        (\"Napiš důkazy pro tvrzení: „{hypothesis}“, které by způsobily, že pravdivost bude {answer}\", \"{premise}\"),\n",
    "        (\"Vytvoř krátké tvrzení.\", \"{hypothesis}\"),\n",
    "    ]\n",
    "    \n",
    "}\n",
    "\n",
    "options = {\n",
    "    \"anli\": {0: 'yes', 1: 'it is not possible to tell', 2: 'no'},\n",
    "    \"snli\": {0: 'yes', 1: 'it is not possible to tell', 2: 'no'},\n",
    "    \"fever\": {2: \"supported\", 0: \"refuted\", 1: \"not enough information\"},\n",
    "    \"averitec\": {\"Supported\": \"supported\", \"Refuted\": \"refuted\", 'Not Enough Evidence': \"not enough evidence\", 'Conflicting Evidence/Cherrypicking':\"conflicting Evidence/Cherrypicking\"},\n",
    "    \"anli_cs\": {2: 'ano', 1: 'není možné říci', 0: 'ne'},\n",
    "    \"snli_cs\": {2: 'ano', 1: 'není možné říci', 0: 'ne'},\n",
    "    \"csfever_nli\": {2: \"podporováno\", 0: \"vyvráceno\", 1: \"nedostatek informací\"},\n",
    "    \"ctkfacts_nli\": {2: \"podporováno\", 0: \"vyvráceno\", 1: \"nedostatek informací\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "def process_fever(dataset_name, dataset):\n",
    "    dataset = dataset.map(lambda x: {\"premise\": x[\"evidence\"], \"hypothesis\": x[\"claim\"], \"answer\": options[\"fever\"][x[\"label\"]], \"dataset\": dataset_name, \"translated\": False})\n",
    "\n",
    "    #drop evidence, claim\n",
    "    dataset = dataset.remove_columns([\"evidence\", \"claim\", \"label\"])\n",
    "\n",
    "    return dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"]\n",
    "\n",
    "\n",
    "def process_anli(dataset_name, dataset):\n",
    "    dataset = dataset.rename_column(\"uid\", \"id\")\n",
    "    dataset = dataset.map(lambda x: {\"answer\": options[\"anli\"][x[\"label\"]], \"dataset\": dataset_name, \"translated\": False})\n",
    "\n",
    "    #drop label\n",
    "    dataset = dataset.remove_columns([\"label\", \"reason\"])\n",
    "\n",
    "    train = concatenate_datasets([dataset[\"train_r1\"], dataset[\"train_r2\"], dataset[\"train_r3\"]])\n",
    "    validation = concatenate_datasets([dataset[\"dev_r1\"], dataset[\"dev_r2\"], dataset[\"dev_r3\"]])\n",
    "    test = concatenate_datasets([dataset[\"test_r1\"], dataset[\"test_r2\"], dataset[\"test_r3\"]])\n",
    "\n",
    "    return train, validation, test\n",
    "\n",
    "def process_snli(dataset_name, dataset):\n",
    "    #some problem with labels\n",
    "    dataset = dataset.filter(lambda x: x[\"label\"] in [0, 1, 2])\n",
    "\n",
    "    dataset = dataset.map(lambda x: {\"answer\": options[\"snli\"][x[\"label\"]], \"dataset\": dataset_name, \"translated\": False, \"id\": \"snli\"})\n",
    "\n",
    "    #drop label\n",
    "    dataset = dataset.remove_columns([\"label\"])\n",
    "\n",
    "    return dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"]\n",
    "\n",
    "def process_averitec(dataset_name, dataset):\n",
    "    dataset = dataset.map(lambda x: {\"premise\": x[\"evidence\"], \"hypothesis\": x[\"claim\"], \"answer\": options[\"averitec\"][x[\"label\"]], \"dataset\": dataset_name, \"translated\": False})\n",
    "\n",
    "    #drop label\n",
    "    dataset = dataset.remove_columns([\"label\", \"evidence\", \"claim\"])\n",
    "\n",
    "    return dataset[\"train\"], dataset[\"validation\"], None\n",
    "    \n",
    "def process_cs_anli(dataset_name, dataset):\n",
    "    dataset = dataset.map(lambda x: {\"premise\": x[\"evidence\"], \"hypothesis\": x[\"claim\"], \"answer\": options[\"anli_cs\"][x[\"label\"]], \"dataset\": dataset_name, \"translated\": True})\n",
    "\n",
    "    #drop old columns\n",
    "    dataset = dataset.remove_columns([\"label\", \"evidence\", \"claim\"])\n",
    "\n",
    "    return dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"]\n",
    "\n",
    "# def process_cs_snli(dataset_name, dataset):\n",
    "#     dataset = dataset.map(lambda x: {\"premise\": x[\"cze_premise\"], \"hypothesis\": x[\"cze_hypothesis\"], \"answer\": options[\"snli_cs\"][x[\"label_cze\"]], \"dataset\": dataset_name, \"translated\": True})\n",
    "\n",
    "#     #drop old columns\n",
    "#     dataset = dataset.remove_columns([\"label_cze\", \"cze_premise\", \"cze_hypothesis\", \"gold\", \"eng_premise\", \"eng_hypothesis\", \"label_eng\", \"label_cze1\", \"row_num\"])\n",
    "\n",
    "#     return dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"]\n",
    "\n",
    "def process_csfever_nli(dataset_name, dataset):\n",
    "    dataset = dataset.map(lambda x: {\"premise\": x[\"evidence\"], \"hypothesis\": x[\"claim\"], \"answer\": options[\"csfever_nli\"][x[\"label\"]], \"dataset\": dataset_name, \"translated\": True})\n",
    "\n",
    "    #drop label\n",
    "    dataset = dataset.remove_columns([\"label\", \"evidence\", \"claim\"])\n",
    "\n",
    "    return dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"]\n",
    "\n",
    "def process_ctkfacts_nli(dataset_name, dataset):\n",
    "    dataset = dataset.map(lambda x: {\"premise\": x[\"evidence\"], \"hypothesis\": x[\"claim\"], \"answer\": options[\"ctkfacts_nli\"][x[\"label\"]], \"dataset\": dataset_name, \"translated\": False, \"new_id\": str(x[\"id\"])})\n",
    "\n",
    "    #drop label\n",
    "    dataset = dataset.remove_columns([\"label\", \"evidence\", \"claim\", \"id\"])\n",
    "    dataset = dataset.rename_column(\"new_id\", \"id\")\n",
    "\n",
    "    return dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"]\n",
    "\n",
    "\n",
    "dataset2process = {\n",
    "    \"fever\": process_fever,\n",
    "    \"anli\": process_anli,\n",
    "    \"snli\": process_snli,\n",
    "    \"averitec\": process_averitec,\n",
    "    \"anli_cs\": process_cs_anli,\n",
    "    \"snli_cs\": process_csfever_nli,\n",
    "    \"csfever_nli\": process_csfever_nli,\n",
    "    \"ctkfacts_nli\": process_ctkfacts_nli\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def process_datasets(datasets):\n",
    "    train_splits = {}\n",
    "    dev_splits = {}\n",
    "    test_splits = {}\n",
    "\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        if isinstance(dataset, str):\n",
    "            dataset = load_dataset(dataset)\n",
    "    \n",
    "        train, dev, test = dataset2process[dataset_name](dataset_name, dataset)\n",
    "        train_splits[dataset_name] = train\n",
    "        if dev is not None:\n",
    "            dev_splits[dataset_name] = dev\n",
    "        if test is not None:\n",
    "            test_splits[dataset_name] = test\n",
    "\n",
    "    train_all = concatenate_datasets(list(train_splits.values()))\n",
    "    validation_all = concatenate_datasets(list(dev_splits.values()))\n",
    "    test_all = concatenate_datasets(list(test_splits.values()))\n",
    "\n",
    "    return train_all, validation_all, test_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ftfy\n",
    "\n",
    "#fix text in premise, hypothesis, answer, input and output\n",
    "def fix_text(x):\n",
    "    x[\"premise\"] = ftfy.fix_text(x[\"premise\"])\n",
    "    x[\"hypothesis\"] = ftfy.fix_text(x[\"hypothesis\"])\n",
    "    x[\"answer\"] = ftfy.fix_text(x[\"answer\"])\n",
    "    x[\"input\"] = ftfy.fix_text(x[\"input\"])\n",
    "    x[\"output\"] = ftfy.fix_text(x[\"output\"])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare English Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['claim', 'evidence', 'label'],\n",
       "        num_rows: 3068\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['claim', 'evidence', 'label'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_path = \"/mnt/data/factcheck/averitec-data/data/train_nli_4concat.jsonl\"\n",
    "dev_path = \"/mnt/data/factcheck/averitec-data/data/dev_nli_4concat.jsonl\"\n",
    "averitec = load_dataset(\"json\", data_files={\"train\": train_path, \"validation\": dev_path})\n",
    "averitec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_NLI_DATASETS = {\n",
    "    \"fever\": \"ctu-aic/enfever_nli\",\n",
    "    \"anli\": \"facebook/anli\",\n",
    "    \"snli\": \"stanfordnlp/snli\",\n",
    "    \"averitec\": averitec,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "en_train, en_validation, en_test = process_datasets(EN_NLI_DATASETS)\n",
    "\n",
    "en_dataset = DatasetDict({\"train\": en_train, \"validation\": en_validation, \"test\": en_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7439c813dd46d7b972fab4d8dd4f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/923646 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707e6f1bd007490eab5eef0b3c9cda8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23541 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "530d77e953a04dfa93f1eda1310a2213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23023 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "#create input and output based od templates\n",
    "def in_out_func_map_en(x):\n",
    "\n",
    "    options_str = \"Options: \"\n",
    "    for option in options[x[\"dataset\"]].values():\n",
    "        options_str += \"\\n\\n - \" + option\n",
    "\n",
    "    options[x[\"dataset\"]]\n",
    "    current_template = random.choice(templates[x[\"dataset\"]])\n",
    "    input_ = current_template[0].format(options_=options_str,**x)\n",
    "\n",
    "    output = current_template[1].format(**x)\n",
    "\n",
    "    return {\"input\": input_, \"output\": output}\n",
    "\n",
    "en_dataset = en_dataset.map(in_out_func_map_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae66ebeb10f42dca60c25ba72b301c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/923646 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c175355736f4c8180a0c330f7b87299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23541 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59a3c71ba34441b82d70d2ca6e35b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23023 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "en_dataset = en_dataset.map(fix_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Czech Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CS_NLI_DATASETS = {\n",
    "    \"anli_cs\": \"ctu-aic/anli_cs\",\n",
    "    \"snli_cs\": \"ctu-aic/snli_cs\",\n",
    "    \"csfever_nli\": \"ctu-aic/csfever_nli\",\n",
    "    \"ctkfacts_nli\": \"ctu-aic/ctkfacts_nli\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e309b0557c4dd5be6fc7ba49600662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100459 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5032c4fa344db49d49844e5e8056b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136d5f320cc94322a237cbcf6f31bdfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f51bda100a45f8a41c54e86784bc91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/550152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f051e98b1eb24b8d8f242b3eb764013a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ab8b8b2e8342a69a0fdfda5590a6f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c236b06b3b74c0bb608022644b2ec2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208346 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029cef2db9d441d3acb2166fc2bdc6fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22c15a0c7cb4ce19e9754af94d31fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cfd775b48174daf9f4a645a4ac1e831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3626 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1796fb3b2f8a4bcb854e04c22e4ea857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/482 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c2c13131fc43c7ac1b8fcceeeabf79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/558 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cs_train, cs_validation, cs_test = process_datasets(CS_NLI_DATASETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "cs_dataset = DatasetDict({\"train\": cs_train, \"validation\": cs_validation, \"test\": cs_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34fd7b043ba40cead727944c74f33f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/862583 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1fc6de5fc14692972abb43e9d04020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21681 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eac31086dc19426c8595afe7a1067533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21757 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "#create input and output based od templates\n",
    "def in_out_func_map_cs(x):\n",
    "\n",
    "    options_str = \"Možnosti: \"\n",
    "    for option in options[x[\"dataset\"]].values():\n",
    "        options_str += \"\\n\\n - \" + option\n",
    "\n",
    "    options[x[\"dataset\"]]\n",
    "    current_template = random.choice(templates[x[\"dataset\"]])\n",
    "    input_ = current_template[0].format(options_=options_str,**x)\n",
    "\n",
    "    output = current_template[1].format(**x)\n",
    "\n",
    "    return {\"input\": input_, \"output\": output}\n",
    "\n",
    "cs_dataset = cs_dataset.map(in_out_func_map_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3888ea2ebe5445ba43d512b160dee83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/862583 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358cd86395e14357a53c1f5d94995e80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21681 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d03aa269b1d41f19780eb8966dd5032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21757 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cs_dataset = cs_dataset.map(fix_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IT format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nli_map(example):\n",
    "    input = example[\"input\"]\n",
    "    output = example[\"output\"]\n",
    "    \n",
    "    user_conversation = {\"role\": \"user\", \"content\": input}\n",
    "    assistant_conversation = {\"role\": \"assistant\", \"content\": output}\n",
    "\n",
    "    conversations = [\n",
    "        user_conversation, assistant_conversation\n",
    "    ]\n",
    "\n",
    "    example[\"conversations\"] = conversations\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23ff2828eedb415c92a8dddcbfb63368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/862583 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "188753c7b013463288c28b3ad4851497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21681 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d486d84e254856b0688df3b03aac7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21757 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd73f40fa7d423cb6ef7ac76f88c996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/923646 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "239ece69120e44f19b375e1416b8eef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23541 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b569f274e9da40db8eeb8b6431ed85cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23023 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cs_dataset = cs_dataset.map(nli_map)\n",
    "en_dataset = en_dataset.map(nli_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2093cfb3-a15f-4282-81e3-0cb793ffd0d7',\n",
       " 'premise': 'Tokio 18. prosince (Reuters) - Japonská společnost Shionogi & Co v úterý uvedla, že požádala zdravotní regulační orgány ve Spojených státech, Kanadě a Evropě o schválení svého léku proti HIV Dolutegravir. Shionogi vyvinul Dolutegravir společně s Viiv Healthcare, společným podnikem pro léky proti AIDS mezi společnostmi GlaxoSmithKline a Pfizer, výměnou za práva na tuto drogu.',\n",
       " 'hypothesis': 'Článek byl napsán 18. prosince.',\n",
       " 'answer': 'ano',\n",
       " 'dataset': 'anli_cs',\n",
       " 'translated': True,\n",
       " 'input': 'Tokio 18. prosince (Reuters) - Japonská společnost Shionogi & Co v úterý uvedla, že požádala zdravotní regulační orgány ve Spojených státech, Kanadě a Evropě o schválení svého léku proti HIV Dolutegravir. Shionogi vyvinul Dolutegravir společně s Viiv Healthcare, společným podnikem pro léky proti AIDS mezi společnostmi GlaxoSmithKline a Pfizer, výměnou za práva na tuto drogu.\\n\\nNa základě tohoto odstavce, můžeme dospět k závěru, že tato věta je pravdivá?\\nČlánek byl napsán 18. prosince.\\n\\nMožnosti: \\n\\n - ano\\n\\n - není možné říci\\n\\n - ne',\n",
       " 'output': 'ano',\n",
       " 'conversations': [{'content': 'Tokio 18. prosince (Reuters) - Japonská společnost Shionogi & Co v úterý uvedla, že požádala zdravotní regulační orgány ve Spojených státech, Kanadě a Evropě o schválení svého léku proti HIV Dolutegravir. Shionogi vyvinul Dolutegravir společně s Viiv Healthcare, společným podnikem pro léky proti AIDS mezi společnostmi GlaxoSmithKline a Pfizer, výměnou za práva na tuto drogu.\\n\\nNa základě tohoto odstavce, můžeme dospět k závěru, že tato věta je pravdivá?\\nČlánek byl napsán 18. prosince.\\n\\nMožnosti: \\n\\n - ano\\n\\n - není možné říci\\n\\n - ne',\n",
       "   'role': 'user'},\n",
       "  {'content': 'ano', 'role': 'assistant'}]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '150448',\n",
       " 'premise': 'Roman Atwood. He is best known for his vlogs, where he posts updates about his life on a daily basis. His vlogging channel, \"RomanAtwoodVlogs\", has a total of 3.3 billion views and 11.9 million subscribers. He also has another YouTube channel called \"RomanAtwood\", where he posts pranks.',\n",
       " 'hypothesis': 'Roman Atwood is a content creator.',\n",
       " 'answer': 'supported',\n",
       " 'dataset': 'fever',\n",
       " 'translated': False,\n",
       " 'input': 'Assess the veracity of the claim based on the provided evidence.\\n\\nClaim: Roman Atwood is a content creator.\\n\\nEvidence: Roman Atwood. He is best known for his vlogs, where he posts updates about his life on a daily basis. His vlogging channel, \"RomanAtwoodVlogs\", has a total of 3.3 billion views and 11.9 million subscribers. He also has another YouTube channel called \"RomanAtwood\", where he posts pranks.\\n\\nOptions: \\n\\n - supported\\n\\n - refuted\\n\\n - not enough information',\n",
       " 'output': 'supported',\n",
       " 'conversations': [{'content': 'Assess the veracity of the claim based on the provided evidence.\\n\\nClaim: Roman Atwood is a content creator.\\n\\nEvidence: Roman Atwood. He is best known for his vlogs, where he posts updates about his life on a daily basis. His vlogging channel, \"RomanAtwoodVlogs\", has a total of 3.3 billion views and 11.9 million subscribers. He also has another YouTube channel called \"RomanAtwood\", where he posts pranks.\\n\\nOptions: \\n\\n - supported\\n\\n - refuted\\n\\n - not enough information',\n",
       "   'role': 'user'},\n",
       "  {'content': 'supported', 'role': 'assistant'}]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_dataset[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab88f0623a041a8a8f1f17887a9380b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/862583 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22a79b2cd974b60bfe1ed91d508bf75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/21681 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43296098a8354fd29e23c339f031a7a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/21757 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b85a98a4688c401088e276cbcb5a7898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/923646 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77bda2e7269948dc87ec8130cb3d367b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/23541 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740686c4c5924fb396838197ff212af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/23023 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cs_dataset.save_to_disk(\"/mnt/data/factcheck/nli_it_collection/cs\")\n",
    "en_dataset.save_to_disk(\"/mnt/data/factcheck/nli_it_collection/en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5f67de57f847cfb6dc2c94cb06f9e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39dd37978cb4f73ab0cd5bda1b44759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/432 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ec56ada2e44f1cb8e49b81d58dbe83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/432 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17c5c97c172f47188a1235b372f5df25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26ef029f8d244088728c8c9a28cacc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b255ebbac14312862660c0534982b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70240ef59e04b2b8c1929508ba4a0a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/22 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0217b791871f46ae8124cfd925d4e052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "594dddd87e434aa6badb9320c91bbadc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/462 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1619ad1bc12a4e78814799c55c7f1dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/462 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a785889ec57475f8ef61650a743800f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ddf281a64a4ad7adfe0e0f0cf08a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/24 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8916a7bfe8e448eb8497c206a6f1eaed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ce571c08ad4b2e8a1a7428f1adc5f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/24 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/ctu-aic/nli_it_collection/commit/651bb9ad4c92ed87d69148bb618ca5f3818b04b6', commit_message='Upload dataset', commit_description='', oid='651bb9ad4c92ed87d69148bb618ca5f3818b04b6', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/ctu-aic/nli_it_collection', endpoint='https://huggingface.co', repo_type='dataset', repo_id='ctu-aic/nli_it_collection'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_dataset.push_to_hub(\"ctu-aic/nli_it_collection\", data_dir=\"cs\")\n",
    "en_dataset.push_to_hub(\"ctu-aic/nli_it_collection\", data_dir=\"en\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
