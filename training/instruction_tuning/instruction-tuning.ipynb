{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFTT (Instruction Tuning) using Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install latest version of unsloth\n",
    "#%pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "#%pip install --upgrade unsloth\n",
    "# %pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
    "# %pip install --upgrade unsloth_zoo\n",
    "# %pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from unsloth.chat_templates import get_chat_template, train_on_responses_only\n",
    "\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "\n",
    "# random state\n",
    "SEED = 42\n",
    "\n",
    "# reproducibility\n",
    "## torch\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "## python\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "## numpy\n",
    "import numpy as np\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.1.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.254 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoftQConfig\n",
    "loftq_config = LoftQConfig(loftq_bits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.19 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 256, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 1,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = SEED,\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    # init_lora_weights = \"loftq\",\n",
    "    # loftq_config = loftq_config, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd519a3fce57436fbcc4bb9b81704561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/4.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "782fe4b5a3d24047abc7d2c43c63057d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/152M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "708726cb528e4969a1a767a89b568256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/7.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504385245c1a4be6afba03fdbb824e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/7.76M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74db9425bb7749febbbb03bdf82c2667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/177757 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4810a4ebf9014238b81856355453889f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/7614 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364a503d0051466596ce48749fe4fef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/7641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import DatasetDict, load_dataset\n",
    "dataset_cs = load_dataset(\"ctu-aic/cs_instruction_tuning_collection\")\n",
    "dataset_en = load_dataset(\"ctu-aic/en_instruction_tuning_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103440"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset_cs[\"train\"].num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51720 1709 1754\n"
     ]
    }
   ],
   "source": [
    "train_half = int(len(dataset_cs[\"train\"]) * 0.5)\n",
    "dev_half = int(len(dataset_cs[\"validation\"]) * 0.5)\n",
    "test_half = int(len(dataset_cs[\"test\"]) * 0.5)\n",
    "\n",
    "print(train_half, dev_half, test_half)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103440, 3418, 3508)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 42\n",
    "#shuffle the datasets\n",
    "dataset_cs = dataset_cs.shuffle(seed=SEED)\n",
    "dataset_en = dataset_en.shuffle(seed=SEED)\n",
    "\n",
    "#take the first half of the datasets\n",
    "cs_train = dataset_cs[\"train\"].take(train_half)\n",
    "en_train = dataset_en[\"train\"].take(train_half)\n",
    "cs_dev = dataset_cs[\"validation\"].take(dev_half)\n",
    "en_dev = dataset_en[\"validation\"].take(dev_half)\n",
    "cs_test = dataset_cs[\"test\"].take(test_half)\n",
    "en_test = dataset_en[\"test\"].take(test_half)\n",
    "\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "#combine the datasets\n",
    "dataset = DatasetDict({\n",
    "    \"train\": concatenate_datasets([cs_train, en_train]),\n",
    "    \"validation\": concatenate_datasets([cs_dev, en_dev]),\n",
    "    \"test\": concatenate_datasets([cs_test, en_test]),\n",
    "})\n",
    "dataset = dataset.shuffle(seed=SEED)\n",
    "dataset[\"train\"].num_rows, dataset[\"validation\"].num_rows, dataset[\"test\"].num_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57508c6bae94445888955ed591b32a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/103440 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb067ffbd4f41e5b6a4e5721482e103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3418 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d2c398fc674ea1aa74da3199669a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/3508 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk(\"/mnt/personal/mlynatom/data/it/mix_11_cs_en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "train = concatenate_datasets([dataset_cs[\"train\"], dataset_en[\"train\"]])\n",
    "validation = concatenate_datasets([dataset_cs[\"validation\"], dataset_en[\"validation\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['original_id', 'conversations', 'origin', 'instruction_type', 'instruction_translated', 'output_type', 'output_translated'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['original_id', 'conversations', 'origin', 'instruction_type', 'instruction_translated', 'output_type', 'output_translated'],\n",
       "        num_rows: 11033\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "train = train.shuffle(seed = SEED)\n",
    "validation = validation.shuffle(seed = SEED)\n",
    "\n",
    "train = train.select(range(20000))\n",
    "validation = validation\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\" : train,\n",
    "    \"validation\" : validation,\n",
    "})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def standardize_ask_library(dataset):\n",
    "#     \"\"\"Standardize dataset to hugginface conversations format\"\"\"\n",
    "#     # filter cs language only\n",
    "#     dataset = dataset.filter(lambda x: x[\"language\"] == \"cs\")\n",
    "\n",
    "#     questions = dataset[\"question\"]\n",
    "#     answers = dataset[\"answer\"]\n",
    "\n",
    "\n",
    "#     conversations = []\n",
    "#     for q, a in zip(questions, answers):\n",
    "#         user = {\"role\": \"user\", \"content\": q}\n",
    "#         assistant = {\"role\": \"assistant\", \"content\": a}\n",
    "#         conversations.append([user, assistant])\n",
    "\n",
    "#     dataset = dataset.add_column(\"conversations\", conversations)\n",
    "\n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = standardize_ask_library(dataset)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Vygenerujte si vizualizaci dat, kter√° bude reprezentovat poƒçet absolvent≈Ø za posledn√≠ch 5 let.',\n",
       "  'role': 'user'},\n",
       " {'content': 'Jeliko≈æ nen√≠ uvedeno, o jakou oblast absolvent≈Ø se jedn√°, budeme pracovat s n√°hodnƒõ vygenerovan√Ωmi daty. Pro vizualizaci vyu≈æijeme sloupcov√Ω graf.\\n\\n<img src=\"https://i.imgur.com/1kAfpwT.png\" alt=\"Graf poƒçtu absolvent≈Ø za posledn√≠ch 5 let\">\\n\\nNa grafu je patrn√© zv√Ω≈°en√≠ poƒçtu absolvent≈Ø v roce 2019, pot√© n√°sleduje m√≠rn√Ω pokles. V roce 2022 se poƒçet absolvent≈Ø v√Ωraznƒõ sn√≠≈æil. Vizualizace n√°m umo≈æ≈àuje rychle a p≈ôehlednƒõ porovnat poƒçty absolvent≈Ø v jednotliv√Ωch letech.',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][125][\"conversations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nIdentifikujte potenci√°ln√≠ probl√©m s internetem a pot√© navrhnƒõte ≈ôe≈°en√≠.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nPotenci√°ln√≠m probl√©mem s internetem m≈Ø≈æe b√Ωt pomal√© nebo nestabiln√≠ p≈ôipojen√≠. Navrhnout lze n√°sleduj√≠c√≠ ≈ôe≈°en√≠:\\n\\n1) Zkontrolujte, zda je rychlost va≈°eho internetov√©ho p≈ôipojen√≠ dostateƒçn√° pro po≈æadovan√© aktivity. Mƒõ≈ôte rychlost pomoc√≠ online test≈Ø rychlosti internetu a porovnejte ji s rychlost√≠, kterou va≈°i poskytovatel√© internetu garantuj√≠.\\n\\n2) P≈ôipojte se k internetu p≈ôes kabel m√≠sto Wi-Fi, proto≈æe Wi-Fi m≈Ø≈æe b√Ωt n√°chylnƒõj≈°√≠ k ru≈°en√≠ a zpomalov√°n√≠.\\n\\n3) Zkuste restartovat sv≈Øj modem a smƒõrovaƒç (router), abyste obnovili spojen√≠ a odstranili p≈ô√≠padn√© probl√©my.\\n\\n4) Zkontrolujte, zda v√°s ostatn√≠ za≈ô√≠zen√≠, kter√© pou≈æ√≠vaj√≠ stejn√© Wi-Fi s√≠tƒõ, nerozptyluj√≠ ≈°√≠≈ôku p√°sma. Ujistƒõte se, ≈æe m√°te vyƒçlenƒõnou dostateƒçnou ≈°√≠≈ôku p√°sma pro va≈°e aktivity.\\n\\n5) V√Ωmƒõna modemu a smƒõrovaƒçe za nov√Ω model s lep≈°√≠m v√Ωkonem m≈Ø≈æe pomoci ≈ôe≈°it probl√©my.<|eot_id|>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][100][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the texts to see how long the longest sequence is\n",
    "tokenized = tokenizer(dataset[\"train\"][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(19151), np.float64(363.28375), np.float64(618.5232587671522))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "lengths = [len(ids) for ids in tokenized[\"input_ids\"]]\n",
    "\n",
    "np.max(lengths), np.mean(lengths), np.std(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "090388240464410b9c0dd29c773ab38a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82e0b8c238a41fbb84e40be1ce78fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/11033 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk(\"data/it/init_mix_cs-en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_template = \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "response_template = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "data_collator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template, response_template=response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset[\"train\"],\n",
    "    eval_dataset = dataset[\"validation\"],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    #data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    data_collator = data_collator,\n",
    "    dataset_num_proc = 8,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 128,\n",
    "        gradient_accumulation_steps = 8,\n",
    "        warmup_ratio = 0.01,\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        #max_steps = 60,\n",
    "        learning_rate = 8e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 5,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.001,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = SEED,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "        run_name=\"llama3.1-8B-it-init\",\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = 50,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308d92a5f6354d5ea959a480a89b4770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=128):   0%|          | 0/103440 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504800dba8b545feabb376eab6775235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=128):   0%|          | 0/3419 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nJak se jmenuje hrdina WoD Murph?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nHrdina WoD Murph se jmenuje Michael Patrick \"Murph\" Murphy.<|eot_id|>'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nJak se jmenuje hrdina WoD Murph?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nHrdina WoD Murph se jmenuje Michael Patrick \"Murph\" Murphy.<|eot_id|>'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"input_ids\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nKartavya Path byla naz√Ωv√°na ve jm√©nu<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nzodpovƒõdnosti a povinnosti. Tento term√≠n se obvykle pou≈æ√≠v√° v hinduistick√© filozofii a kultu≈ôe a zd≈Øraz≈àuje d≈Øle≈æitost plnƒõn√≠ sv√Ωch povinnost√≠ a zodpovƒõdnost√≠ v≈Øƒçi rodinƒõ, spoleƒçnosti a celkovƒõ v≈Øƒçi ≈æivotu. Je to filozofie, kter√° n√°s vede k tomu, abychom neuplat≈àovali sv√° pr√°va, ale abychom se zamƒõ≈ôili na to, co mus√≠me udƒõlat pro ostatn√≠ a pro svƒõt jako celek.<|eot_id|>'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                              \\n\\nzodpovƒõdnosti a povinnosti. Tento term√≠n se obvykle pou≈æ√≠v√° v hinduistick√© filozofii a kultu≈ôe a zd≈Øraz≈àuje d≈Øle≈æitost plnƒõn√≠ sv√Ωch povinnost√≠ a zodpovƒõdnost√≠ v≈Øƒçi rodinƒõ, spoleƒçnosti a celkovƒõ v≈Øƒçi ≈æivotu. Je to filozofie, kter√° n√°s vede k tomu, abychom neuplat≈àovali sv√° pr√°va, ale abychom se zamƒõ≈ôili na to, co mus√≠me udƒõlat pro ostatn√≠ a pro svƒõt jako celek.<|eot_id|>'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA A100-SXM4-80GB. Max memory = 79.254 GB.\n",
      "18.502 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 103,440 | Num Epochs = 1 | Total steps = 101\n",
      "O^O/ \\_/ \\    Batch size per device = 128 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (128 x 8 x 1) = 1,024\n",
      " \"-____-\"     Trainable parameters = 671,088,640/8,701,349,888 (7.71% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='101' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3/101 04:35 < 7:30:22, 0.00 it/s, Epoch 0.02/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/unsloth_venv/lib/python3.11/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:315\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4329.0353 seconds used for training.\n",
      "72.15 minutes used for training.\n",
      "Peak reserved memory = 19.932 GB.\n",
      "Peak reserved memory for training = 17.297 GB.\n",
      "Peak reserved memory % of max memory = 50.613 %.\n",
      "Peak reserved memory for training % of max memory = 43.922 %.\n"
     ]
    }
   ],
   "source": [
    "#Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nKdo je nejlep≈°√≠ fotbalista na svƒõtƒõ?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nTato ot√°zka je v≈ædy aktu√°ln√≠ a zp≈Øsobuje mnoho debat. Nejlep≈°√≠ fotbalista na svƒõtƒõ je subjektivn√≠ a m≈Ø≈æe se li≈°it podle osobn√≠ch preferenc√≠, statistik a individu√°ln√≠ch v√Ωkon≈Ø. Nicm√©nƒõ, podle mnoha odborn√≠k≈Ø a fanou≈°k≈Ø, jsou mezi nejlep≈°√≠mi fotbalisty na svƒõtƒõ:\\n\\n1. **Lionel Messi**: Argentinsk√Ω √∫toƒçn√≠k, kter√Ω hraje za Paris Saint-Germain, je ƒçasto pova≈æov√°n za jednoho z nejlep≈°√≠ch fotbalist≈Ø v≈°ech dob. Jeho v√Ωjimeƒçn√© technick√© dovednosti, rychlost, driblov√°n√≠ a st≈ôeleck√© schopnosti ho ƒçin√≠ √∫≈æasn√Ωm hr√°ƒçem.\\n2. **Cristiano Ronaldo**: Portugalsk√Ω √∫toƒçn√≠k, kter√Ω hraje za Al-Nassr, je dal≈°√≠ velk√Ωm jm√©nem ve fotbalov√©m svƒõtƒõ. Jeho neuvƒõ≈ôiteln√° s√≠la, rychlost, st≈ôeleck√© schopnosti a veden√≠ na h≈ôi≈°ti ho ƒçin√≠ jedn√≠m z nejlep≈°√≠ch hr√°ƒç≈Ø souƒçasnosti.\\n3. **Kylian Mbapp√©**: Francouzsk√Ω √∫toƒçn√≠k, kter√Ω hraje za Paris Saint-Germain, je mlad√Ωm a talentovan√Ωm hr√°ƒçem, kter√Ω ji≈æ dos√°hl mnoha √∫spƒõch≈Ø ve fotbalov√©m svƒõtƒõ. Jeho rychlost, technick√© dovednosti a st≈ôeleck√© schopnosti ho ƒçin√≠ jedn√≠m z nej≈æ√°danƒõj≈°√≠ch hr√°ƒç≈Ø souƒçasnosti.\\n\\nSamoz≈ôejmƒõ, ≈æe existuj√≠ i jin√≠ v√Ωborn√≠ fotbalist√©, jako nap≈ô√≠klad **Robert Lewandowski**, **Neymar**, **Mohamed Salah** a dal≈°√≠, kte≈ô√≠ tak√© pat≈ô√≠ mezi nejlep≈°√≠ hr√°ƒçe souƒçasnosti.\\n\\nKter√Ω fotbalista je podle tebe nejlep≈°√≠?<|eot_id|>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Kdo je nejlep≈°√≠ fotbalista na svƒõtƒõ?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 2048, use_cache = True,\n",
    "                         temperature = 1, min_p = 0.1)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nMarie Terezie je<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nDobr√Ω den, Marie Terezie se v ƒçesk√©m jazyce naz√Ωv√° Marie Ter√©zie, je ƒçeskou kr√°lovskou kr√°lovnou. Pozdƒõj√≠ se na dvƒõ ƒç√°sti: Marie - z ≈ôeck√©ho word Marie (kr√°l, prince) a Terezie - z ≈ôeck√©ho word Ter√©sia (kr√°l) a to v dobƒõ, kdy se kr√°lovna pozd√≠vala jako kr√°lovna a pozd√≠vala se jako kr√°lovna. D√°le se podepisovala jako Marie Terezie, kde se pozd√≠vala jako kr√°lovna a']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
