{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFTT (Instruction Tuning) using Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install latest version of unsloth\n",
    "#%pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "#%pip install --upgrade unsloth\n",
    "# %pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
    "# %pip install --upgrade unsloth_zoo\n",
    "# %pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from unsloth.chat_templates import get_chat_template, train_on_responses_only\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "\n",
    "# random state\n",
    "SEED = 42\n",
    "\n",
    "# reproducibility\n",
    "## torch\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "## python\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "## numpy\n",
    "import numpy as np\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.381 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.15 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 256, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 1,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = SEED,\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "dataset_cs = load_dataset(\"ctu-aic/cs_instruction_tuning_collection\")\n",
    "dataset_en = load_dataset(\"ctu-aic/en_instruction_tuning_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_id': 'alpaca-134',\n",
       " 'conversations': [{'content': 'Classify the following data with three labels.\\n\\nfjsklfjdsklfjsklfjsklfjs',\n",
       "   'role': 'user'},\n",
       "  {'content': 'Invalid data. Please provide data in a clear and structured format for classification.',\n",
       "   'role': 'assistant'}],\n",
       " 'origin': 'bactrian-x-alpaca',\n",
       " 'instruction_type': 'gpt-3.5',\n",
       " 'instruction_translated': False,\n",
       " 'output_type': 'gpt-3.5-turbo',\n",
       " 'output_translated': False}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_en[\"train\"][125]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "train = concatenate_datasets([dataset_cs[\"train\"], dataset_en[\"train\"]])\n",
    "validation = concatenate_datasets([dataset_cs[\"validation\"], dataset_en[\"validation\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['original_id', 'conversations', 'origin', 'instruction_type', 'instruction_translated', 'output_type', 'output_translated'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['original_id', 'conversations', 'origin', 'instruction_type', 'instruction_translated', 'output_type', 'output_translated'],\n",
       "        num_rows: 11033\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "train = train.shuffle(seed = SEED)\n",
    "validation = validation.shuffle(seed = SEED)\n",
    "\n",
    "train = train.select(range(20000))\n",
    "validation = validation\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\" : train,\n",
    "    \"validation\" : validation,\n",
    "})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def standardize_ask_library(dataset):\n",
    "#     \"\"\"Standardize dataset to hugginface conversations format\"\"\"\n",
    "#     # filter cs language only\n",
    "#     dataset = dataset.filter(lambda x: x[\"language\"] == \"cs\")\n",
    "\n",
    "#     questions = dataset[\"question\"]\n",
    "#     answers = dataset[\"answer\"]\n",
    "\n",
    "\n",
    "#     conversations = []\n",
    "#     for q, a in zip(questions, answers):\n",
    "#         user = {\"role\": \"user\", \"content\": q}\n",
    "#         assistant = {\"role\": \"assistant\", \"content\": a}\n",
    "#         conversations.append([user, assistant])\n",
    "\n",
    "#     dataset = dataset.add_column(\"conversations\", conversations)\n",
    "\n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841cc82c6c004e2ca1159034a8510736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11033 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#dataset = standardize_ask_library(dataset)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Determine the Sex of a Dwarf Hamster', 'role': 'user'},\n",
       " {'content': \"It is important to know the sex of your hamsters if you have more than one and plan on keeping them in the same cage. You do not want your hamsters to end up breeding, especially if you're not equipped to care for a litter. You can examine your hamster's genitals directly to determine sex. If you can't determine sex this way, other factors such as size and scent glands can help. Be safe when sexing your hamster. You want to avoid accidents or injuries during the process.\\n1. Flip your hamster over carefully. You want to pick your hamster up gently and then turn it on its back. Hamsters, especially young hamster, may resist this process. Go slowly and be gentle.\\n\\nCup your hand and allow your hamster to get into your hand. Once your hamster is seated on the palm of your hand, gently flip it over.\\nYou can clasp your hand gently around the hamster's body and slowly flip it backwards. Without squeezing, hold your hamster securely. Hamsters do not like being flipped over and will likely kick and resist.\\n2. Locate the genitals. A hamster's genitals are located near its tail. You  may have to gently spread apart your hamster's legs to get a clear look. Hamster's have two holes near their tail, one for the anus and another for the genitals.\\n\\nYou may also have to part the fur in order to get a look at your hamster's genitals.\\nIf your hamster is squirming, you may need a friend to assist.\\n3. Check the distance between the two openings. Genitalia can be differentiated by the distance between the two holes. With male hamsters, there will be a clear distance between the anal and genital opening. With female hamsters, the two openings will be side-by-side. They may be so close they appear to be touching.\\n4. Check for nipples. It is not always possible to view a hamster's genitals. If you have your hamster flipped over, thick fur or excessive kicking can prevent you from noticing the genitals. If this is the case, look for nipples. Nipples are usually an indication your hamster is female.\\n\\nRun your finger gently along the underside of your hamster.\\nIf you have a female, you should feel two rows of nipples on the stomach. You can part the hair to see down to your hamster's skin to confirm the presence of nipples.\\nIf you do not find nipples, your hamster may be male. However, young female hamsters may also have less pronounced nipples that are harder to detect.\\n5. Look for testicles. If your hamster will not allow you to flip it over, observe the hamster as it's moving around. If your hamster is male, testicles may be overtly visible at times.\\n\\nIn an older male hamster, testicles will give the rear end a pronounced and pointed appearance. There will also be noticeable swelling near the penile area by the tail.\\nThe rear of a female hamster lacks bulges. It's much smoother in appearance.\\nTesticles may be more difficult to locate in younger hamsters.\\n6. Consider size. As your hamster grows, sex will be easier to determine. Male hamsters are both longer and wider than female hamsters. If your hamster is noticeably bigger than other hamsters in the cage, you may have a male.\\n7. Do not attempt to determine sex before 3 or 4 weeks. You will not be able to tell sex before this age. If your hamster has a litter, wait until the babies are 3 or 4 weeks old to determine sex. Their genitals will not be fully formed before this age, and trying to determine sex before this will only stress them out.\\n8. Do not flip a hamster over it if resists. The best way to determine a hamster's sex is to examine its genitals. However, not all hamsters will allow you to flip them over. If a hamster is squirming, biting, or resisting excessively, do not attempt to flip it over. Find other ways to examine the genitals.\\n\\nTry putting your hamster on a glass surface and looking at its genitals through this surface. You could, for example, place your hamster on a glass table or in a glass ball.\\nIf you drop your hamster, you can injure it. While sex determination is important, it's not worth risking the health of your hamster.\\n9. Separate dwarf hamsters around 4 weeks old. If you want to prevent breeding, you should separate hamsters by sex when they reach 4 weeks old. Have two separate cages ready for the female and male hamsters.\\nMothers and daughters can safely be paired together.\",\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][125][\"conversations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nVytvo≈ôte vstup pro n√°sleduj√≠c√≠ pokyn: napi≈°te p≈ô√≠bƒõh o ≈æenƒõ, kter√° zkoum√° jeskyni\\n\\nMary je zku≈°en√° speleologiƒçka, kter√° je nad≈°en√° z pr≈Øzkumu neobjeven√© jeskynƒõ. Jej√≠ p≈ô√°tel√© ji varovali p≈ôed nebezpeƒç√≠m jeskynƒõ, vƒçetnƒõ mo≈ænosti z≈ô√≠cen√≠, ale ona je odhodl√°na j√≠t d√°l.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nZaƒçala sv≈Øj v√Ωzkum a brzy objevila k≈ôi≈°≈•√°lov√© stƒõny, kter√© se leskly svƒõtlem jej√≠ sv√≠tilny. Jej√≠ srdce se rozbu≈°ilo, jakmile spat≈ôila kouzeln√Ω pohled na tento √∫≈æasn√Ω v√Ωtvor p≈ô√≠rody.\\n\\nMary pokraƒçovala d√°l a se zvƒõdavost√≠ pronikala hloubƒõji do jeskynƒõ. Najednou c√≠tila, ≈æe se j√≠ zved√° ≈æaludek, kdy≈æ se ocitla na √∫plnƒõ nov√©m m√≠stƒõ a uprost≈ôed tmy najednou zahl√©dla mal√© svƒõt√Ωlko.\\n\\nSna≈æila se p≈ôibl√≠≈æit k t√©to z√°≈ôi, ale kdy≈æ se dostala bl√≠≈æe, zjistila, ≈æe je to vr√°sƒçit√° dlanƒõ star√©ho mu≈æe. Mƒõl ≈°ediv√© vlasy a tmavou tv√°≈ô. Mary poc√≠tila v tƒõle strach, ale z√°rove≈à se rozhodla pokraƒçovat v cestƒõ d√°l.\\n\\nMu≈æ k n√≠ promluvil a ≈ôekl: ‚ÄûJsme v jeskyni, kde se mnoho lid√≠ nec√≠t√≠ dob≈ôe a setkali se s koncem. M√°≈° ≈°anci vr√°tit se zpƒõt, ale pokud se rozhodne≈° pokraƒçovat d√°l, bude tƒõ ƒçekat nekoneƒçn√Ω v√Ωzkum a hled√°n√≠.\"\\n\\nMary se rozhodla pokraƒçovat d√°l, nechtƒõla odstoupit ze sv√© cesty. A tak se d√°le pot√°cela jeskyn√≠, kdy≈æ se objevila zar√°≈æen√° p≈ôi pohledu na n√°pisy na rozpadaj√≠c√≠ch se stƒõn√°ch, co≈æ nebyly pouh√© ledy, ale velk√© n√°pisy, kter√© se tv√°≈ôily, ≈æe se rozhodnƒõ nedaj√≠ p≈ôeƒç√≠st.\\n\\nCel√° odv√°≈æn√° Mary se vydala na dlouhou pout, kter√° ji p≈ô√≠mo vedla k pokladu, kde se p≈ôed n√≠ objevil velik√Ω, ohromuj√≠c√≠ diamant! A tak se Mary vr√°tila zpƒõt ze sv√©ho v√Ωzkumu, bohat≈°√≠ a moud≈ôej≈°√≠ d√≠ky sv√©mu skvƒõl√©mu v√Ωkonu.<|eot_id|>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][100][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the texts to see how long the longest sequence is\n",
    "tokenized = tokenizer(dataset[\"train\"][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(19151), np.float64(363.28375), np.float64(618.5232587671522))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "lengths = [len(ids) for ids in tokenized[\"input_ids\"]]\n",
    "\n",
    "np.max(lengths), np.mean(lengths), np.std(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "090388240464410b9c0dd29c773ab38a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82e0b8c238a41fbb84e40be1ce78fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/11033 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk(\"data/it/init_mix_cs-en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlynatom/master-thesis-repository-tomas-mlynar/unsloth_compiled_cache/UnslothSFTTrainer.py:578: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/mlynatom/master-thesis-repository-tomas-mlynar/unsloth_compiled_cache/UnslothSFTTrainer.py:592: UserWarning: You passed a `dataset_num_proc` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/mlynatom/master-thesis-repository-tomas-mlynar/unsloth_compiled_cache/UnslothSFTTrainer.py:606: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19b7f4e62a444c2b682eb0ac4ecceaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd186c8f1f14e968dfa64e02694c4be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/11033 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.other:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset[\"train\"],\n",
    "    eval_dataset = dataset[\"validation\"],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 256,\n",
    "        gradient_accumulation_steps = 2,\n",
    "        warmup_ratio = 0.01,\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        #max_steps = 60,\n",
    "        learning_rate = 1e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 5,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = SEED,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"wandb\", # Use this for WandB etc\n",
    "        run_name=\"llama3.2-1B-it-init\",\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = 50,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f935d1349a014d11a1cb8f6c3ca84048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116d30338655491f80e2d3abff3bb80e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11033 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nKartavya Path byla naz√Ωv√°na ve jm√©nu<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nzodpovƒõdnosti a povinnosti. Tento term√≠n se obvykle pou≈æ√≠v√° v hinduistick√© filozofii a kultu≈ôe a zd≈Øraz≈àuje d≈Øle≈æitost plnƒõn√≠ sv√Ωch povinnost√≠ a zodpovƒõdnost√≠ v≈Øƒçi rodinƒõ, spoleƒçnosti a celkovƒõ v≈Øƒçi ≈æivotu. Je to filozofie, kter√° n√°s vede k tomu, abychom neuplat≈àovali sv√° pr√°va, ale abychom se zamƒõ≈ôili na to, co mus√≠me udƒõlat pro ostatn√≠ a pro svƒõt jako celek.<|eot_id|>'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                              \\n\\nzodpovƒõdnosti a povinnosti. Tento term√≠n se obvykle pou≈æ√≠v√° v hinduistick√© filozofii a kultu≈ôe a zd≈Øraz≈àuje d≈Øle≈æitost plnƒõn√≠ sv√Ωch povinnost√≠ a zodpovƒõdnost√≠ v≈Øƒçi rodinƒõ, spoleƒçnosti a celkovƒõ v≈Øƒçi ≈æivotu. Je to filozofie, kter√° n√°s vede k tomu, abychom neuplat≈àovali sv√° pr√°va, ale abychom se zamƒõ≈ôili na to, co mus√≠me udƒõlat pro ostatn√≠ a pro svƒõt jako celek.<|eot_id|>'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA A100-SXM4-40GB. Max memory = 39.381 GB.\n",
      "1.729 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 20,000 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 256 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 512 | Total steps = 39\n",
      " \"-____-\"     Number of trainable parameters = 180,355,072\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmlynatom\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mlynatom/master-thesis-repository-tomas-mlynar/wandb/run-20250225_104246-tokpnu49</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlynatom/huggingface/runs/tokpnu49' target=\"_blank\">llama3.2-1B-it-init</a></strong> to <a href='https://wandb.ai/mlynatom/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlynatom/huggingface' target=\"_blank\">https://wandb.ai/mlynatom/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlynatom/huggingface/runs/tokpnu49' target=\"_blank\">https://wandb.ai/mlynatom/huggingface/runs/tokpnu49</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='29' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [29/39 11:43 < 04:20, 0.04 it/s, Epoch 0.71/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:385\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4329.0353 seconds used for training.\n",
      "72.15 minutes used for training.\n",
      "Peak reserved memory = 19.932 GB.\n",
      "Peak reserved memory for training = 17.297 GB.\n",
      "Peak reserved memory % of max memory = 50.613 %.\n",
      "Peak reserved memory for training % of max memory = 43.922 %.\n"
     ]
    }
   ],
   "source": [
    "#Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nKdo je nejlep≈°√≠ fotbalista na svƒõtƒõ?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nTato ot√°zka je v≈ædy aktu√°ln√≠ a zp≈Øsobuje mnoho debat. Nejlep≈°√≠ fotbalista na svƒõtƒõ je subjektivn√≠ a m≈Ø≈æe se li≈°it podle osobn√≠ch preferenc√≠, statistik a individu√°ln√≠ch v√Ωkon≈Ø. Nicm√©nƒõ, podle mnoha odborn√≠k≈Ø a fanou≈°k≈Ø, jsou mezi nejlep≈°√≠mi fotbalisty na svƒõtƒõ:\\n\\n1. **Lionel Messi**: Argentinsk√Ω √∫toƒçn√≠k, kter√Ω hraje za Paris Saint-Germain, je ƒçasto pova≈æov√°n za jednoho z nejlep≈°√≠ch fotbalist≈Ø v≈°ech dob. Jeho v√Ωjimeƒçn√© technick√© dovednosti, rychlost, driblov√°n√≠ a st≈ôeleck√© schopnosti ho ƒçin√≠ √∫≈æasn√Ωm hr√°ƒçem.\\n2. **Cristiano Ronaldo**: Portugalsk√Ω √∫toƒçn√≠k, kter√Ω hraje za Al-Nassr, je dal≈°√≠ velk√Ωm jm√©nem ve fotbalov√©m svƒõtƒõ. Jeho neuvƒõ≈ôiteln√° s√≠la, rychlost, st≈ôeleck√© schopnosti a veden√≠ na h≈ôi≈°ti ho ƒçin√≠ jedn√≠m z nejlep≈°√≠ch hr√°ƒç≈Ø souƒçasnosti.\\n3. **Kylian Mbapp√©**: Francouzsk√Ω √∫toƒçn√≠k, kter√Ω hraje za Paris Saint-Germain, je mlad√Ωm a talentovan√Ωm hr√°ƒçem, kter√Ω ji≈æ dos√°hl mnoha √∫spƒõch≈Ø ve fotbalov√©m svƒõtƒõ. Jeho rychlost, technick√© dovednosti a st≈ôeleck√© schopnosti ho ƒçin√≠ jedn√≠m z nej≈æ√°danƒõj≈°√≠ch hr√°ƒç≈Ø souƒçasnosti.\\n\\nSamoz≈ôejmƒõ, ≈æe existuj√≠ i jin√≠ v√Ωborn√≠ fotbalist√©, jako nap≈ô√≠klad **Robert Lewandowski**, **Neymar**, **Mohamed Salah** a dal≈°√≠, kte≈ô√≠ tak√© pat≈ô√≠ mezi nejlep≈°√≠ hr√°ƒçe souƒçasnosti.\\n\\nKter√Ω fotbalista je podle tebe nejlep≈°√≠?<|eot_id|>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Kdo je nejlep≈°√≠ fotbalista na svƒõtƒõ?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 2048, use_cache = True,\n",
    "                         temperature = 1, min_p = 0.1)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nMarie Terezie je<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nDobr√Ω den, Marie Terezie se v ƒçesk√©m jazyce naz√Ωv√° Marie Ter√©zie, je ƒçeskou kr√°lovskou kr√°lovnou. Pozdƒõj√≠ se na dvƒõ ƒç√°sti: Marie - z ≈ôeck√©ho word Marie (kr√°l, prince) a Terezie - z ≈ôeck√©ho word Ter√©sia (kr√°l) a to v dobƒõ, kdy se kr√°lovna pozd√≠vala jako kr√°lovna a pozd√≠vala se jako kr√°lovna. D√°le se podepisovala jako Marie Terezie, kde se pozd√≠vala jako kr√°lovna a']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
