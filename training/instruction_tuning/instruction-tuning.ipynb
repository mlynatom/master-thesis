{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFTT (Instruction Tuning) using Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install latest version of unsloth\n",
    "#%pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "#%pip install --upgrade unsloth\n",
    "# %pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
    "# %pip install --upgrade unsloth_zoo\n",
    "# %pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from unsloth.chat_templates import get_chat_template, train_on_responses_only\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "\n",
    "# random state\n",
    "SEED = 42\n",
    "\n",
    "# reproducibility\n",
    "## torch\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "## python\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "## numpy\n",
    "import numpy as np\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.381 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.15 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 256, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 1,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = SEED,\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "dataset_cs = load_dataset(\"ctu-aic/cs_instruction_tuning_collection\")\n",
    "dataset_en = load_dataset(\"ctu-aic/en_instruction_tuning_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_id': 'alpaca-134',\n",
       " 'conversations': [{'content': 'Classify the following data with three labels.\\n\\nfjsklfjdsklfjsklfjsklfjs',\n",
       "   'role': 'user'},\n",
       "  {'content': 'Invalid data. Please provide data in a clear and structured format for classification.',\n",
       "   'role': 'assistant'}],\n",
       " 'origin': 'bactrian-x-alpaca',\n",
       " 'instruction_type': 'gpt-3.5',\n",
       " 'instruction_translated': False,\n",
       " 'output_type': 'gpt-3.5-turbo',\n",
       " 'output_translated': False}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_en[\"train\"][125]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "train = concatenate_datasets([dataset_cs[\"train\"], dataset_en[\"train\"]])\n",
    "validation = concatenate_datasets([dataset_cs[\"validation\"], dataset_en[\"validation\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['original_id', 'conversations', 'origin', 'instruction_type', 'instruction_translated', 'output_type', 'output_translated'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['original_id', 'conversations', 'origin', 'instruction_type', 'instruction_translated', 'output_type', 'output_translated'],\n",
       "        num_rows: 11033\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "train = train.shuffle(seed = SEED)\n",
    "validation = validation.shuffle(seed = SEED)\n",
    "\n",
    "train = train.select(range(20000))\n",
    "validation = validation\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\" : train,\n",
    "    \"validation\" : validation,\n",
    "})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def standardize_ask_library(dataset):\n",
    "#     \"\"\"Standardize dataset to hugginface conversations format\"\"\"\n",
    "#     # filter cs language only\n",
    "#     dataset = dataset.filter(lambda x: x[\"language\"] == \"cs\")\n",
    "\n",
    "#     questions = dataset[\"question\"]\n",
    "#     answers = dataset[\"answer\"]\n",
    "\n",
    "\n",
    "#     conversations = []\n",
    "#     for q, a in zip(questions, answers):\n",
    "#         user = {\"role\": \"user\", \"content\": q}\n",
    "#         assistant = {\"role\": \"assistant\", \"content\": a}\n",
    "#         conversations.append([user, assistant])\n",
    "\n",
    "#     dataset = dataset.add_column(\"conversations\", conversations)\n",
    "\n",
    "#     return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841cc82c6c004e2ca1159034a8510736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11033 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#dataset = standardize_ask_library(dataset)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Determine the Sex of a Dwarf Hamster', 'role': 'user'},\n",
       " {'content': \"It is important to know the sex of your hamsters if you have more than one and plan on keeping them in the same cage. You do not want your hamsters to end up breeding, especially if you're not equipped to care for a litter. You can examine your hamster's genitals directly to determine sex. If you can't determine sex this way, other factors such as size and scent glands can help. Be safe when sexing your hamster. You want to avoid accidents or injuries during the process.\\n1. Flip your hamster over carefully. You want to pick your hamster up gently and then turn it on its back. Hamsters, especially young hamster, may resist this process. Go slowly and be gentle.\\n\\nCup your hand and allow your hamster to get into your hand. Once your hamster is seated on the palm of your hand, gently flip it over.\\nYou can clasp your hand gently around the hamster's body and slowly flip it backwards. Without squeezing, hold your hamster securely. Hamsters do not like being flipped over and will likely kick and resist.\\n2. Locate the genitals. A hamster's genitals are located near its tail. You  may have to gently spread apart your hamster's legs to get a clear look. Hamster's have two holes near their tail, one for the anus and another for the genitals.\\n\\nYou may also have to part the fur in order to get a look at your hamster's genitals.\\nIf your hamster is squirming, you may need a friend to assist.\\n3. Check the distance between the two openings. Genitalia can be differentiated by the distance between the two holes. With male hamsters, there will be a clear distance between the anal and genital opening. With female hamsters, the two openings will be side-by-side. They may be so close they appear to be touching.\\n4. Check for nipples. It is not always possible to view a hamster's genitals. If you have your hamster flipped over, thick fur or excessive kicking can prevent you from noticing the genitals. If this is the case, look for nipples. Nipples are usually an indication your hamster is female.\\n\\nRun your finger gently along the underside of your hamster.\\nIf you have a female, you should feel two rows of nipples on the stomach. You can part the hair to see down to your hamster's skin to confirm the presence of nipples.\\nIf you do not find nipples, your hamster may be male. However, young female hamsters may also have less pronounced nipples that are harder to detect.\\n5. Look for testicles. If your hamster will not allow you to flip it over, observe the hamster as it's moving around. If your hamster is male, testicles may be overtly visible at times.\\n\\nIn an older male hamster, testicles will give the rear end a pronounced and pointed appearance. There will also be noticeable swelling near the penile area by the tail.\\nThe rear of a female hamster lacks bulges. It's much smoother in appearance.\\nTesticles may be more difficult to locate in younger hamsters.\\n6. Consider size. As your hamster grows, sex will be easier to determine. Male hamsters are both longer and wider than female hamsters. If your hamster is noticeably bigger than other hamsters in the cage, you may have a male.\\n7. Do not attempt to determine sex before 3 or 4 weeks. You will not be able to tell sex before this age. If your hamster has a litter, wait until the babies are 3 or 4 weeks old to determine sex. Their genitals will not be fully formed before this age, and trying to determine sex before this will only stress them out.\\n8. Do not flip a hamster over it if resists. The best way to determine a hamster's sex is to examine its genitals. However, not all hamsters will allow you to flip them over. If a hamster is squirming, biting, or resisting excessively, do not attempt to flip it over. Find other ways to examine the genitals.\\n\\nTry putting your hamster on a glass surface and looking at its genitals through this surface. You could, for example, place your hamster on a glass table or in a glass ball.\\nIf you drop your hamster, you can injure it. While sex determination is important, it's not worth risking the health of your hamster.\\n9. Separate dwarf hamsters around 4 weeks old. If you want to prevent breeding, you should separate hamsters by sex when they reach 4 weeks old. Have two separate cages ready for the female and male hamsters.\\nMothers and daughters can safely be paired together.\",\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][125][\"conversations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nVytvořte vstup pro následující pokyn: napište příběh o ženě, která zkoumá jeskyni\\n\\nMary je zkušená speleologička, která je nadšená z průzkumu neobjevené jeskyně. Její přátelé ji varovali před nebezpečím jeskyně, včetně možnosti zřícení, ale ona je odhodlána jít dál.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nZačala svůj výzkum a brzy objevila křišťálové stěny, které se leskly světlem její svítilny. Její srdce se rozbušilo, jakmile spatřila kouzelný pohled na tento úžasný výtvor přírody.\\n\\nMary pokračovala dál a se zvědavostí pronikala hlouběji do jeskyně. Najednou cítila, že se jí zvedá žaludek, když se ocitla na úplně novém místě a uprostřed tmy najednou zahlédla malé světýlko.\\n\\nSnažila se přiblížit k této záři, ale když se dostala blíže, zjistila, že je to vrásčitá dlaně starého muže. Měl šedivé vlasy a tmavou tvář. Mary pocítila v těle strach, ale zároveň se rozhodla pokračovat v cestě dál.\\n\\nMuž k ní promluvil a řekl: „Jsme v jeskyni, kde se mnoho lidí necítí dobře a setkali se s koncem. Máš šanci vrátit se zpět, ale pokud se rozhodneš pokračovat dál, bude tě čekat nekonečný výzkum a hledání.\"\\n\\nMary se rozhodla pokračovat dál, nechtěla odstoupit ze své cesty. A tak se dále potácela jeskyní, když se objevila zarážená při pohledu na nápisy na rozpadajících se stěnách, což nebyly pouhé ledy, ale velké nápisy, které se tvářily, že se rozhodně nedají přečíst.\\n\\nCelá odvážná Mary se vydala na dlouhou pout, která ji přímo vedla k pokladu, kde se před ní objevil veliký, ohromující diamant! A tak se Mary vrátila zpět ze svého výzkumu, bohatší a moudřejší díky svému skvělému výkonu.<|eot_id|>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][100][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the texts to see how long the longest sequence is\n",
    "tokenized = tokenizer(dataset[\"train\"][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.int64(19151), np.float64(363.28375), np.float64(618.5232587671522))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "lengths = [len(ids) for ids in tokenized[\"input_ids\"]]\n",
    "\n",
    "np.max(lengths), np.mean(lengths), np.std(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "090388240464410b9c0dd29c773ab38a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82e0b8c238a41fbb84e40be1ce78fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/11033 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk(\"data/it/init_mix_cs-en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlynatom/master-thesis-repository-tomas-mlynar/unsloth_compiled_cache/UnslothSFTTrainer.py:578: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/mlynatom/master-thesis-repository-tomas-mlynar/unsloth_compiled_cache/UnslothSFTTrainer.py:592: UserWarning: You passed a `dataset_num_proc` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/mlynatom/master-thesis-repository-tomas-mlynar/unsloth_compiled_cache/UnslothSFTTrainer.py:606: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19b7f4e62a444c2b682eb0ac4ecceaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd186c8f1f14e968dfa64e02694c4be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/11033 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.other:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset[\"train\"],\n",
    "    eval_dataset = dataset[\"validation\"],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 256,\n",
    "        gradient_accumulation_steps = 2,\n",
    "        warmup_ratio = 0.01,\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        #max_steps = 60,\n",
    "        learning_rate = 1e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 5,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = SEED,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"wandb\", # Use this for WandB etc\n",
    "        run_name=\"llama3.2-1B-it-init\",\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = 50,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f935d1349a014d11a1cb8f6c3ca84048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116d30338655491f80e2d3abff3bb80e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11033 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nKartavya Path byla nazývána ve jménu<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nzodpovědnosti a povinnosti. Tento termín se obvykle používá v hinduistické filozofii a kultuře a zdůrazňuje důležitost plnění svých povinností a zodpovědností vůči rodině, společnosti a celkově vůči životu. Je to filozofie, která nás vede k tomu, abychom neuplatňovali svá práva, ale abychom se zaměřili na to, co musíme udělat pro ostatní a pro svět jako celek.<|eot_id|>'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                              \\n\\nzodpovědnosti a povinnosti. Tento termín se obvykle používá v hinduistické filozofii a kultuře a zdůrazňuje důležitost plnění svých povinností a zodpovědností vůči rodině, společnosti a celkově vůči životu. Je to filozofie, která nás vede k tomu, abychom neuplatňovali svá práva, ale abychom se zaměřili na to, co musíme udělat pro ostatní a pro svět jako celek.<|eot_id|>'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA A100-SXM4-40GB. Max memory = 39.381 GB.\n",
      "1.729 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 20,000 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 256 | Gradient Accumulation steps = 2\n",
      "\\        /    Total batch size = 512 | Total steps = 39\n",
      " \"-____-\"     Number of trainable parameters = 180,355,072\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmlynatom\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mlynatom/master-thesis-repository-tomas-mlynar/wandb/run-20250225_104246-tokpnu49</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlynatom/huggingface/runs/tokpnu49' target=\"_blank\">llama3.2-1B-it-init</a></strong> to <a href='https://wandb.ai/mlynatom/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlynatom/huggingface' target=\"_blank\">https://wandb.ai/mlynatom/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlynatom/huggingface/runs/tokpnu49' target=\"_blank\">https://wandb.ai/mlynatom/huggingface/runs/tokpnu49</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='29' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [29/39 11:43 < 04:20, 0.04 it/s, Epoch 0.71/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:385\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4329.0353 seconds used for training.\n",
      "72.15 minutes used for training.\n",
      "Peak reserved memory = 19.932 GB.\n",
      "Peak reserved memory for training = 17.297 GB.\n",
      "Peak reserved memory % of max memory = 50.613 %.\n",
      "Peak reserved memory for training % of max memory = 43.922 %.\n"
     ]
    }
   ],
   "source": [
    "#Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nKdo je nejlepší fotbalista na světě?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nTato otázka je vždy aktuální a způsobuje mnoho debat. Nejlepší fotbalista na světě je subjektivní a může se lišit podle osobních preferencí, statistik a individuálních výkonů. Nicméně, podle mnoha odborníků a fanoušků, jsou mezi nejlepšími fotbalisty na světě:\\n\\n1. **Lionel Messi**: Argentinský útočník, který hraje za Paris Saint-Germain, je často považován za jednoho z nejlepších fotbalistů všech dob. Jeho výjimečné technické dovednosti, rychlost, driblování a střelecké schopnosti ho činí úžasným hráčem.\\n2. **Cristiano Ronaldo**: Portugalský útočník, který hraje za Al-Nassr, je další velkým jménem ve fotbalovém světě. Jeho neuvěřitelná síla, rychlost, střelecké schopnosti a vedení na hřišti ho činí jedním z nejlepších hráčů současnosti.\\n3. **Kylian Mbappé**: Francouzský útočník, který hraje za Paris Saint-Germain, je mladým a talentovaným hráčem, který již dosáhl mnoha úspěchů ve fotbalovém světě. Jeho rychlost, technické dovednosti a střelecké schopnosti ho činí jedním z nejžádanějších hráčů současnosti.\\n\\nSamozřejmě, že existují i jiní výborní fotbalisté, jako například **Robert Lewandowski**, **Neymar**, **Mohamed Salah** a další, kteří také patří mezi nejlepší hráče současnosti.\\n\\nKterý fotbalista je podle tebe nejlepší?<|eot_id|>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Kdo je nejlepší fotbalista na světě?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 2048, use_cache = True,\n",
    "                         temperature = 1, min_p = 0.1)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nMarie Terezie je<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nDobrý den, Marie Terezie se v českém jazyce nazývá Marie Terézie, je českou královskou královnou. Pozdějí se na dvě části: Marie - z řeckého word Marie (král, prince) a Terezie - z řeckého word Terésia (král) a to v době, kdy se královna pozdívala jako královna a pozdívala se jako královna. Dále se podepisovala jako Marie Terezie, kde se pozdívala jako královna a']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
