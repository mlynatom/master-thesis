{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continued pretraining (CP) using Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: unsloth 2025.2.15\n",
      "Uninstalling unsloth-2025.2.15:\n",
      "  Successfully uninstalled unsloth-2025.2.15\n",
      "Collecting git+https://github.com/unslothai/unsloth.git\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /data/temporary/pip-req-build-jljbsugj\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /data/temporary/pip-req-build-jljbsugj\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit 14c9be1d7160162a90ce7a9a6cae36965563a0e6\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: unsloth\n",
      "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unsloth: filename=unsloth-2025.2.15-py3-none-any.whl size=188380 sha256=acacb3407993170d5a87e1498e4d9627227e9d9637b951ceddd68c95d01ab546\n",
      "  Stored in directory: /data/temporary/pip-ephem-wheel-cache-d5nc_pez/wheels/60/3e/1f/e576c07051d90cf64b6a41434d87ccf4db33fafd5343bf5de0\n",
      "Successfully built unsloth\n",
      "Installing collected packages: unsloth\n",
      "Successfully installed unsloth-2025.2.15\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: unsloth_zoo in ./venv/master_venv/lib/python3.12/site-packages (2025.2.7)\n",
      "Requirement already satisfied: torch in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (2.5.1)\n",
      "Requirement already satisfied: triton in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (3.1.0)\n",
      "Requirement already satisfied: packaging in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (24.1)\n",
      "Requirement already satisfied: tyro in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (0.9.2)\n",
      "Requirement already satisfied: transformers>=4.46.1 in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (4.46.3)\n",
      "Requirement already satisfied: datasets>=2.16.0 in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (3.0.1)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (0.2.0)\n",
      "Requirement already satisfied: tqdm in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (4.66.5)\n",
      "Requirement already satisfied: psutil in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (6.1.0)\n",
      "Requirement already satisfied: wheel>=0.42.0 in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (0.45.1)\n",
      "Requirement already satisfied: numpy in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (2.1.2)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (1.0.1)\n",
      "Requirement already satisfied: trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (0.12.1)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (0.13.2)\n",
      "Requirement already satisfied: protobuf<4.0.0 in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (3.20.3)\n",
      "Requirement already satisfied: huggingface_hub in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (0.26.1)\n",
      "Requirement already satisfied: hf_transfer in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (0.1.8)\n",
      "Requirement already satisfied: cut_cross_entropy in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (24.11.4)\n",
      "Requirement already satisfied: pillow in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (11.0.0)\n",
      "Requirement already satisfied: pyyaml in ./venv/master_venv/lib/python3.12/site-packages (from accelerate>=0.34.1->unsloth_zoo) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./venv/master_venv/lib/python3.12/site-packages (from accelerate>=0.34.1->unsloth_zoo) (0.4.5)\n",
      "Requirement already satisfied: filelock in ./venv/master_venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth_zoo) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./venv/master_venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth_zoo) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./venv/master_venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth_zoo) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./venv/master_venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth_zoo) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./venv/master_venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth_zoo) (2.32.3)\n",
      "Requirement already satisfied: xxhash in ./venv/master_venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth_zoo) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in ./venv/master_venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth_zoo) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in ./venv/master_venv/lib/python3.12/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.16.0->unsloth_zoo) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in ./venv/master_venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth_zoo) (3.10.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/master_venv/lib/python3.12/site-packages (from huggingface_hub->unsloth_zoo) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (12.4.127)\n",
      "Requirement already satisfied: setuptools in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (75.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/master_venv/lib/python3.12/site-packages (from sympy==1.13.1->torch->unsloth_zoo) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/master_venv/lib/python3.12/site-packages (from transformers>=4.46.1->unsloth_zoo) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./venv/master_venv/lib/python3.12/site-packages (from transformers>=4.46.1->unsloth_zoo) (0.20.1)\n",
      "Requirement already satisfied: rich in ./venv/master_venv/lib/python3.12/site-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth_zoo) (13.9.4)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in ./venv/master_venv/lib/python3.12/site-packages (from tyro->unsloth_zoo) (0.16)\n",
      "Requirement already satisfied: shtab>=1.5.6 in ./venv/master_venv/lib/python3.12/site-packages (from tyro->unsloth_zoo) (1.7.1)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in ./venv/master_venv/lib/python3.12/site-packages (from tyro->unsloth_zoo) (4.4.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./venv/master_venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/master_venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/master_venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/master_venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/master_venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./venv/master_venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (1.15.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/master_venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth_zoo) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/master_venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth_zoo) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/master_venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth_zoo) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/master_venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth_zoo) (2024.8.30)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/master_venv/lib/python3.12/site-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth_zoo) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/master_venv/lib/python3.12/site-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth_zoo) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/master_venv/lib/python3.12/site-packages (from jinja2->torch->unsloth_zoo) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/master_venv/lib/python3.12/site-packages (from pandas->datasets>=2.16.0->unsloth_zoo) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/master_venv/lib/python3.12/site-packages (from pandas->datasets>=2.16.0->unsloth_zoo) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/master_venv/lib/python3.12/site-packages (from pandas->datasets>=2.16.0->unsloth_zoo) (2024.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venv/master_venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth_zoo) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/master_venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth_zoo) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./venv/master_venv/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.16.0->unsloth_zoo) (0.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: wandb in ./venv/master_venv/lib/python3.12/site-packages (0.19.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in ./venv/master_venv/lib/python3.12/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in ./venv/master_venv/lib/python3.12/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in ./venv/master_venv/lib/python3.12/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in ./venv/master_venv/lib/python3.12/site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in ./venv/master_venv/lib/python3.12/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in ./venv/master_venv/lib/python3.12/site-packages (from wandb) (6.1.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in ./venv/master_venv/lib/python3.12/site-packages (from wandb) (2.10.3)\n",
      "Requirement already satisfied: pyyaml in ./venv/master_venv/lib/python3.12/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in ./venv/master_venv/lib/python3.12/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in ./venv/master_venv/lib/python3.12/site-packages (from wandb) (2.19.2)\n",
      "Requirement already satisfied: setproctitle in ./venv/master_venv/lib/python3.12/site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in ./venv/master_venv/lib/python3.12/site-packages (from wandb) (75.2.0)\n",
      "Requirement already satisfied: six>=1.4.0 in ./venv/master_venv/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./venv/master_venv/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/master_venv/lib/python3.12/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in ./venv/master_venv/lib/python3.12/site-packages (from pydantic<3,>=2.6->wandb) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./venv/master_venv/lib/python3.12/site-packages (from pydantic<3,>=2.6->wandb) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/master_venv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/master_venv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/master_venv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/master_venv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in ./venv/master_venv/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#install latest version of unsloth\n",
    "#%pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "#%pip install --upgrade unsloth\n",
    "%pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
    "%pip install --upgrade unsloth_zoo\n",
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "\n",
    "from unsloth import UnslothTrainingArguments, UnslothTrainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "\n",
    "# random state\n",
    "SEED = 42\n",
    "\n",
    "# reproducibility\n",
    "## torch\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "## python\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "## numpy\n",
    "import numpy as np\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.381 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 4096 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading input_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlynatom/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/unsloth/models/_utils.py:760: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  offloaded_W = torch.load(filename, map_location = \"cpu\", mmap = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading output_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.15 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Training embed_tokens in mixed precision to save VRAM\n",
      "Unsloth: Training lm_head in mixed precision to save VRAM\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 256, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "                      \"embed_tokens\", \"lm_head\"], #now also embeddings and lm_head\n",
    "    lora_alpha = 512,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = SEED,\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    # init_lora_weights = \"loftq\",\n",
    "    # loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504d15b0d5214af4a1f9ad7cdbf33e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa04de176404b85a2373c1b0643cf97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/473 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 26631\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 62703458\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# this dataset has already fixed encoding using ftfy (as is used by me in the preprocessing steps of other datasets)\n",
    "dataset = load_dataset(\"HuggingFaceFW/fineweb-2\", \"ces_Latn\")\n",
    "#we need only texts\n",
    "dataset = dataset.remove_columns([\"id\", \"dump\", \"url\", \"date\", \"file_path\", \"language\", \"language_score\", \"language_script\", \"minhash_cluster_size\", \"top_langs\"])\n",
    "#shuffle to be sure we select \"random sample\"\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"] = dataset[\"train\"].select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return {\"text\": [example + tokenizer.eos_token for example in examples[\"text\"]]}\n",
    "\n",
    "dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "V dneÅ¡nÃ­ dobÄ› je wifi pÅ™ipojenÃ­ doma Äi v prÃ¡ci nezbytnostÃ­. AÅ¥ uÅ¾ pracujete z domova, streamujete filmy nebo hrajete online hry, stabilnÃ­ a rychlÃ© pÅ™ipojenÃ­ je klÃ­Äem k ÃºspÄ›chu. Frekvence wifi signÃ¡lu je jednÃ­m z faktorÅ¯, kterÃ½ mÅ¯Å¾e ovlivnit rychlost a stabilitu pÅ™ipojenÃ­. V tomto ÄlÃ¡nku se zamÄ›Å™Ã­me na pÄ›t krokÅ¯, kterÃ© vÃ¡m pomohou optimÃ¡lnÄ› nastavit frekvenci vaÅ¡eho wifi signÃ¡lu.\n",
      "Krok 1: Vyberte sprÃ¡vnÃ½ kanÃ¡l\n",
      "NaÅ¡e wifi sÃ­tÄ› fungujÃ­ na dvou frekvencÃ­ch â€“ 2,4 GHz a 5 GHz. KaÅ¾dÃ¡ z tÄ›chto frekvencÃ­ mÃ¡ urÄitÃ© mnoÅ¾stvÃ­ kanÃ¡lÅ¯. Na frekvenci 2,4 GHz mÃ¡me k dispozici 14 kanÃ¡lÅ¯. KanÃ¡ly jsou vÅ¡ak pÅ™ekrÃ½vajÃ­cÃ­ se, coÅ¾ znamenÃ¡, Å¾e pokud jsou dva sousedÃ­cÃ­ kanÃ¡ly pouÅ¾ity, mohou si vzÃ¡jemnÄ› ruÅ¡it. Proto se v praxi pouÅ¾Ã­vajÃ­ pÅ™evÃ¡Å¾nÄ› kanÃ¡ly 1, 6 a 11.\n",
      "Na frekvenci 5 GHz je k dispozici vÃ­ce kanÃ¡lÅ¯ a ty jsou navÃ­c nezÃ¡vislÃ©, tedy se nepÅ™ekrÃ½vajÃ­. To znamenÃ¡, Å¾e na tÃ©to frekvenci mÅ¯Å¾eme dosÃ¡hnout vyÅ¡Å¡Ã­ pÅ™enosovÃ© rychlosti.\n",
      "Krok 2: ZmÄ›Å™te ruÅ¡enÃ­\n",
      "Pokud chcete zjistit, kterÃ½ kanÃ¡l je pro vÃ¡s nejlepÅ¡Ã­, mÅ¯Å¾ete pouÅ¾Ã­t rÅ¯znÃ© aplikace k mÄ›Å™enÃ­ ruÅ¡enÃ­ wifi. Tyto aplikace skenujÃ­ dostupnÃ© kanÃ¡ly a vyhodnotÃ­, kterÃ½ z nich je nejmÃ©nÄ› ruÅ¡enÃ½. Na zÃ¡kladÄ› tÄ›chto informacÃ­ si pak mÅ¯Å¾ete vybrat optimÃ¡lnÃ­ kanÃ¡l pro svou sÃ­Å¥.\n",
      "Krok 3: Nastavte svÅ¯j router\n",
      "Po zjiÅ¡tÄ›nÃ­ nejvhodnÄ›jÅ¡Ã­ho kanÃ¡lu se pÅ™ihlaste do nastavenÃ­ svÃ©ho routeru. VÄ›tÅ¡ina routerÅ¯ mÃ¡ moÅ¾nost manuÃ¡lnÃ­ho vÃ½bÄ›ru kanÃ¡lu. PÅ™epnÄ›te tedy kanÃ¡l na ten, kterÃ½ jste si vybrali jako optimÃ¡lnÃ­.\n",
      "Krok 4: Zkuste obÄ› frekvence\n",
      "Pokud vÃ¡Å¡ router podporuje obÄ› frekvence, zkuste je obÄ›. Frekvence 5 GHz nabÃ­zÃ­ vyÅ¡Å¡Ã­ pÅ™enosovÃ© rychlosti, ale mÃ¡ menÅ¡Ã­ dosah a hÅ¯Å™e prochÃ¡zÃ­ pÅ™ekÃ¡Å¾kami. Frekvence 2,4 GHz naopak nabÃ­zÃ­ menÅ¡Ã­ rychlost, ale lepÅ¡Ã­ dosah a schopnost prochÃ¡zet pÅ™ekÃ¡Å¾kami. Zkuste tedy obÄ› frekvence a zjistÄ›te, kterÃ¡ z nich funguje pro vaÅ¡i situaci lÃ©pe.\n",
      "Krok 5: Optimizujte umÃ­stÄ›nÃ­ routeru\n",
      "PoslednÃ­m krokem ke zlepÅ¡enÃ­ frekvence wifi je optimÃ¡lnÃ­ umÃ­stÄ›nÃ­ routeru. Router by mÄ›l bÃ½t umÃ­stÄ›n co nejblÃ­Å¾e zaÅ™Ã­zenÃ­m, kterÃ¡ se k wifi pÅ™ipojujÃ­. MÄ›l by bÃ½t takÃ© umÃ­stÄ›n co nejdÃ¡le od pÅ™ekÃ¡Å¾ek, jako jsou stÄ›ny nebo elektronickÃ© zaÅ™Ã­zenÃ­, kterÃ© mohou signÃ¡l ruÅ¡it.\n",
      "ZÃ¡vÄ›r\n",
      "Optimalizace frekvence wifi mÅ¯Å¾e zlepÅ¡it rychlost a stabilitu vaÅ¡eho pÅ™ipojenÃ­. Sledujte tedy naÅ¡e pÄ›t krokÅ¯ a zlepÅ¡ete svÃ© online zÃ¡Å¾itky.\n",
      "HledÃ¡te spolehlivÃ½ internetovÃ½ tarif, kterÃ½ vÃ¡m poskytne superrychlÃ© a stabilnÃ­ pÅ™ipojenÃ­, a to za fÃ©rovou cenu? Pak je tu pro vÃ¡s Tarif pevnÃ½ Eri Internet! ZÃ­skejte vÃ½hody jako roÄnÃ­ platba, zÅ™Ã­zenÃ­ ZDARMA a sleva 600 KÄ kaÅ¾dÃ½ rok! A to nenÃ­ vÅ¡e...\n",
      "NeÄekejte, aÅ¾ vÃ¡m unikne tato skvÄ›lÃ¡ nabÃ­dka! StaÄÃ­ jednoduÅ¡e vyplnit formulÃ¡Å™ na naÅ¡em webu a my se o vÅ¡e postarÃ¡me. NavÃ­c, pokud se pÅ™ihlÃ¡sÃ­te nynÃ­, zÃ­skÃ¡te Eri LiveTV na dva mÄ›sÃ­ce zdarma a mÅ¯Å¾ete sledovat vÅ¡ech 133 TV programÅ¯! UÅ¾ijte si spolehlivÃ½ internet od Eri uÅ¾ dnes!<|eot_id|>\n",
      "=========================\n",
      "stiskacÃ­ mechanismus dÃ­ky rÃ½chloschnoucÃ­mu inkoustu a tvaru hrotu doporuÄeno i pro levÃ¡ky zabezpeÄuje pÅ™Ã­jemnÃ© a pohodlnÃ© psanÃ­ tlaÄÃ­tko v barvÄ› nÃ¡pnÄ› ruÅ¾ovÃ©, zelenÃ© a svetlomodrÃ© pera dodÃ¡vame s...\n",
      "Ke sprÃ¡vnÃ© registraci chybÃ­ poslednÃ­ krok.\n",
      "Zkontrolujte svÅ¯j e-mail (email@example.com) a kliknutÃ­m na odkaz\n",
      "ve zprÃ¡vÄ› potvrÄte sprÃ¡vnost registrace.\n",
      "DÄ›kujeme, tÃ½m Artikul.<|eot_id|>\n",
      "=========================\n",
      "LaserovÃ© Å™ezÃ¡nÃ­ trubek\n",
      "- Å˜ezÃ¡nÃ­ a opracovÃ¡nÃ­ nerezovÃ½ch, ocelovÃ½ch uzavÅ™enÃ½ch profilÅ¯ na vysoce vÃ½konnÃ©m CNC laserovÃ©m systÃ©mu s ÄistÃ½m Å™ezem vhodnÃ½m pod povrchovou Ãºpravu\n",
      "- PouÅ¾itÃ­m laseru je optimalizovÃ¡n vÃ½robnÃ­ proces. PÅ™i jedinÃ©m vÃ½robnÃ­m kroku jsou vyÅ™ezÃ¡ny vÅ¡echny otvory, komplexnÃ­ kontury a upravena dÃ©lka trubky. Tak jsou vÃ½raznÄ› sniÅ¾ovÃ¡ny vÃ½robnÃ­ nÃ¡klady uÅ¡etÅ™enÃ­m dalÅ¡Ã­ch operacÃ­ a Äasu (Å™ezÃ¡nÃ­, vrtÃ¡nÃ­, frÃ©zovÃ¡nÃ­ a vysekÃ¡vÃ¡nÃ­).\n",
      "- DÃ¡le jsou stÃ¡le vyvÃ­jeny novÃ© a novÃ© trubkovÃ© konstrukce, kterÃ© vedou k redukci a zjednoduÅ¡enÃ­ nÃ¡slednÃ½ch vÃ½robnÃ­ch operacÃ­. PotÅ™eba i vÃ½roba svaÅ™ovacÃ­ch pÅ™Ã­pravkÅ¯ klesÃ¡ vlivem pouÅ¾Ã­vÃ¡nÃ­ rychlospojek a konektorÅ¯. VodÃ­cÃ­ kolÃ­Äky a zÃ¡mky usnadÅˆujÃ­ nÃ¡slednou montÃ¡Å¾.\n",
      "LaserovÃ½ systÃ©m ADIGE LASERTUBE\n",
      "FlexibilnÃ­ stroj italskÃ© spoleÄnosti BLM Group urÄenÃ½ pro 3D opracovÃ¡nÃ­ uzavÅ™enÃ½ch (trubky a jekle) a otevÅ™enÃ½ch ocelovÃ½ch profilÅ¯ (L, C a U). Je vybavenÃ½ vÃ½kyvnou Å™ezacÃ­ hlavou, kterÃ¡ umoÅ¾Åˆuje Å™ez pod Ãºhlem tam, kde to nÃ¡slednÃ¡ technologie vyÅ¾aduje.\n",
      "- splÅˆuje nejnÃ¡roÄnÄ›jÅ¡Ã­ poÅ¾adavky na laserovÃ© Å™ezÃ¡nÃ­ 3D profilÅ¯ do Ã˜ 220 mm\n",
      "- celkovÃ© rozÅ™ezÃ¡nÃ­ dÃ©lkovÃ©ho profilu na opracovanÃ© jednotlivÃ© kusy vÃ½raznÄ› zkracuje nÃ¡klady a zjednoduÅ¡uje postupy dalÅ¡Ã­ho zpracovÃ¡nÃ­\n",
      "- vysokÃ¡ pÅ™esnost Å™ezanÃ½ch otvorÅ¯ (Â± 0,2 mm)\n",
      "- minimÃ¡lnÃ­ ztrÃ¡ty z tyÄe\n",
      "- gravÃ­rovÃ¡nÃ­\n",
      "- technologickÃ© zÃ¡mky rÅ¯znÃ½ch tvarÅ¯\n",
      "- rohovÃ½ vÃ½Å™ez pro ohnutÃ­ profilu\n",
      "ZpracovatelnÃ© prÅ¯Å™ezy:\n",
      "- KRUHOVÃ s prÅ¯mÄ›rem od 12 mm do 160 mm\n",
      "- ÄŒTVERCOVÃ se stranou od 12 do 120\n",
      "- OBDELNÃKOVÃ a PLOCHOOVÃLNÃ s prÅ¯Å™ezem opsanÃ½m prÅ¯mÄ›rem 170 mm. SÃ­la stÄ›ny Å™ezanÃ©ho materiÃ¡lu: max. 5 mm - Å™ezÃ¡no dusÃ­kem.<|eot_id|>\n",
      "=========================\n",
      "SportovnÃ­ sluneÄnÃ­ brÃ½le Relax Victoria R5398FKÃ³d: 5015\n",
      "DetailnÃ­ popis produktu\n",
      "Vlastnosti\n",
      "Velikost- Standard\n",
      "Barva- bÃ­lÃ¡\n",
      "MateriÃ¡l- Grilamid TR90\n",
      "PohlavÃ­- pÃ¡nskÃ©\n",
      "PohlavÃ­- dÃ¡mskÃ©\n",
      "MateriÃ¡l ÄoÄky- NÃ¡razuvzdornÃ½ polykarbonÃ¡t\n",
      "Barva ÄoÄky- hnÄ›dÃ¡\n",
      "Kategorie sluneÄnÃ­ho filtru- 2\n",
      "PovrchovÃ¡ Ãºprava ÄoÄky- OCEAN SENSOR\n",
      "CertifikÃ¡t\n",
      "Garantujeme provÄ›Å™enou bezpeÄnost.\n",
      "TR90\n",
      "ExtrÃ©mnÄ› lehkÃ½ a pÅ™itom velmi odolnÃ½ high-tec materiÃ¡l s nadprÅ¯mÄ›rnou pruÅ¾nostÃ­ a tvarovou pamÄ›tÃ­, zvÃ½Å¡enÃ½ komfort noÅ¡enÃ­.\n",
      "ProtiskluzovÃ© plochy\n",
      "ProtiskluzovÃ© nosnÃ­ky a straniÄky. ZamezujÃ­ skluzu brÃ½lÃ­, kterÃ© perfektnÄ› drÅ¾Ã­ na obliÄeji.\n",
      "UpravitelnÃ½ nosnÃ­k\n",
      "VÄ›tÅ¡Ã­ pohodlÃ­ pÅ™i noÅ¡enÃ­, nosnÃ­k lze pÅ™izpÅ¯sobit tvaru nosu.\n",
      "PolykarbonÃ¡t\n",
      "TÃ©mÄ›Å™ nerozbitnÃ½ materiÃ¡l ÄoÄek zajiÅ¡Å¥uje vyÅ¡Å¡Ã­ bezpeÄnost oÄÃ­.\n",
      "MÄ›kkÃ© pouzdro\n",
      "V mÄ›kkÃ©m mikrovlÃ¡knovÃ©m pouzdru se brÃ½le nepoÅ¡krÃ¡bou a navÃ­c jÃ­m lze brÃ½le dobÅ™e vyÄistit.\n",
      "DoplÅˆkovÃ© parametry\n",
      "|Kategorie:||BrÃ½le|\n",
      "|ZÃ¡ruka:||24|<|eot_id|>\n",
      "=========================\n",
      "- 812 008\n",
      "Å˜Ã­kÃ¡ se tomu domÃ½Å¡lenÃ­. A je nebezpeÄnÃ©. Nebo snad pomÃ¡hÃ¡ pÅ™edchÃ¡zet problÃ©mÅ¯m? DokonalÃ¡ myÅ¡lenka v tomto komiksu vystihuje to, jak smÃ½Å¡lÃ­me o ostatnÃ­ch v porovnÃ¡nÃ­ se skuteÄnostÃ­. Co si o vÃ¡s a situaci kolem vÃ¡s Å™Ã­kajÃ­ lidÃ© skuteÄnÄ› versus co si myslÃ­te, Å¾e si o nÃ­ domnÄ›le Å™Ã­kajÃ­? No, abychom se v tom neztratili, bude lepÅ¡Ã­ si to zÃ¡bavnÃ½m zpÅ¯sobem ukÃ¡zat v tomto dokonalÃ©m komiksu.\n",
      "- 304 131\n",
      "Å½enskÃ¡ logika prostÄ› nenÃ­ logickÃ¡. Jak jinak byste vysvÄ›tlili, Å¾e pokud vÃ¡s podvede holka, nakonec to vÅ¡echno dopadne tak, Å¾e byste se mÄ›li omlouvat vy? No, nechtÄ›li bychom teÄ bÃ½t v kÅ¯Å¾i chudÃ¡ka Derpa (nebo toho chlÃ¡pka, co po dobu trvÃ¡nÃ­ komiksu drÅ¾Ã­ ukÃ¡zkovÃ½ poker face). PonauÄenÃ­ pro dneÅ¡nÃ­ den je velmi jednoduchÃ© - kdyÅ¾ uÅ¾ mÃ¡te potÅ™ebu svou pÅ™Ã­telkyni sledovat, nikdy jÃ­ to nepÅ™iznÃ¡vejte.\n",
      "- 301 603\n",
      "Tento jednoduchÃ½ test osobnosti je zaloÅ¾en na tom, Å¾e se ÄlovÄ›k podÃ­vÃ¡ na obrÃ¡zku a Å™ekne, kterÃ¡ vÄ›c ho na nÄ›m zaujala jako prvnÃ­. V tomto pÅ™Ã­padÄ› existujÃ­ 3 moÅ¾nosti, kterÃ© vÃ¡m nÃ¡slednÄ› poodhalÃ­ skrytou tvÃ¡Å™ vaÅ¡Ã­ osobnosti. Test si mÅ¯Å¾ete vyzkouÅ¡et takÃ© s pÅ™Ã­teli, protoÅ¾e to, co hned vidÃ­ jeden ÄlovÄ›k, mÅ¯Å¾e bÃ½t druhÃ©mu (minimÃ¡lnÄ› v prvnÃ­ moment) ÃºplnÄ› skryto.\n",
      "- 307 788\n",
      "KaÅ¾dÃ¡ hra mÃ¡ jinÃ¡ specifika. PojÄme se podÃ­vat na jednotlivÃ© stÅ™Ã­leÄky, ve kterÃ½ch mÃ¡me pÅ™idÄ›lenou nÄ›jakou hernÃ­ mechaniku. Jak si modelovÃ½ hrÃ¡Ä poradÃ­ s protivnÃ­ky nebo jakou finty je moÅ¾nÃ© pouÅ¾Ã­t? NÄ›kdy se bohuÅ¾el i nejzkuÅ¡enÄ›jÅ¡Ã­m hrÃ¡ÄÅ¯m stÃ¡vÃ¡, Å¾e je jim k niÄemu. NÃ¡sledujÃ­cÃ­ obrÃ¡zky pochopÃ­ jen opravdovÃ½ pÅ™Ã­znivce her. RozkliknÄ›te si tedy hrÃ¡ÄskÃ½ komiks a pÅ™ipomeÅˆte si Äasy, kdyÅ¾ jste proÅ¾Ã­vali to napÄ›tÃ­.\n",
      "- 456 923\n",
      "V nynÄ›jÅ¡Ã­m modernÃ­m svÄ›tÄ› je sport stejnÄ› tak populÃ¡rnÃ­, jako tomu bylo v dÃ¡vnÃ½ch dobÃ¡ch. KaÅ¾dÃ½ druhÃ½ ÄlovÄ›k chodÃ­ potit krev do posilovny, nebo cviÄÃ­ jÃ³gu, bÄ›hÃ¡ po ulicÃ­ch v dopolednÃ­ch hodinÃ¡ch nebo dodrÅ¾uje pÅ™Ã­snÃ½ jÃ­delnÃ­Äek plnÃ½ zdravÃ½ch pochutin. Je to tak, McDonaldu a ostatnÃ­m fast foodÅ¯m uÅ¾ odzvonilo a v mÃ³dÄ› jsou smoothie plnÃ© vitamÃ­nÅ¯ pro skvÄ›lou rovnovÃ¡hu a nastartovÃ¡nÃ­ dne. Co se tÃ½Äe cviÄenÃ­, jsou zde urÄitÃ¡ fakta, Å™eknÄ›me mÃ½ty, kterÃ½m hodnÄ› lidÃ­ neustÃ¡le vÄ›Å™Ã­ a kterÃ¡ jsou naprosto Å¡patnÄ›! JakÃ©? PodÃ­vejte se sami!\n",
      "- 409 916\n",
      "KaÅ¾dÃ¡ Å¾ena je jedineÄnÃ¡, to takÃ© v tom, co dÄ›lÃ¡. PodÃ­vejte se na komiks Å¾enskÃ© pÅ™Ã­bÄ›hy z brightside.me, kterÃ© vytvoÅ™ila autorka Valeriya Fortuna a zaruÄenÄ› se mohly kaÅ¾dÃ© z nich stÃ¡t. NejrÅ¯znÄ›jÅ¡Ã­ kaÅ¾dodennÃ­ situace, obÄasnÃ© nezdary, chyby nebo zlomyslnosti Å¾en. PÅ™iznejme si, Å¾e kaÅ¾dÃ¡ mÃ¡ i svÃ© osobitÃ© manÃ½ry a nÄ›kterÃ© pochopÃ­ jen mÃ¡lokterÃ½ muÅ¾, ale pÅ™es to bez nÄ›Å¾nÃ©ho pohlavÃ­ by byl svÄ›t chmurnÄ›jÅ¡Ã­ a smutnÄ›jÅ¡Ã­. PouÅ¡tÃ­me tedy do svÄ›ta dalÅ¡Ã­ galerii o Å¾enÃ¡ch a jejich Å¾ivotÄ› z rubriky SrandovnÃ­ obrÃ¡zky.\n",
      "- 612 811\n",
      "VÃ­te, proÄ dinosauÅ™i vyhinuli? A takÃ© to, jak vznikli? DokonalÃ½ komiks od ItsTheTie vyobrazuje Boha, strejdu Satana a moment, kterÃ½ tam nahoÅ™e speÄetil osud jeÅ¡tÄ›rek pÅ™ed 65 miliony let (i kdyÅ¾ ony to nebyly ani tak jeÅ¡tÄ›rky jako slepice, podle nejnovÄ›jÅ¡Ã­ch vÄ›deckÃ½ch dÅ¯kazÅ¯, ale vem to Äert, Å¾e?). Jsou to hadi a jdou do pekel! :-D\n",
      "- 304 950\n",
      "Velmi nÃ¡s zajÃ­mÃ¡, kolik z vÃ¡s vyhedÃ¡ po pÅ™eÄtenÃ­ tÃ©to galerie odbornonu pomoc u psychologa, takÅ¾e nÃ¡m nezapomeÅˆte sdÄ›lit svÅ¯j verdikt v komentÃ¡Å™i! MÃ¡me pro vÃ¡s dalÅ¡Ã­ psychologickÃ½ test, kterÃ½ odhalÃ­ vaÅ¡e mentÃ¡lnÃ­ poruchy, o kterÃ½ch jste ani nevÄ›dÄ›li! StaÄÃ­ si udÄ›lat tento jednoduchÃ½ test. PozornÄ› se podÃ­vejte na kaÅ¾dÃ© z nÃ¡sledujÃ­cÃ­ch Å¡esti koleÄek. Nenechte se zmÃ¡st, nenÃ­ to test u oÄnÃ­ho lÃ©kaÅ™e, ale probÃ­hÃ¡ tÃ©mÄ›Å™ stejnÄ›. VidÃ­te v kaÅ¾dÃ©m koleÄku ÄÃ­slo? Pak je vÅ¡e v poÅ™Ã¡dku. Pokud ale nÄ›jakÃ© ÄÃ­slo nedokÃ¡Å¾ete identifikovat, pÅ™eÄtÄ›te si svoji charakteristiku!\n",
      "- 323 344\n",
      "MazlenÃ­ mÃ¡ rÃ¡d kaÅ¾dÃ½. NÄ›kdo s opaÄnÃ½m pohlavÃ­m, nÄ›kdo se stejnÃ½m pohlavÃ­m, jinÃ½ zase se svÃ½mi domÃ¡cÃ­mi mazlÃ­Äky, Äi nedÃ¡vnou zakoupenÃ½m kalashnikovem - to se tÃ½kÃ¡ vÃ­ce fanouÅ¡kÅ¯ armÃ¡dnÃ­ch zÃ¡leÅ¾itostÃ­. ProÄ je ale mazlenÃ­ s partnerem tak dÅ¯leÅ¾itÃ©? A proÄ je vlastnÄ› nejlepÅ¡Ã­? V tomto ponÄ›kud krÃ¡tkÃ©m, zato vÃ½stiÅ¾nÃ©m komiksu pochopÃ­te.\n",
      "- 292 835\n",
      "Bezesporu k nejoblÃ­benÄ›jÅ¡Ã­m se na internetu Å™adÃ­ komiksy s Hipsterkou a HrÃ¡Äkou od Jago Dibuja. V tÃ©to kolekci komiksÅ¯ od tohoto autora si ukÃ¡Å¾eme nÄ›kolik dechberoucÃ­ch momentÅ¯ s dÄ›tmi a postaviÄkami z komiksu, kterÃ© zcela oÄividnÄ› devalvujÃ­ veÅ¡kerÃ© zÃ¡sady a pravidla, jimiÅ¾ se dospÄ›lÃ­ Å™Ã­dÃ­ vÅ¯Äi dÄ›tem. ZkrÃ¡tka, hipsterka a hrÃ¡Äka si neberou servÃ­tky a s dÄ›tmi to opravdu \"umÃ­\". :-D\n",
      "- 404 794\n",
      "Tohle je Harry Potter. A postavy, kterÃ© nakreslil jeden *bezejmennÃ½* umÄ›lec dle popisu v knize a souÄasnÄ› je postavil vstÅ™Ã­c kontrastu podoby, kterou tyto postavy zÃ­skaly ve filmu. Jak moc se liÅ¡Ã­ samotnÃ½ Harry? A jak moc Hermiona? SleÄna GrangerovÃ¡ byla pÅ¯vodnÄ› pizizubkou, v originÃ¡lnÃ­m filmu vÅ¡ak ztratila tento defekt a stala se ponÄ›kud atraktivnÃ­ bordelmÃ¡mou. Å koda, Å¾e se autoÅ™i neÅ™Ã­dili popisem postav u dalÅ¡Ã­ch charakterÅ¯. A nebo se trefili? Co myslÃ­te?\n",
      "- 374 057\n",
      "PonÄ›kud cynickÃ© a plnÃ© sarkasmu jsou nÃ¡sledujÃ­cÃ­ omalovÃ¡nky, striktnÄ› urÄenÃ© jen dospÄ›lÃ½m lidem. To, kdybyste se fakt hodnÄ› nudili ve svÃ©m zamÄ›stnÃ¡nÃ­, mÅ¯Å¾ete pak zkusit nalistovat na strÃ¡nku s titulkem \"Pokuste se utÃ©ct z Friendzone\". V tÄ›chto omalovÃ¡nkÃ¡ch totiÅ¾ budete nejen omalovÃ¡vat, nÃ½brÅ¾ takÃ© utÃ­kat. A nejen to. PodÃ­vejte se sami. A kreslete!\n",
      "- 393 788\n",
      "V tÃ©to galerii se mÅ¯Å¾ete pÅ™esvÄ›dÄit, Å¾e vÄ›tÅ¡ina postav z Harryho Pottera opravdu vyrostla do krÃ¡sy. PÅ™Ã­kladem mÅ¯Å¾e bÃ½t tÅ™eba Matthew Lewis, kterÃ½ si zahrÃ¡l postavu Neville Longbottoma. Odtud takÃ© novÃ© pÅ™Ã­davnÃ© jmÃ©no longbottoming, kterÃ© bychom mohli pÅ™eloÅ¾it jako dokrÃ¡syrostoucÃ­.\n",
      "- 555 182\n",
      "Mezi nÃ¡mi Å¾enami-obÄas mÃ¡me sloÅ¾itÃ½ Å¾ivot, za kterÃ½ si obÄas mÅ¯Å¾eme i samy. A sloÅ¾itÃ½ Å¾ivotsi nÄ›kdy Å¾Ã¡dÃ¡ i sloÅ¾itÃ© Å™eÅ¡enÃ­ na sloÅ¾itÃ© situace. NenÃ­ divu, Å¾e nÃ¡m muÅ¾i dost Äasto nerozumÃ­. Ano, hÃ¡dÃ¡te sprÃ¡vnÄ›, dnes pro vÃ¡s mÃ¡me vtipnÄ› udÄ›lanou galerii o Å¾enÃ¡ch takovÃ½ch, jakÃ© doopravdy jsou. KlidnÄ› se s vÃ¡mi vsadÃ­m, Å¾e alespoÅˆ jednou v Å¾ivotÄ› se kaÅ¾dÃ¡ Å¾ena ocitla v jednÃ© z tÄ›ch patnÃ¡cti situacÃ­. JÃ¡ tÅ™eba snad skoro ve vÅ¡ech. ZajÃ­mÃ¡ vÃ¡s logika Å¾en? Pak tahle galerie ÄekÃ¡ prÃ¡vÄ› na vÃ¡s!\n",
      "- 442 641\n",
      "KdyÅ¾ jste byli malÃ­, urÄitÄ› vÃ¡m rodiÄe Å™Ã­kali, abyste si neÄetli po tmÄ›, Å¾e si zkazÃ­te oÄi. MoÅ¾nÃ¡, Å¾e nÄ›kterÃ© z vÃ¡s napadlo, Å¾e kdyÅ¾ potom snÃ­te mrkev, kterÃ¡ je podle vaÅ¡ich rodiÄÅ¯ zdravÃ¡ na oÄi, tak si ten zrak zlepÅ¡Ã­te a oba dva jevy se vynulujÃ­. TeÄ vÃ¡s ale musÃ­me zklamat - nejenÅ¾e to takhle na svÄ›tÄ› nefunguje, ale oba dva fakty jsou ve skuteÄnosti mÃ½ty. A patÅ™Ã­ do Å¡irokÃ© skupiny mÃ½tÅ¯, kterÃ© nÃ¡m odmala vtloukali do hlavy. TakÅ¾e o Äem vÅ¡em Å¾e nÃ¡m to rodiÄe, pÅ™Ã­buznÃ­, ba dokonce i uÄitelÃ© ve Å¡kole lhali?\n",
      "- 288 805\n",
      "Lehce starÅ¡Ã­ generaci, ale ne zase ÃºplnÄ› starou, dost moÅ¾nÃ¡ udeÅ™Ã­ do oÄÃ­ fakt, Å¾e herec Samy Naceri, hlavnÃ­ postava filmovÃ© sÃ©rie Taxi, mÃ¡ ve skuteÄnosti uÅ¾ 53 let. Bum. A to si ho celÃ¡ Å™ada z nÃ¡s pamatuje jako mlÃ¡dence, co se prohÃ¡nÃ­ italskÃ½mi ulicemi mÄ›st ve svÃ½m super ultra dokonalÃ©m fÃ¡ru s cedulkou \"odvezu vÃ¡s kamkoliv a za pÅ™Ã­platek mi mÅ¯Å¾ete v autÄ› zvracet\". KterÃ© celebrity jsou vÅ¡ak stejnÄ› starÃ© a neÅ™ekli byste to do nich? V tÃ©to galerii zahlÃ©dnete filmovÃ© here jako Silvester Stalone nebo dÄ›deÄka Chucka Norrise. Vedle nich i Terryho Crewse a nebo kapitÃ¡na Picarda z legendÃ¡rnÃ­ho Star Treku.\n",
      "- 1 838 735\n",
      "A kdyÅ¾ Å™Ã­kÃ¡me \"aÅ¾ do konce\", tak myslÃ­me zcela vÃ¡Å¾nÄ›, abyste tento rage komiks pÅ™eÄetli celÃ½. PatÅ™Ã­ do zlatÃ© sbÃ­rky nejlepÅ¡Ã­ch rage komiksÅ¯, kterÃ© najdete na Trololol. A i pokud jste se s nÃ­m uÅ¾ nÄ›kdy setkali, urÄitÄ› vÃ¡s repete neurazÃ­. ProtoÅ¾e tohle patÅ™Ã­ k nestÃ¡rnoucÃ­m vtipÅ¯m! :-D\n",
      "- 681 351\n",
      "Pokud mÃ¡te velkÃ¡ prsa, nese to s sebou pochopitelnÄ› celou Å™adu vÃ½hod, zejmÃ©na co se tÃ½Äe muÅ¾Å¯. NicmÃ©nÄ› kaÅ¾dÃ¡ bohatÄ› obdaÅ™enÃ¡ Å¾ena vÃ¡m potvrdÃ­, Å¾e to mÃ¡ takÃ© velkou spoustu nevÃ½hod, kterÃ½ vÃ¡m dokÃ¡Å¾ou pÄ›knÄ› znepÅ™Ã­jemnit Å¾ivot.\n",
      "- 286 354\n",
      "Vztah dvou lidÃ­ je zaloÅ¾en na vÃ­ce vÄ›cech, neÅ¾ je lÃ¡ska, tolerance a porozumÄ›nÃ­. Nemalou roli hraje i sexuÃ¡lnÃ­ Å¾ivot. Asi vÅ¡ichni se shodneme na tom, Å¾e to, co se odehrÃ¡vÃ¡ ve filmech pro dospÄ›lÃ©, nemÃ¡ vÅ¯bec nic spoleÄnÃ©ho s lÃ¡skou. DokÃ¡Å¾ete si pÅ™edstavit, Å¾e byste to dÄ›lali tak, jak se to dÄ›lÃ¡ ve filmech pro dospÄ›lÃ©? V tÃ©to galerii vÃ¡m ukÃ¡Å¾eme, jak by to vypadalo! PodÃ­vejte se sami.\n",
      "- 292 831\n",
      "O pÅ™estÃ¡vce se ve Å¡kole dÃ¡ dÄ›lat spousta vÄ›cÃ­. NÄ›kdo ji vyuÅ¾ije k tomu, aby si dodÄ›lal Ãºkol ze vÄerejÅ¡ka, jinÃ½ se zase snaÅ¾Ã­ nauÄit se na test, dalÅ¡Ã­ hraje hry na mobilu, kdeÅ¾to nÄ›kdo dalÅ¡Ã­ si jde popovÃ­dat s kamarÃ¡dy. Ale o pÅ™estÃ¡vkÃ¡ch se dajÃ­ dÄ›lat i jinÃ© vÄ›ci, aÄkoli by se dÄ›lat nemÄ›ly. NepÅ™emÃ½Å¡leli jste nÄ›kdy o tom, Å¾e byste si se svou poloviÄkou nÄ›kam zalezli a trochu se tam uvolnili?\n",
      "- 417 710\n",
      "Autor mnoha komiksÅ¯ a rÅ¯znÃ½ch kreslenÃ½ch postav Yehuda Adi Devir ukÃ¡zal svÄ›tu svoji sÃ©rii obrÃ¡zkÅ¯, ve kterÃ© je vykreslenÃ½ Å¾ivot s jeho manÅ¾elkou Mayou. Ilutrace jsou velmi vtipnÃ©, milÃ© a zahÅ™ejÃ­ u srdce. AÅ¥ uÅ¾ jste ve vÃ¡Å¾nÃ©m vztahu Äi manÅ¾elstvÃ­, najdete alespoÅˆ jeden obrÃ¡zek, kterÃ½ se k vÃ¡m hodÃ­.\n",
      "- 441 042\n",
      "Autorka, kterÃ¡ se vydÃ¡vÃ¡ pod nickem C-Cassandra, zobrazila muÅ¾Å¯m 7 vÄ›cÃ­, se kterÃ½mi se Å¾eny trÃ¡pÃ­ a kterÃ© muÅ¾i nikdy neocenÃ­. MuÅ¾i vÅ¡ak zcela jistÄ› ocenÃ­ dvÄ› vÄ›ci, kterÃ© Å¾eny zase tak moc netrÃ¡pÃ­. PrvnÃ­ je nahota. A druhÃ¡ je pivo v ruce. TakÅ¾e pokud na to chcete jÃ­t jednoduÅ¡e, prostÄ› tÄ›chto jinak dÅ¯leÅ¾itÃ½ch sedm bodÅ¯ vynechte.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "for row in dataset[\"train\"][:5][\"text\"]:\n",
    "    print(\"=========================\")\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_kl_loss = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnslothKLLossTrainer(UnslothTrainer):\n",
    "    def __init__(self, ref_model, kl_weight=0.1, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.ref_model = ref_model\n",
    "        self.ref_model.eval()\n",
    "        self.kl_weight = kl_weight\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        #account for the actual number of items in the batch\n",
    "        if num_items_in_batch is not None:\n",
    "            effective_batch_size = num_items_in_batch\n",
    "        else:\n",
    "            effective_batch_size = labels.size(0)\n",
    "        \n",
    "        #cross entropy loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        loss_ce = loss_fct(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "        loss_ce = loss_ce / effective_batch_size # Scale by actual number of items\n",
    "\n",
    "        # KL loss\n",
    "        with torch.no_grad():\n",
    "            ref_outputs = self.ref_model(**inputs)\n",
    "            ref_logits = ref_outputs.logits\n",
    "        \n",
    "        loss_kl_fct = torch.nn.KLDivLoss(reduction=\"sum\")\n",
    "        loss_kl = loss_kl_fct(\n",
    "            torch.nn.functional.log_softmax(logits, dim=-1),\n",
    "            torch.nn.functional.softmax(ref_logits, dim=-1),\n",
    "        )\n",
    "        loss_kl = loss_kl / effective_batch_size # Scale by actual number of items\n",
    "\n",
    "        total_loss = loss_ce + self.kl_weight*loss_kl\n",
    "\n",
    "        return (total_loss, outputs) if return_outputs else total_loss\n",
    "\n",
    "\n",
    "if use_kl_loss:\n",
    "    TrainerClass = UnslothKLLossTrainer\n",
    "else:\n",
    "    TrainerClass = UnslothTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlynatom/master-thesis-repository-tomas-mlynar/unsloth_compiled_cache/UnslothSFTTrainer.py:578: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/mlynatom/master-thesis-repository-tomas-mlynar/unsloth_compiled_cache/UnslothSFTTrainer.py:592: UserWarning: You passed a `dataset_num_proc` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/mlynatom/master-thesis-repository-tomas-mlynar/unsloth_compiled_cache/UnslothSFTTrainer.py:606: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "trainer = TrainerClass(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset[\"train\"],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    ref_model=copy.deepcopy(model),\n",
    "    \n",
    "    args = UnslothTrainingArguments(\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_ratio = 0.1,\n",
    "        #num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 1000,\n",
    "        learning_rate = 5e-5,\n",
    "        embedding_learning_rate = 5e-6,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = SEED,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"wandb\", # Use this for WandB etc\n",
    "        run_name=\"llama3.2-3b-instruct-cont_pretrain_example\",\n",
    "        # eval_strategy = \"steps\",\n",
    "        # eval_steps = 50,\n",
    "        \n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA A100-SXM4-40GB. Max memory = 39.381 GB.\n",
      "10.312 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 1,000 | Num Epochs = 17\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 16 | Total steps = 1,000\n",
      " \"-____-\"     Number of trainable parameters = 1,177,026,560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Setting lr = 5.00e-06 instead of 5.00e-05 for embed_tokens.\n",
      "Unsloth: Setting lr = 5.00e-06 instead of 5.00e-05 for lm_head.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmlynatom\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mlynatom/master-thesis-repository-tomas-mlynar/wandb/run-20250221_151746-1ee8wr73</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlynatom/huggingface/runs/1ee8wr73' target=\"_blank\">llama3.2-3b-instruct-cont_pretrain_example</a></strong> to <a href='https://wandb.ai/mlynatom/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlynatom/huggingface' target=\"_blank\">https://wandb.ai/mlynatom/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlynatom/huggingface/runs/1ee8wr73' target=\"_blank\">https://wandb.ai/mlynatom/huggingface/runs/1ee8wr73</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mlynatom/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/unsloth/models/llama.py:1891: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  start = re.search('logger\\.info\\([\\\"\\'].+?Running training', inner_training_loop).span(0)[0]\n",
      "/home/mlynatom/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/unsloth/models/llama.py:1894: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  spaces = re.search('\\n([\\s\\t]{1,})', original_debug).group(0)[1:]\n",
      "/home/mlynatom/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/unsloth/models/llama.py:1895: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  front_spaces = re.match('([\\s\\t]{1,})', inner_training_loop).group(0)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 5.50 GiB. GPU 0 has a total capacity of 39.38 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 37.53 GiB memory in use. Of the allocated memory 36.69 GiB is allocated by PyTorch, and 344.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:380\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m<string>:31\u001b[0m, in \u001b[0;36m_unsloth_training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n",
      "Cell \u001b[0;32mIn[10], line 26\u001b[0m, in \u001b[0;36mUnslothKLLossTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# KL loss\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 26\u001b[0m     ref_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     ref_logits \u001b[38;5;241m=\u001b[39m ref_outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     29\u001b[0m loss_kl_fct \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mKLDivLoss(reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/torch/_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m prior \u001b[38;5;241m=\u001b[39m _maybe_set_eval_frame(callback)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    634\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/unsloth/models/llama.py:1216\u001b[0m, in \u001b[0;36mPeftModelForCausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, num_logits_to_keep, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_dynamo\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mPeftModelForCausalLM_fast_forward\u001b[39m(\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1214\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1215\u001b[0m ):\n\u001b[0;32m-> 1216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:197\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/unsloth/models/llama.py:1061\u001b[0m, in \u001b[0;36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, logits_to_keep, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1059\u001b[0m     \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_has_no_labels \u001b[38;5;241m=\u001b[39m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1061\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1074\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/unsloth/models/llama.py:885\u001b[0m, in \u001b[0;36mLlamaModel_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 885\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/unsloth/models/llama.py:532\u001b[0m, in \u001b[0;36mLlamaDecoderLayer_fast_forward\u001b[0;34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, position_embeddings, *args, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    531\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m fast_rms_layernorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm, hidden_states)\n\u001b[0;32m--> 532\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    545\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/unsloth/models/llama.py:454\u001b[0m, in \u001b[0;36mLlamaAttention_fast_forward\u001b[0;34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, position_embeddings, *args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;66;03m# Grouped query attention\u001b[39;00m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SDPA_HAS_GQA:\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;66;03m# Needs (batch_size, n_heads, seq_len, head_dim)\u001b[39;00m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;66;03m# is_casual and attention_mask must not be both set!\u001b[39;00m\n\u001b[0;32m--> 454\u001b[0m         A \u001b[38;5;241m=\u001b[39m \u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_gqa\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_groups\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m         \u001b[38;5;66;03m# Go back to (batch_size, seq_len, n_heads, head_dim)\u001b[39;00m\n\u001b[1;32m    456\u001b[0m         A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;66;03m#.contiguous()\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.50 GiB. GPU 0 has a total capacity of 39.38 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 37.53 GiB memory in use. Of the allocated memory 36.69 GiB is allocated by PyTorch, and 344.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4329.0353 seconds used for training.\n",
      "72.15 minutes used for training.\n",
      "Peak reserved memory = 19.932 GB.\n",
      "Peak reserved memory for training = 17.297 GB.\n",
      "Peak reserved memory % of max memory = 50.613 %.\n",
      "Peak reserved memory for training % of max memory = 43.922 %.\n"
     ]
    }
   ],
   "source": [
    "#Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
