{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continued pretraining (CP) using Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: unsloth 2025.3.18\n",
      "Uninstalling unsloth-2025.3.18:\n",
      "  Successfully uninstalled unsloth-2025.3.18\n",
      "Collecting git+https://github.com/unslothai/unsloth.git\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-req-build-qo_xz0p1\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-req-build-qo_xz0p1\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit eefba34e94443971533bffdf2ac32069ed07b0c2\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: unsloth\n",
      "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unsloth: filename=unsloth-2025.3.19-py3-none-any.whl size=192249 sha256=c7aa5801276b72e6e75c104e16de19679d3cdbe41260ceedeeabd2c20d2c22a7\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-qb7btzxj/wheels/d1/17/05/850ab10c33284a4763b0595cd8ea9d01fce6e221cac24b3c01\n",
      "Successfully built unsloth\n",
      "Installing collected packages: unsloth\n",
      "Successfully installed unsloth-2025.3.19\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#install latest version of unsloth\n",
    "#%pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "#%pip install --upgrade unsloth\n",
    "%pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
    "# %pip install --upgrade unsloth_zoo\n",
    "# %pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xFormers 0.0.29.post3\n",
      "memory_efficient_attention.ckF:                    unavailable\n",
      "memory_efficient_attention.ckB:                    unavailable\n",
      "memory_efficient_attention.ck_decoderF:            unavailable\n",
      "memory_efficient_attention.ck_splitKF:             unavailable\n",
      "memory_efficient_attention.cutlassF-pt:            available\n",
      "memory_efficient_attention.cutlassB-pt:            available\n",
      "memory_efficient_attention.fa2F@v2.5.7-pt:         available\n",
      "memory_efficient_attention.fa2B@v2.5.7-pt:         available\n",
      "memory_efficient_attention.fa3F@0.0.0:             unavailable\n",
      "memory_efficient_attention.fa3B@0.0.0:             unavailable\n",
      "memory_efficient_attention.triton_splitKF:         available\n",
      "indexing.scaled_index_addF:                        available\n",
      "indexing.scaled_index_addB:                        available\n",
      "indexing.index_select:                             available\n",
      "sp24.sparse24_sparsify_both_ways:                  available\n",
      "sp24.sparse24_apply:                               available\n",
      "sp24.sparse24_apply_dense_output:                  available\n",
      "sp24._sparse24_gemm:                               available\n",
      "sp24._cslt_sparse_mm_search@0.0.0:                 available\n",
      "sp24._cslt_sparse_mm@0.0.0:                        available\n",
      "swiglu.dual_gemm_silu:                             available\n",
      "swiglu.gemm_fused_operand_sum:                     available\n",
      "swiglu.fused.p.cpp:                                available\n",
      "is_triton_available:                               True\n",
      "pytorch.version:                                   2.6.0\n",
      "pytorch.cuda:                                      available\n",
      "gpu.compute_capability:                            8.0\n",
      "gpu.name:                                          NVIDIA A100-SXM4-80GB\n",
      "dcgm_profiler:                                     available\n",
      "build.info:                                        available\n",
      "build.cuda_version:                                1204\n",
      "build.hip_version:                                 None\n",
      "build.python_version:                              3.11.5\n",
      "build.torch_version:                               2.6.0\n",
      "build.env.TORCH_CUDA_ARCH_LIST:                    8.0\n",
      "build.env.PYTORCH_ROCM_ARCH:                       None\n",
      "build.env.XFORMERS_BUILD_TYPE:                     None\n",
      "build.env.XFORMERS_ENABLE_DEBUG_ASSERTIONS:        None\n",
      "build.env.NVCC_FLAGS:                              None\n",
      "build.env.XFORMERS_PACKAGE_FROM:                   None\n",
      "build.nvcc_version:                                12.4.99\n",
      "source.privacy:                                    open source\n"
     ]
    }
   ],
   "source": [
    "!python -m xformers.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++ BUG REPORT INFORMATION ++++++++++++++++++\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++++++++ OTHER +++++++++++++++++++++++++++\n",
      "CUDA specs: CUDASpecs(highest_compute_capability=(8, 0), cuda_version_string='124', cuda_version_tuple=(12, 4))\n",
      "PyTorch settings found: CUDA_VERSION=124, Highest Compute Capability: (8, 0).\n",
      "To manually override the PyTorch CUDA version please see: https://github.com/TimDettmers/bitsandbytes/blob/main/docs/source/nonpytorchcuda.mdx\n",
      "The directory listed in your path is found to be non-existent: /mnt/personal/mlynatom/.vscode-server/.vscode-server/extensions/ms-python.debugpy-2025.4.1-linux-x64/.noConfigDebugAdapterEndpoints/endpoint-41cacdfa2708845e.txt\n",
      "The directory listed in your path is found to be non-existent: /home/mlynatom/.jupyter/lab/workspaces\n",
      "The directory listed in your path is found to be non-existent: /home/mlynatom/master-thesis-repository-tomas-mlynar/continued_pretraining-49cc2c70-94af-4bd0-828c-9f617d952646.ipynb\n",
      "The directory listed in your path is found to be non-existent: /usr/share/lmod/lmod/share/man\n",
      "The directory listed in your path is found to be non-existent: /etc/pki/tls/certs\n",
      "The directory listed in your path is found to be non-existent: /usr/modulefiles\n",
      "The directory listed in your path is found to be non-existent: /home/mlynatom/.jupyter/lab/user-settings\n",
      "The directory listed in your path is found to be non-existent: /run/user/498831/vscode-git-66baea06e8.sock\n",
      "The directory listed in your path is found to be non-existent: GCCcore/13.2.0\n",
      "The directory listed in your path is found to be non-existent: zlib/1.2.13-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: binutils/2.40-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: GCC/13.2.0\n",
      "The directory listed in your path is found to be non-existent: numactl/2.0.16-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: XZ/5.4.4-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: libxml2/2.11.5-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: libpciaccess/0.17-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: hwloc/2.9.2-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: OpenSSL/1.1\n",
      "The directory listed in your path is found to be non-existent: libevent/2.1.12-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: libfabric/1.19.0-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: UCC/1.2.0-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: OpenBLAS/0.3.24-GCC-13.2.0\n",
      "The directory listed in your path is found to be non-existent: FlexiBLAS/3.3.1-GCC-13.2.0\n",
      "The directory listed in your path is found to be non-existent: FFTW/3.3.10-GCC-13.2.0\n",
      "The directory listed in your path is found to be non-existent: PMIx/4.2.5-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: OpenMPI/4.1.5-GCC-13.2.0\n",
      "The directory listed in your path is found to be non-existent: gompi/2023b\n",
      "The directory listed in your path is found to be non-existent: FFTW.MPI/3.3.10-gompi-2023b\n",
      "The directory listed in your path is found to be non-existent: ScaLAPACK/2.2.0-gompi-2023b-fb\n",
      "The directory listed in your path is found to be non-existent: foss/2023b\n",
      "The directory listed in your path is found to be non-existent: bzip2/1.0.8-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: ncurses/6.4-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: libreadline/8.2-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: Tcl/8.6.13-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: SQLite/3.43.1-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: libffi/3.4.4-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: Python/3.11.5-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: CUDA/12.4.0\n",
      "The directory listed in your path is found to be non-existent: Ninja/1.11.1-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: cffi/1.15.1-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: cryptography/41.0.5-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: virtualenv/20.24.6-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: Python-bundle-PyPI/2023.10-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: Abseil/20240116.1-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: protobuf/25.3-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: protobuf-python/4.25.3-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: pybind11/2.11.1-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: gfbf/2023b\n",
      "The directory listed in your path is found to be non-existent: SciPy-bundle/2023.11-gfbf-2023b\n",
      "The directory listed in your path is found to be non-existent: libyaml/0.2.5-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: PyYAML/6.0.1-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: GMP/6.3.0-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: MPFR/4.2.1-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: NASM/2.16.01-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: x264/20231019-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: LAME/3.100-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: x265/3.5-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: expat/2.5.0-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: libpng/1.6.40-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: Brotli/1.1.0-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: freetype/2.13.2-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: util-linux/2.39-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: fontconfig/2.14.2-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: xorg-macros/1.20.0-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: X11/20231019-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: FriBidi/1.0.13-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: SDL2/2.28.5-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: FFmpeg/6.0-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: libjpeg-turbo/3.0.1-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: jbigkit/2.1-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: gzip/1.13-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: lz4/1.9.4-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: zstd/1.5.5-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: libdeflate/1.19-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: LibTIFF/4.6.0-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: Pillow/10.2.0-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: expecttest/0.2.1-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: networkx/3.2.1-gfbf-2023b\n",
      "The directory listed in your path is found to be non-existent: MPC/1.3.1-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: gmpy2/2.1.5-GCC-13.2.0\n",
      "The directory listed in your path is found to be non-existent: sympy/1.13.1-gfbf-2023b\n",
      "The directory listed in your path is found to be non-existent: Z3/4.13.0-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: cuDNN/9.5.0.50-CUDA-12.4.0\n",
      "The directory listed in your path is found to be non-existent: UCX/1.15.0-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: GDRCopy/2.4-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: UCX-CUDA/1.15.0-GCCcore-13.2.0-CUDA-12.4.0\n",
      "The directory listed in your path is found to be non-existent: magma/2.7.2-foss-2023b-CUDA-12.4.0\n",
      "The directory listed in your path is found to be non-existent: NCCL/2.20.5-GCCcore-13.2.0-CUDA-12.4.0\n",
      "The directory listed in your path is found to be non-existent: PyTorch/2.6.0-foss-2023b-CUDA-12.4.0\n",
      "The directory listed in your path is found to be non-existent: Triton/3.1.1-foss-2023b-CUDA-12.4.0\n",
      "The directory listed in your path is found to be non-existent: xformers/0.0.29.post3-foss-2023b-CUDA-12.4.0\n",
      "The directory listed in your path is found to be non-existent: giflib/5.2.1-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: libwebp/1.3.2-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: OpenJPEG/2.5.0-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: LittleCMS/2.15-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: Pillow-SIMD/10.4.0-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: torchvision/0.21.0-foss-2023b-CUDA-12.4.0\n",
      "The directory listed in your path is found to be non-existent: OpenPGM/5.2.122-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: libsodium/1.0.19-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: ZeroMQ/4.3.5-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: libxslt/1.1.38-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: lxml/4.9.3-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: hatchling/1.18.0-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: BeautifulSoup/4.12.2-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: jedi/0.19.1-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: IPython/8.17.2-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: PyZMQ/25.1.2-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: tornado/6.4-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: jupyter-server/2.14.0-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: JupyterLab/4.2.0-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: JupyterNotebook/7.2.0-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: typing-extensions/4.11.0-GCCcore-13.2.0\n",
      "The directory listed in your path is found to be non-existent: //matplotlib_inline.backend_inline\n",
      "The directory listed in your path is found to be non-existent: /etc/ssl/certs/ca-bundle.crt\n",
      "The directory listed in your path is found to be non-existent: /run/user/498831/vscode-ipc-f735b479-6023-421f-919e-d4e29baecccf.sock\n",
      "Found duplicate CUDA runtime files (see below).\n",
      "\n",
      "We select the PyTorch default CUDA runtime, which is 12.4,\n",
      "but this might mismatch with the CUDA version that is needed for bitsandbytes.\n",
      "To override this behavior set the `BNB_CUDA_VERSION=<version string, e.g. 122>` environmental variable.\n",
      "\n",
      "For example, if you want to use the CUDA version 122,\n",
      "    BNB_CUDA_VERSION=122 python ...\n",
      "\n",
      "OR set the environmental variable in your .bashrc:\n",
      "    export BNB_CUDA_VERSION=122\n",
      "\n",
      "In the case of a manual override, make sure you set LD_LIBRARY_PATH, e.g.\n",
      "export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.2,\n",
      "* Found CUDA runtime at: /mnt/appl/software/CUDA/12.4.0/lib/libcudart.so.12.4.99\n",
      "* Found CUDA runtime at: /mnt/appl/software/CUDA/12.4.0/lib/libcudart.so.12\n",
      "* Found CUDA runtime at: /mnt/appl/software/CUDA/12.4.0/lib/libcudart.so\n",
      "* Found CUDA runtime at: /mnt/appl/software/CUDA/12.4.0/lib/libcudart.so.12.4.99\n",
      "* Found CUDA runtime at: /mnt/appl/software/CUDA/12.4.0/lib/libcudart.so.12\n",
      "* Found CUDA runtime at: /mnt/appl/software/CUDA/12.4.0/lib/libcudart.so\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++++ DEBUG INFO END ++++++++++++++++++++++\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Checking that the library is importable and CUDA is callable...\n",
      "SUCCESS!\n",
      "Installation was successful!\n"
     ]
    }
   ],
   "source": [
    "!python -m bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/personal/mlynatom/.cache/huggingface\n"
     ]
    }
   ],
   "source": [
    "!echo $HF_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI5MzRjMjcyMS03YTdjLTQ4ZDgtODMzYS1lZmJjMTEyOTA2NTcifQ==\n"
     ]
    }
   ],
   "source": [
    "!echo $NEPTUNE_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "\n",
    "from unsloth import UnslothTrainingArguments, UnslothTrainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "\n",
    "# random state\n",
    "SEED = 42\n",
    "\n",
    "# reproducibility\n",
    "## torch\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "## python\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "## numpy\n",
    "import numpy as np\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.2.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.254 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "#model_id = \"meta-llama/Llama-3.2-3B\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoftQConfig\n",
    "\n",
    "\"\"\"\n",
    "    This is the sub-configuration class to store the configuration of a [`LoraModel`].\n",
    "\n",
    "    Args:\n",
    "        bits_pattern (`dict`): The mapping from layer names or regexp expression to bits which are different from the\n",
    "            default bits specified by `bits`. For example, `{model.decoder.layers.0.encoder_attn.k_proj: 2`}.\n",
    "        bits (`int`): Quantization bits for LoftQ.\n",
    "        iter (`int`): Alternating iterations for LoftQ.\n",
    "        fake (`bool`): True: use fp16/fp32; used for first time to save weights. False: use bitsandbytes 4bit linear\n",
    "            models. weights can't be saved. Recommend to set to True, save the weights and load the saved weights in 4\n",
    "            bits.\n",
    "    \"\"\"\n",
    "\n",
    "loftq_config = LoftQConfig(loftq_bits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading input_embeddings to disk to save VRAM\n",
      "Unsloth: Offloading output_embeddings to disk to save VRAM\n",
      "Unsloth: Training embed_tokens in mixed precision to save VRAM\n",
      "Unsloth: Training lm_head in mixed precision to save VRAM\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 256, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "                      \"embed_tokens\", \"lm_head\"], #now also embeddings and lm_head\n",
    "    lora_alpha = 1,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", #\"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = SEED,\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    # init_lora_weights = \"loftq\",\n",
    "    # loftq_config = loftq_config, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# # this dataset has already fixed encoding using ftfy (as is used by me in the preprocessing steps of other datasets)\n",
    "dataset_cs = load_dataset(\"HuggingFaceFW/fineweb-2\", \"ces_Latn\", split=\"train\", streaming=True)\n",
    "#we need only texts\n",
    "dataset_cs = dataset_cs.remove_columns([\"id\", \"dump\", \"url\", \"date\", \"file_path\", \"language\", \"language_score\", \"language_script\", \"minhash_cluster_size\", \"top_langs\"])\n",
    "#shuffle to be sure we select \"random sample\"\n",
    "dataset_cs = dataset_cs.shuffle(seed=42)\n",
    "dataset_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cs = dataset_cs.take(range(19531250))\n",
    "dataset_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_torch = dataset_cs.with_format(\"torch\")\n",
    "dataset_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "19531250/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cs = dataset_cs.take(19531250)\n",
    "dataset_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=10000000000/512\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_all_tokens = len(dataset_cs)*512 #number of tokens when using 512 tokens per data point\n",
    "num_all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of tokens per batch (processed in one step)\n",
    "global_batch_size = 1024\n",
    "\n",
    "tokens_per_step = global_batch_size*max_seq_length\n",
    "tokens_per_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of steps to process 10 B tokens\n",
    "num_steps = 10e9/tokens_per_step\n",
    "num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine step number when len of dataset is not available when streaming\n",
    "len(dataset_cs) / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_en = load_dataset(\"HuggingFaceFW/fineweb-edu\", \"sample-10BT\", split=\"train\")\n",
    "# dataset_en\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_en = dataset_en.remove_columns([\"id\", \"dump\", \"url\", \"file_path\", \"language\", \"language_score\", \"token_count\", \"score\", \"int_score\"])\n",
    "# dataset_en.shuffle(seed=42)\n",
    "# dataset_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import concatenate_datasets\n",
    "# #select 40000 samples (for the initial exp.)\n",
    "\n",
    "# dataset_cs = dataset_cs.select(range(30000))\n",
    "# dataset_en = dataset_en.select(range(10000))\n",
    "\n",
    "# dataset = concatenate_datasets([dataset_cs, dataset_en])\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.save_to_disk(\"data/pretraining/init_31mix_cs-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_from_disk\n",
    "# dataset = load_from_disk(\"data/pretraining/init_31mix_cs-en\")\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return {\"text\": [example + tokenizer.eos_token for example in examples[\"text\"]]}\n",
    "\n",
    "dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for row in dataset[:5][\"text\"]:\n",
    "#     print(\"=========================\")\n",
    "#     print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Prepared Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6e4f385d0b4c43b6ed5dc3a9b65ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function NeptuneCallback.__del__ at 0x7f50769c0400>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mlynatom/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/transformers/integrations/integration_utils.py\", line 1661, in __del__\n",
      "    if self._volatile_checkpoints_dir is not None:\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'NeptuneCallback' object has no attribute '_volatile_checkpoints_dir'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk, load_dataset\n",
    "\n",
    "dataset = load_from_disk(\"/mnt/personal/mlynatom/data/pretraining/fineweb-2_ces_Latn_19531250_llama_preprocessed\")\n",
    "#dataset = dataset.take(500000)\n",
    "dataset = dataset.take(1000)\n",
    "# dataset = dataset.to_iterable_dataset()\n",
    "# dataset = dataset.with_format(\"torch\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "855"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(dataset[4886][\"text\"])[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(945.159, 1421.484906609634, 14821, 91)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compute on sample average and std of tokenized text lenghts\n",
    "import numpy as np\n",
    "\n",
    "lengths = []\n",
    "for i in range(1000):\n",
    "    tokenized = tokenizer(dataset[i][\"text\"])[\"input_ids\"]\n",
    "    lengths.append(len(tokenized))\n",
    "\n",
    "np.mean(lengths), np.std(lengths), np.max(lengths), np.min(lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([211., 179., 124., 102.,  90.,  40.,  34.,  29.,  17.,  15.,  17.,\n",
       "         18.,  19.,  14.,   7.,  11.,   5.,   3.,   5.,  10.,   6.,   1.,\n",
       "          0.,   4.,   2.,   3.,   1.,   3.,   2.,   3.,   0.,   1.,   2.,\n",
       "          1.,   0.,   1.,   2.,   2.,   2.,   0.,   0.,   0.,   1.,   1.,\n",
       "          0.,   0.,   1.,   0.,   0.,   0.,   0.,   1.,   0.,   1.,   0.,\n",
       "          0.,   0.,   1.,   1.,   0.,   1.,   1.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,   1.,\n",
       "          0.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          1.]),\n",
       " array([   91. ,   238.3,   385.6,   532.9,   680.2,   827.5,   974.8,\n",
       "         1122.1,  1269.4,  1416.7,  1564. ,  1711.3,  1858.6,  2005.9,\n",
       "         2153.2,  2300.5,  2447.8,  2595.1,  2742.4,  2889.7,  3037. ,\n",
       "         3184.3,  3331.6,  3478.9,  3626.2,  3773.5,  3920.8,  4068.1,\n",
       "         4215.4,  4362.7,  4510. ,  4657.3,  4804.6,  4951.9,  5099.2,\n",
       "         5246.5,  5393.8,  5541.1,  5688.4,  5835.7,  5983. ,  6130.3,\n",
       "         6277.6,  6424.9,  6572.2,  6719.5,  6866.8,  7014.1,  7161.4,\n",
       "         7308.7,  7456. ,  7603.3,  7750.6,  7897.9,  8045.2,  8192.5,\n",
       "         8339.8,  8487.1,  8634.4,  8781.7,  8929. ,  9076.3,  9223.6,\n",
       "         9370.9,  9518.2,  9665.5,  9812.8,  9960.1, 10107.4, 10254.7,\n",
       "        10402. , 10549.3, 10696.6, 10843.9, 10991.2, 11138.5, 11285.8,\n",
       "        11433.1, 11580.4, 11727.7, 11875. , 12022.3, 12169.6, 12316.9,\n",
       "        12464.2, 12611.5, 12758.8, 12906.1, 13053.4, 13200.7, 13348. ,\n",
       "        13495.3, 13642.6, 13789.9, 13937.2, 14084.5, 14231.8, 14379.1,\n",
       "        14526.4, 14673.7, 14821. ]),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAKVFJREFUeJzt3X1wVFWe//FPh5AGJJ2QQNJEEp5UQJ7kQWJGxoElSwgsDmtmHJjIgFIgbkAhs4ipQRR3Z5NBVxldhJ2tAZwaEKUKcEQHi2dkDCDBiCCTAeRJIcGVTRqCNIGc3x9T3J9NApLQTU6a96vqVuXec/r0+TYh/anT9952GWOMAAAALBLR0BMAAAC4EgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGCdyIaeQH1UV1frxIkTio6OlsvlaujpAACA62CM0ZkzZ5SUlKSIiGuvkTTKgHLixAklJyc39DQAAEA9HD9+XO3atbtmn0YZUKKjoyX9vUCPx9PAswEAANfD5/MpOTnZeR+/lkYZUC5/rOPxeAgoAAA0MtdzegYnyQIAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYJ7KhJ2CjDs+8F7B/pGBEA80EAIBbEysoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxTp4CSn5+ve++9V9HR0UpISNCoUaNUUlIS0Of8+fPKyclRfHy8WrZsqaysLJWVlQX0OXbsmEaMGKEWLVooISFBM2bM0MWLF2+8GgAAEBbqFFC2bNminJwcbd++XevWrVNVVZWGDh2qyspKp8/06dP17rvvasWKFdqyZYtOnDihhx56yGm/dOmSRowYoQsXLuijjz7SG2+8oSVLlmj27NnBqwoAADRqLmOMqe+Dv/76ayUkJGjLli164IEHVFFRoTZt2mjZsmX6yU9+Ikn661//qm7duqmwsFD33Xef/vznP+uf/umfdOLECSUmJkqSFi5cqJkzZ+rrr79WVFTU9z6vz+dTTEyMKioq5PF46jv9q+rwzHsB+0cKRgT9OQAAuNXU5f37hs5BqaiokCTFxcVJkoqKilRVVaX09HSnT9euXZWSkqLCwkJJUmFhoXr27OmEE0nKyMiQz+fTvn37bmQ6AAAgTETW94HV1dWaNm2a7r//fvXo0UOSVFpaqqioKMXGxgb0TUxMVGlpqdPnu+Hkcvvlttr4/X75/X5n3+fz1XfaAACgEaj3CkpOTo727t2r5cuXB3M+tcrPz1dMTIyzJScnh/w5AQBAw6lXQJkyZYrWrFmjTZs2qV27ds5xr9erCxcuqLy8PKB/WVmZvF6v0+fKq3ou71/uc6W8vDxVVFQ42/Hjx+szbQAA0EjUKaAYYzRlyhStWrVKGzduVMeOHQPa+/Xrp6ZNm2rDhg3OsZKSEh07dkxpaWmSpLS0NH322Wc6deqU02fdunXyeDy6++67a31et9stj8cTsAEAgPBVp3NQcnJytGzZMr3zzjuKjo52zhmJiYlR8+bNFRMTowkTJig3N1dxcXHyeDyaOnWq0tLSdN9990mShg4dqrvvvltjx47V3LlzVVpaqlmzZiknJ0dutzv4FQIAgEanTgFlwYIFkqRBgwYFHF+8eLHGjx8vSXrllVcUERGhrKws+f1+ZWRk6PXXX3f6NmnSRGvWrNETTzyhtLQ03XbbbRo3bpxeeOGFG6sEAACEjRu6D0pD4T4oAAA0PjftPigAAAChQEABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6kQ09gcagwzPv1Th2pGBEA8wEAIBbAysoAADAOnUOKFu3btXIkSOVlJQkl8ul1atXB7S7XK5atxdffNHp06FDhxrtBQUFN1wMAAAID3UOKJWVlerdu7fmz59fa/vJkycDtkWLFsnlcikrKyug3wsvvBDQb+rUqfWrAAAAhJ06n4OSmZmpzMzMq7Z7vd6A/XfeeUeDBw9Wp06dAo5HR0fX6AsAACCF+ByUsrIyvffee5owYUKNtoKCAsXHx6tPnz568cUXdfHixauO4/f75fP5AjYAABC+QnoVzxtvvKHo6Gg99NBDAceffPJJ9e3bV3Fxcfroo4+Ul5enkydP6uWXX651nPz8fM2ZMyeUUwUAABYJaUBZtGiRsrOz1axZs4Djubm5zs+9evVSVFSUHn/8ceXn58vtdtcYJy8vL+AxPp9PycnJoZs4AABoUCELKB9++KFKSkr01ltvfW/f1NRUXbx4UUeOHFGXLl1qtLvd7lqDCwAACE8hOwfl97//vfr166fevXt/b9/i4mJFREQoISEhVNMBAACNSJ1XUM6ePauDBw86+4cPH1ZxcbHi4uKUkpIi6e8fwaxYsUL/+Z//WePxhYWF2rFjhwYPHqzo6GgVFhZq+vTpeuSRR9SqVasbKAUAAISLOgeUXbt2afDgwc7+5XNDxo0bpyVLlkiSli9fLmOMxowZU+Pxbrdby5cv1/PPPy+/36+OHTtq+vTpAeeYAACAW5vLGGMaehJ15fP5FBMTo4qKCnk8nqCPX9t371yJ7+IBAKBu6vL+zXfxAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsU+eAsnXrVo0cOVJJSUlyuVxavXp1QPv48ePlcrkCtmHDhgX0OX36tLKzs+XxeBQbG6sJEybo7NmzN1QIAAAIH3UOKJWVlerdu7fmz59/1T7Dhg3TyZMnne3NN98MaM/Ozta+ffu0bt06rVmzRlu3btWkSZPqPnsAABCWIuv6gMzMTGVmZl6zj9vtltfrrbVt//79Wrt2rT7++GP1799fkvTaa69p+PDheumll5SUlFTXKQEAgDATknNQNm/erISEBHXp0kVPPPGEvvnmG6etsLBQsbGxTjiRpPT0dEVERGjHjh21juf3++Xz+QI2AAAQvoIeUIYNG6Y//OEP2rBhg37zm99oy5YtyszM1KVLlyRJpaWlSkhICHhMZGSk4uLiVFpaWuuY+fn5iomJcbbk5ORgTxsAAFikzh/xfJ/Ro0c7P/fs2VO9evVS586dtXnzZg0ZMqReY+bl5Sk3N9fZ9/l8hBQAAMJYyC8z7tSpk1q3bq2DBw9Kkrxer06dOhXQ5+LFizp9+vRVz1txu93yeDwBGwAACF8hDyhffvmlvvnmG7Vt21aSlJaWpvLychUVFTl9Nm7cqOrqaqWmpoZ6OgAAoBGo80c8Z8+edVZDJOnw4cMqLi5WXFyc4uLiNGfOHGVlZcnr9erQoUN6+umndccddygjI0OS1K1bNw0bNkwTJ07UwoULVVVVpSlTpmj06NFcwQMAACTVYwVl165d6tOnj/r06SNJys3NVZ8+fTR79mw1adJEe/bs0YMPPqi77rpLEyZMUL9+/fThhx/K7XY7YyxdulRdu3bVkCFDNHz4cA0cOFC/+93vglcVAABo1Oq8gjJo0CAZY67a/sEHH3zvGHFxcVq2bFldnxoAANwi+C4eAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB16hxQtm7dqpEjRyopKUkul0urV6922qqqqjRz5kz17NlTt912m5KSkvSLX/xCJ06cCBijQ4cOcrlcAVtBQcENFwMAAMJDnQNKZWWlevfurfnz59doO3funHbv3q1nn31Wu3fv1sqVK1VSUqIHH3ywRt8XXnhBJ0+edLapU6fWrwIAABB2Iuv6gMzMTGVmZtbaFhMTo3Xr1gUc+6//+i8NGDBAx44dU0pKinM8OjpaXq+3rk8PAABuASE/B6WiokIul0uxsbEBxwsKChQfH68+ffroxRdf1MWLF686ht/vl8/nC9gAAED4qvMKSl2cP39eM2fO1JgxY+TxeJzjTz75pPr27au4uDh99NFHysvL08mTJ/Xyyy/XOk5+fr7mzJkTyqkCAACLuIwxpt4Pdrm0atUqjRo1qkZbVVWVsrKy9OWXX2rz5s0BAeVKixYt0uOPP66zZ8/K7XbXaPf7/fL7/c6+z+dTcnKyKioqrjlufXV45r06P+ZIwYigzwMAgHDi8/kUExNzXe/fIVlBqaqq0sMPP6yjR49q48aN3zuJ1NRUXbx4UUeOHFGXLl1qtLvd7lqDCwAACE9BDyiXw8mBAwe0adMmxcfHf+9jiouLFRERoYSEhGBPBwAANEJ1Dihnz57VwYMHnf3Dhw+ruLhYcXFxatu2rX7yk59o9+7dWrNmjS5duqTS0lJJUlxcnKKiolRYWKgdO3Zo8ODBio6OVmFhoaZPn65HHnlErVq1Cl5lAACg0apzQNm1a5cGDx7s7Ofm5kqSxo0bp+eff15/+tOfJEn33HNPwOM2bdqkQYMGye12a/ny5Xr++efl9/vVsWNHTZ8+3RkHAACgzgFl0KBButZ5td93zm3fvn21ffv2uj4tAAC4hfBdPAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1qlzQNm6datGjhyppKQkuVwurV69OqDdGKPZs2erbdu2at68udLT03XgwIGAPqdPn1Z2drY8Ho9iY2M1YcIEnT179oYKAQAA4aPOAaWyslK9e/fW/Pnza22fO3euXn31VS1cuFA7duzQbbfdpoyMDJ0/f97pk52drX379mndunVas2aNtm7dqkmTJtW/Cgt0eOa9GhsAAKifyLo+IDMzU5mZmbW2GWM0b948zZo1Sz/+8Y8lSX/4wx+UmJio1atXa/To0dq/f7/Wrl2rjz/+WP3795ckvfbaaxo+fLheeuklJSUl3UA5AAAgHAT1HJTDhw+rtLRU6enpzrGYmBilpqaqsLBQklRYWKjY2FgnnEhSenq6IiIitGPHjlrH9fv98vl8ARsAAAhfQQ0opaWlkqTExMSA44mJiU5baWmpEhISAtojIyMVFxfn9LlSfn6+YmJinC05OTmY0wYAAJZpFFfx5OXlqaKiwtmOHz/e0FMCAAAhFNSA4vV6JUllZWUBx8vKypw2r9erU6dOBbRfvHhRp0+fdvpcye12y+PxBGwAACB8BTWgdOzYUV6vVxs2bHCO+Xw+7dixQ2lpaZKktLQ0lZeXq6ioyOmzceNGVVdXKzU1NZjTAQAAjVSdr+I5e/asDh486OwfPnxYxcXFiouLU0pKiqZNm6Z///d/15133qmOHTvq2WefVVJSkkaNGiVJ6tatm4YNG6aJEydq4cKFqqqq0pQpUzR69Giu4AEAAJLqEVB27dqlwYMHO/u5ubmSpHHjxmnJkiV6+umnVVlZqUmTJqm8vFwDBw7U2rVr1axZM+cxS5cu1ZQpUzRkyBBFREQoKytLr776ahDKAQAA4cBljDENPYm68vl8iomJUUVFRUjORwnWTdaOFIwIyjgAAISDurx/N4qreAAAwK2FgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArFPn7+LB9bvylvnc+h4AgOvDCgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWCXpA6dChg1wuV40tJydHkjRo0KAabZMnTw72NAAAQCMWGewBP/74Y126dMnZ37t3r/7xH/9RP/3pT51jEydO1AsvvODst2jRItjTAAAAjVjQA0qbNm0C9gsKCtS5c2f96Ec/co61aNFCXq832E8NAADCREjPQblw4YL++Mc/6rHHHpPL5XKOL126VK1bt1aPHj2Ul5enc+fOXXMcv98vn88XsAEAgPAV9BWU71q9erXKy8s1fvx459jPf/5ztW/fXklJSdqzZ49mzpypkpISrVy58qrj5Ofna86cOaGcKgAAsIjLGGNCNXhGRoaioqL07rvvXrXPxo0bNWTIEB08eFCdO3eutY/f75ff73f2fT6fkpOTVVFRIY/HE/R5d3jmvaCPKUlHCkaEZFwAABoDn8+nmJiY63r/DtkKytGjR7V+/fprroxIUmpqqiRdM6C43W653e6gzxEAANgpZOegLF68WAkJCRox4tqrBsXFxZKktm3bhmoqAACgkQnJCkp1dbUWL16scePGKTLy/z/FoUOHtGzZMg0fPlzx8fHas2ePpk+frgceeEC9evUKxVQAAEAjFJKAsn79eh07dkyPPfZYwPGoqCitX79e8+bNU2VlpZKTk5WVlaVZs2aFYhoAAKCRCklAGTp0qGo79zY5OVlbtmwJxVMCAIAwwnfxAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsQ0ABAADWIaAAAADrEFAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1iGgAAAA6xBQAACAdQgoAADAOgQUAABgHQIKAACwDgEFAABYh4ACAACsE/SA8vzzz8vlcgVsXbt2ddrPnz+vnJwcxcfHq2XLlsrKylJZWVmwpwEAABqxkKygdO/eXSdPnnS2bdu2OW3Tp0/Xu+++qxUrVmjLli06ceKEHnrooVBMAwAANFKRIRk0MlJer7fG8YqKCv3+97/XsmXL9A//8A+SpMWLF6tbt27avn277rvvvlBMBwAANDIhWUE5cOCAkpKS1KlTJ2VnZ+vYsWOSpKKiIlVVVSk9Pd3p27VrV6WkpKiwsPCq4/n9fvl8voANAACEr6AHlNTUVC1ZskRr167VggULdPjwYf3whz/UmTNnVFpaqqioKMXGxgY8JjExUaWlpVcdMz8/XzExMc6WnJwc7GkDAACLBP0jnszMTOfnXr16KTU1Ve3bt9fbb7+t5s2b12vMvLw85ebmOvs+n4+QAgBAGAv5ZcaxsbG66667dPDgQXm9Xl24cEHl5eUBfcrKymo9Z+Uyt9stj8cTsAEAgPAV8oBy9uxZHTp0SG3btlW/fv3UtGlTbdiwwWkvKSnRsWPHlJaWFuqpAACARiLoH/H867/+q0aOHKn27dvrxIkTeu6559SkSRONGTNGMTExmjBhgnJzcxUXFyePx6OpU6cqLS2NK3gAAIAj6AHlyy+/1JgxY/TNN9+oTZs2GjhwoLZv3642bdpIkl555RVFREQoKytLfr9fGRkZev3114M9DQAA0Ii5jDGmoSdRVz6fTzExMaqoqAjJ+Sgdnnkv6GNK0pGCESEZFwCAxqAu7998Fw8AALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWCfo38WDq6vtFvrc/h4AgJpYQQEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIeAAgAArENAAQAA1uFOsg3syrvLcmdZAABYQQEAABYioAAAAOsQUAAAgHUIKAAAwDoEFAAAYB0CCgAAsA4BBQAAWIf7oFjmyvuiSNwbBQBw62EFBQAAWCfoASU/P1/33nuvoqOjlZCQoFGjRqmkpCSgz6BBg+RyuQK2yZMnB3sqAACgkQp6QNmyZYtycnK0fft2rVu3TlVVVRo6dKgqKysD+k2cOFEnT550trlz5wZ7KgAAoJEK+jkoa9euDdhfsmSJEhISVFRUpAceeMA53qJFC3m93mA/PQAACAMhPweloqJCkhQXFxdwfOnSpWrdurV69OihvLw8nTt3LtRTAQAAjURIr+Kprq7WtGnTdP/996tHjx7O8Z///Odq3769kpKStGfPHs2cOVMlJSVauXJlreP4/X75/X5n3+fzhXLaAACggYU0oOTk5Gjv3r3atm1bwPFJkyY5P/fs2VNt27bVkCFDdOjQIXXu3LnGOPn5+ZozZ04opwoAACwSso94pkyZojVr1mjTpk1q167dNfumpqZKkg4ePFhre15enioqKpzt+PHjQZ8vAACwR9BXUIwxmjp1qlatWqXNmzerY8eO3/uY4uJiSVLbtm1rbXe73XK73cGcJgAAsFjQA0pOTo6WLVumd955R9HR0SotLZUkxcTEqHnz5jp06JCWLVum4cOHKz4+Xnv27NH06dP1wAMPqFevXsGeDgAAaISCHlAWLFgg6e83Y/uuxYsXa/z48YqKitL69es1b948VVZWKjk5WVlZWZo1a1awpwIAABqpkHzEcy3JycnasmVLsJ8WAACEEb6LBwAAWIeAAgAArENAAQAA1iGgAAAA64T0TrKwS4dn3gvYP1IwooFmAgDAtRFQwtSVYQQAgMaEj3gAAIB1WEFpBK7noxlWTAAA4YQVFAAAYB0CCgAAsA4f8TRCfJwDAAh3rKAAAADrsIJyC6ttJYZ7owAAbMAKCgAAsA4BBQAAWIeAAgAArENAAQAA1uEkWVwTJ9ICABoCKygAAMA6rKDgpmAlBgBQF6ygAAAA6xBQAACAdQgoAADAOgQUAABgHU6SRQC+KRkAYAMCChrMlWGIq3oAAJfxEQ8AALAOKyios+tZ+eCjIgDAjWAFBQAAWIcVFDQq3JEWAG4NBBTcMNs+zrmeEBOsPgCA0GjQj3jmz5+vDh06qFmzZkpNTdXOnTsbcjoAAMASDbaC8tZbbyk3N1cLFy5Uamqq5s2bp4yMDJWUlCghIaGhpoVGKFgrOKEap76rLvWZDys8AMJFg62gvPzyy5o4caIeffRR3X333Vq4cKFatGihRYsWNdSUAACAJRpkBeXChQsqKipSXl6ecywiIkLp6ekqLCys0d/v98vv9zv7FRUVkiSfzxeS+VX7z4VkXFxbyvQVNY7tnZMRsF/ff5vaxq6PK3/nejz3Qb2e+8q6rmec61Hf/xPX8/xXzvl6x7mex12PK8eubdzr6VOf56pNff4Ng/VahKtQ/v7U5/nr+zsWrN/Dm+lmvfaX/0YZY76/s2kAX331lZFkPvroo4DjM2bMMAMGDKjR/7nnnjOS2NjY2NjY2MJgO378+PdmhUZxFU9eXp5yc3Od/erqap0+fVrx8fFyuVw3PL7P51NycrKOHz8uj8dzw+M1BtR8a9Qs3Zp1UzM1h6vGXrMxRmfOnFFSUtL39m2QgNK6dWs1adJEZWVlAcfLysrk9Xpr9He73XK73QHHYmNjgz4vj8fTKP/BbwQ13zpuxbqp+dZAzY1LTEzMdfVrkJNko6Ki1K9fP23YsME5Vl1drQ0bNigtLa0hpgQAACzSYB/x5Obmaty4cerfv78GDBigefPmqbKyUo8++mhDTQkAAFiiwQLKz372M3399deaPXu2SktLdc8992jt2rVKTEy86XNxu9167rnnanyMFM6o+dZxK9ZNzbcGag5vLmOu51ofAACAm4dvMwYAANYhoAAAAOsQUAAAgHUIKAAAwDq3fECZP3++OnTooGbNmik1NVU7d+5s6Cldt/z8fN17772Kjo5WQkKCRo0apZKSkoA+58+fV05OjuLj49WyZUtlZWXVuEHesWPHNGLECLVo0UIJCQmaMWOGLl68GNBn8+bN6tu3r9xut+644w4tWbIk1OV9r4KCArlcLk2bNs05Fq71fvXVV3rkkUcUHx+v5s2bq2fPntq1a5fTbozR7Nmz1bZtWzVv3lzp6ek6cOBAwBinT59Wdna2PB6PYmNjNWHCBJ09ezagz549e/TDH/5QzZo1U3JysubOnXtT6rvSpUuX9Oyzz6pjx45q3ry5OnfurH/7t38L+P6OcKh569atGjlypJKSkuRyubR69eqA9ptZ44oVK9S1a1c1a9ZMPXv21Pvvvx/0eqVr11xVVaWZM2eqZ8+euu2225SUlKRf/OIXOnHiRNjWfKXJkyfL5XJp3rx5AccbW81BcePfrNN4LV++3ERFRZlFixaZffv2mYkTJ5rY2FhTVlbW0FO7LhkZGWbx4sVm7969pri42AwfPtykpKSYs2fPOn0mT55skpOTzYYNG8yuXbvMfffdZ37wgx847RcvXjQ9evQw6enp5pNPPjHvv/++ad26tcnLy3P6fPHFF6ZFixYmNzfXfP755+a1114zTZo0MWvXrr2p9X7Xzp07TYcOHUyvXr3MU0895RwPx3pPnz5t2rdvb8aPH2927NhhvvjiC/PBBx+YgwcPOn0KCgpMTEyMWb16tfn000/Ngw8+aDp27Gi+/fZbp8+wYcNM7969zfbt282HH35o7rjjDjNmzBinvaKiwiQmJprs7Gyzd+9e8+abb5rmzZub//7v/76p9RpjzK9//WsTHx9v1qxZYw4fPmxWrFhhWrZsaX772986fcKh5vfff9/86le/MitXrjSSzKpVqwLab1aNf/nLX0yTJk3M3Llzzeeff25mzZplmjZtaj777LObWnN5eblJT083b731lvnrX/9qCgsLzYABA0y/fv0Cxginmr9r5cqVpnfv3iYpKcm88sorjbrmYLilA8qAAQNMTk6Os3/p0iWTlJRk8vPzG3BW9Xfq1CkjyWzZssUY8/f/7E2bNjUrVqxw+uzfv99IMoWFhcaYv//HiYiIMKWlpU6fBQsWGI/HY/x+vzHGmKefftp079494Ll+9rOfmYyMjFCXVKszZ86YO++806xbt8786Ec/cgJKuNY7c+ZMM3DgwKu2V1dXG6/Xa1588UXnWHl5uXG73ebNN980xhjz+eefG0nm448/dvr8+c9/Ni6Xy3z11VfGGGNef/1106pVK+d1uPzcXbp0CXZJ32vEiBHmscceCzj20EMPmezsbGNMeNZ85RvXzazx4YcfNiNGjAiYT2pqqnn88ceDWuOVrvVmfdnOnTuNJHP06FFjTPjW/OWXX5rbb7/d7N2717Rv3z4goDT2muvrlv2I58KFCyoqKlJ6erpzLCIiQunp6SosLGzAmdVfRUWFJCkuLk6SVFRUpKqqqoAau3btqpSUFKfGwsJC9ezZM+AGeRkZGfL5fNq3b5/T57tjXO7TUK9TTk6ORowYUWNO4Vrvn/70J/Xv318//elPlZCQoD59+uh//ud/nPbDhw+rtLQ0YM4xMTFKTU0NqDs2Nlb9+/d3+qSnpysiIkI7duxw+jzwwAOKiopy+mRkZKikpET/93//F+oyA/zgBz/Qhg0b9Le//U2S9Omnn2rbtm3KzMyUFJ41X+lm1mjb7/x3VVRUyOVyOd+/Fo41V1dXa+zYsZoxY4a6d+9eoz0ca74et2xA+d///V9dunSpxp1rExMTVVpa2kCzqr/q6mpNmzZN999/v3r06CFJKi0tVVRUVI0vVvxujaWlpbW+BpfbrtXH5/Pp22+/DUU5V7V8+XLt3r1b+fn5NdrCsV5J+uKLL7RgwQLdeeed+uCDD/TEE0/oySef1BtvvBEw72v9LpeWliohISGgPTIyUnFxcXV6bW6WZ555RqNHj1bXrl3VtGlT9enTR9OmTVN2dnbAfMKp5ivdzBqv1qehX4Pz589r5syZGjNmjPPFeOFY829+8xtFRkbqySefrLU9HGu+Hg12q3sEV05Ojvbu3att27Y19FRC5vjx43rqqae0bt06NWvWrKGnc9NUV1erf//++o//+A9JUp8+fbR3714tXLhQ48aNa+DZhcbbb7+tpUuXatmyZerevbuKi4s1bdo0JSUlhW3NCFRVVaWHH35YxhgtWLCgoacTMkVFRfrtb3+r3bt3y+VyNfR0rHLLrqC0bt1aTZo0qXGFR1lZmbxebwPNqn6mTJmiNWvWaNOmTWrXrp1z3Ov16sKFCyovLw/o/90avV5vra/B5bZr9fF4PGrevHmwy7mqoqIinTp1Sn379lVkZKQiIyO1ZcsWvfrqq4qMjFRiYmJY1XtZ27Ztdffddwcc69atm44dOybp/8/7Wr/LXq9Xp06dCmi/ePGiTp8+XafX5maZMWOGs4rSs2dPjR07VtOnT3dWzsKx5ivdzBqv1qehXoPL4eTo0aNat26ds3oihV/NH374oU6dOqWUlBTn79rRo0f1y1/+Uh06dHDmGk41X69bNqBERUWpX79+2rBhg3OsurpaGzZsUFpaWgPO7PoZYzRlyhStWrVKGzduVMeOHQPa+/Xrp6ZNmwbUWFJSomPHjjk1pqWl6bPPPgv45b/8B+Hym2JaWlrAGJf73OzXaciQIfrss89UXFzsbP3791d2drbzczjVe9n9999f4/Lxv/3tb2rfvr0kqWPHjvJ6vQFz9vl82rFjR0Dd5eXlKioqcvps3LhR1dXVSk1Ndfps3bpVVVVVTp9169apS5cuatWqVcjqq825c+cUERH456lJkyaqrq6WFJ41X+lm1mjT7/zlcHLgwAGtX79e8fHxAe3hVvPYsWO1Z8+egL9rSUlJmjFjhj744ANnruFU83Vr6LN0G9Ly5cuN2+02S5YsMZ9//rmZNGmSiY2NDbjCw2ZPPPGEiYmJMZs3bzYnT550tnPnzjl9Jk+ebFJSUszGjRvNrl27TFpamklLS3PaL192O3ToUFNcXGzWrl1r2rRpU+tltzNmzDD79+838+fPb/DLjC/77lU8xoRnvTt37jSRkZHm17/+tTlw4IBZunSpadGihfnjH//o9CkoKDCxsbHmnXfeMXv27DE//vGPa70ctU+fPmbHjh1m27Zt5s477wy4TLG8vNwkJiaasWPHmr1795rly5ebFi1aNMhlxuPGjTO33367c5nxypUrTevWrc3TTz/t9AmHms+cOWM++eQT88knnxhJ5uWXXzaffPKJc8XKzarxL3/5i4mMjDQvvfSS2b9/v3nuuedCdvnptWq+cOGCefDBB027du1McXFxwN+1716dEk411+bKq3gaY83BcEsHFGOMee2110xKSoqJiooyAwYMMNu3b2/oKV03SbVuixcvdvp8++235l/+5V9Mq1atTIsWLcw///M/m5MnTwaMc+TIEZOZmWmaN29uWrdubX75y1+aqqqqgD6bNm0y99xzj4mKijKdOnUKeI6GdGVACdd63333XdOjRw/jdrtN165dze9+97uA9urqavPss8+axMRE43a7zZAhQ0xJSUlAn2+++caMGTPGtGzZ0ng8HvPoo4+aM2fOBPT59NNPzcCBA43b7Ta33367KSgoCHlttfH5fOapp54yKSkpplmzZqZTp07mV7/6VcCbVDjUvGnTplr/D48bN84Yc3NrfPvtt81dd91loqKiTPfu3c17771302s+fPjwVf+ubdq0KSxrrk1tAaWx1RwMLmO+c2tGAAAAC9yy56AAAAB7EVAAAIB1CCgAAMA6BBQAAGAdAgoAALAOAQUAAFiHgAIAAKxDQAEAANYhoAAAAOsQUAAAgHUIKAAAwDoEFAAAYJ3/Bwncp1l+36U7AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(lengths, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "463453.3898305085"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1953125*42/177"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "500000*1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer Modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnslothKLLossTrainer(UnslothTrainer):\n",
    "    def __init__(self, ref_model, kl_weight=0.1, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.ref_model = ref_model\n",
    "        self.ref_model.eval()\n",
    "        self.kl_weight = kl_weight\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        #account for the actual number of items in the batch\n",
    "        if num_items_in_batch is not None:\n",
    "            effective_batch_size = num_items_in_batch\n",
    "        else:\n",
    "            effective_batch_size = labels.size(0)\n",
    "        \n",
    "        #cross entropy loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        loss_ce = loss_fct(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "        loss_ce = loss_ce / effective_batch_size # Scale by actual number of items\n",
    "\n",
    "        # KL loss\n",
    "        with torch.no_grad():\n",
    "            ref_outputs = self.ref_model(**inputs)\n",
    "            ref_logits = ref_outputs.logits\n",
    "        \n",
    "        loss_kl_fct = torch.nn.KLDivLoss(reduction=\"sum\")\n",
    "        loss_kl = loss_kl_fct(\n",
    "            torch.nn.functional.log_softmax(logits, dim=-1),\n",
    "            torch.nn.functional.softmax(ref_logits, dim=-1),\n",
    "        )\n",
    "        loss_kl = loss_kl / effective_batch_size # Scale by actual number of items\n",
    "\n",
    "        total_loss = loss_ce + self.kl_weight*loss_kl\n",
    "\n",
    "        return (total_loss, outputs) if return_outputs else total_loss\n",
    "\n",
    "from transformers.utils.import_utils import is_torch_xla_available\n",
    "#from transformers.trainer_utils import SaveStrategy\n",
    "from typing import Dict\n",
    "import numpy as np\n",
    "\n",
    "class PerplexityTrainer(UnslothTrainer):\n",
    "    # def training_step(self, model, inputs, num_items_in_batch):\n",
    "    #     loss = super().training_step(model, inputs, num_items_in_batch)\n",
    "\n",
    "    #     # log perplexity\n",
    "    #     perplexity = torch.exp(loss)\n",
    "    #     self.log({\"train_perplexity\": perplexity})\n",
    "\n",
    "    #     return loss\n",
    "    def _maybe_log_save_evaluate(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval):\n",
    "        if self.control.should_log and self.state.global_step > self._globalstep_last_logged:\n",
    "            if is_torch_xla_available():\n",
    "                xm.mark_step()\n",
    "\n",
    "            logs: Dict[str, float] = {}\n",
    "\n",
    "            # all_gather + mean() to get average loss over all processes\n",
    "            tr_loss_scalar = self._nested_gather(tr_loss).mean().item()\n",
    "\n",
    "            # reset tr_loss to zero\n",
    "            tr_loss -= tr_loss\n",
    "\n",
    "            loss = round(tr_loss_scalar / (self.state.global_step - self._globalstep_last_logged), 4)\n",
    "            logs[\"loss\"] = loss\n",
    "\n",
    "            #PERPLEXITY\n",
    "            logs[\"perplexity\"] = round(float(np.exp(tr_loss_scalar)), 4)\n",
    "            #########################\n",
    "\n",
    "\n",
    "            if grad_norm is not None:\n",
    "                logs[\"grad_norm\"] = grad_norm.detach().item() if isinstance(grad_norm, torch.Tensor) else grad_norm\n",
    "            logs[\"learning_rate\"] = self._get_learning_rate()\n",
    "\n",
    "            self._total_loss_scalar += tr_loss_scalar\n",
    "            self._globalstep_last_logged = self.state.global_step\n",
    "            self.store_flos()\n",
    "\n",
    "            self.log(logs)\n",
    "\n",
    "        metrics = None\n",
    "        if self.control.should_evaluate:\n",
    "            metrics = self._evaluate(trial, ignore_keys_for_eval)\n",
    "\n",
    "        if self.control.should_save:\n",
    "            self._save_checkpoint(model, metrics=metrics)\n",
    "            self.control = self.callback_handler.on_save(self.args, self.state, self.control)\n",
    "\n",
    "TrainerClass = PerplexityTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EvalPrediction\n",
    "import numpy as np\n",
    "\n",
    "#prepare perplexity computation\n",
    "def compute_metrics(pred: EvalPrediction):\n",
    "    losses = pred.losses\n",
    "\n",
    "    mean_loss = np.mean(losses)\n",
    "\n",
    "    try:\n",
    "        perplexity = np.exp(mean_loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    return {\"perplexity\": perplexity}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import numpy as np\n",
    "from peft import LoftQConfig, get_peft_model, LoraConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from typing import Optional\n",
    "from dataclasses import dataclass, field\n",
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "\n",
    "\n",
    "def _create_unsloth_optimizer(\n",
    "    model,\n",
    "    optimizer_cls,\n",
    "    optimizer_kwargs,\n",
    "    embedding_lr = 5e-5,\n",
    "):\n",
    "    lr = optimizer_kwargs[\"lr\"]\n",
    "    weight_decay = optimizer_kwargs.get(\"weight_decay\", 0.0)\n",
    "\n",
    "    param_groups = \\\n",
    "    {\n",
    "        \"non_embeddings\" : {},\n",
    "        \"embeddings\"     : {},\n",
    "    }\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad: continue\n",
    "        if name.endswith(\"modules_to_save.default.weight\"):\n",
    "            partial_name = name[:-len(\".modules_to_save.default.weight\")]\n",
    "            partial_name = partial_name[partial_name.rfind(\".\")+1:]\n",
    "            print(f\"Unsloth: Setting lr = {embedding_lr:.2e} instead of {lr:.2e} for {partial_name}.\")\n",
    "            param_groups[\"embeddings\"]    [name] = param\n",
    "        else:\n",
    "            param_groups[\"non_embeddings\"][name] = param\n",
    "        pass\n",
    "    pass\n",
    "\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\"       : list(param_groups[\"non_embeddings\"].values()),\n",
    "            \"weight_decay\" : weight_decay,\n",
    "            \"lr\"           : lr,\n",
    "        },\n",
    "        {\n",
    "            \"params\"       : list(param_groups[\"embeddings\"].values()),\n",
    "            \"weight_decay\" : weight_decay,\n",
    "            \"lr\"           : embedding_lr,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = optimizer_cls(optimizer_grouped_parameters, **optimizer_kwargs)\n",
    "    return optimizer\n",
    "\n",
    "@dataclass\n",
    "class UnslothTrainingArguments(SFTConfig):\n",
    "    embedding_learning_rate : Optional[float] = field(\n",
    "        default = None,\n",
    "        metadata = {\"help\" : \"Different learning rates for embeddings and lm_head.\"}\n",
    "    )\n",
    "\n",
    "class UnslothTrainer(SFTTrainer):\n",
    "    def create_optimizer(self):\n",
    "        embedding_learning_rate = getattr(self.args, \"embedding_learning_rate\", None)\n",
    "        if embedding_learning_rate is None: return super().create_optimizer()\n",
    "\n",
    "        if self.optimizer is None:\n",
    "            optimizer_cls, optimizer_kwargs = SFTTrainer.get_optimizer_cls_and_kwargs(self.args)\n",
    "            self.optimizer = _create_unsloth_optimizer(\n",
    "                self.model,\n",
    "                optimizer_cls,\n",
    "                optimizer_kwargs,\n",
    "                embedding_learning_rate,\n",
    "            )\n",
    "        \n",
    "        return self.optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!export WANDB_NO_VERIFY=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"NEPTUNE_PROJECT\"] = \"mlynatom/cp-exp\"  # replace with your own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11af9d2e410640c58ff9284b6b467d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = UnslothTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    \n",
    "    # ref_model=copy.deepcopy(model),\n",
    "    #compute_metrics=compute_metrics,\n",
    "    #packing=True,\n",
    "    \n",
    "    args = UnslothTrainingArguments(\n",
    "        dataset_text_field = \"text\",\n",
    "        max_seq_length = max_seq_length,\n",
    "        dataset_num_proc = 8,\n",
    "        packing=False,\n",
    "        #group_by_length = True,\n",
    "        per_device_train_batch_size = 64,\n",
    "        gradient_accumulation_steps = 16,\n",
    "        warmup_ratio = 0.05,\n",
    "        num_train_epochs = 1, # Set this for 1 full training run. - not supported for streaming -> use max_steps (here 61234=dataset_len/global_batch_size)\n",
    "        #max_steps = 10,\n",
    "        learning_rate = 5e-4,\n",
    "        embedding_learning_rate = 5e-4/2,\n",
    "        #fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = True,\n",
    "        #tf32 = True,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = SEED,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"neptune\", # Use this for WandB etc\n",
    "        run_name=\"llama3.1-8b-cp-exp-loftq_final\",\n",
    "        # eval_strategy = \"steps\",\n",
    "        # eval_steps = 50,\n",
    "        save_strategy = \"steps\",\n",
    "        save_steps = 100,\n",
    "        save_total_limit = 2,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA A100-SXM4-80GB. Max memory = 79.254 GB.\n",
      "17.877 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 500,000 | Num Epochs = 1 | Total steps = 488\n",
      "O^O/ \\_/ \\    Batch size per device = 64 | Gradient accumulation steps = 16\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (64 x 16 x 1) = 1,024\n",
      " \"-____-\"     Trainable parameters = 1,721,761,792/9,752,023,040 (17.66% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\n\u001b[1;32m      2\u001b[0m     activities\u001b[38;5;241m=\u001b[39m[torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mProfilerActivity\u001b[38;5;241m.\u001b[39mCPU, torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mProfilerActivity\u001b[38;5;241m.\u001b[39mCUDA],\n\u001b[1;32m      3\u001b[0m     profile_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m prof:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Run training step\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/unsloth_venv/lib/python3.11/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:310\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m<string>:31\u001b[0m, in \u001b[0;36m_unsloth_training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/unsloth_compiled_cache/UnslothSFTTrainer.py:754\u001b[0m, in \u001b[0;36m_UnslothSFTTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 754\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/unsloth_venv/lib/python3.11/site-packages/unsloth/models/_utils.py:1029\u001b[0m, in \u001b[0;36m_unsloth_pre_compute_loss\u001b[0;34m(self, model, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   1024\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: Not an error, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept `num_items_in_batch`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m   1025\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing gradient accumulation will be very slightly less accurate.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m   1026\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1027\u001b[0m     )\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m-> 1029\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_compute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/unsloth_venv/lib/python3.11/site-packages/transformers/trainer.py:3783\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3781\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3782\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3783\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3784\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3785\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3786\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/mnt/appl/software/PyTorch/2.6.0-foss-2023b-CUDA-12.4.0/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/appl/software/PyTorch/2.6.0-foss-2023b-CUDA-12.4.0/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/unsloth_venv/lib/python3.11/site-packages/accelerate/utils/operations.py:819\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/unsloth_venv/lib/python3.11/site-packages/accelerate/utils/operations.py:807\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/mnt/appl/software/PyTorch/2.6.0-foss-2023b-CUDA-12.4.0/lib/python3.11/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/appl/software/PyTorch/2.6.0-foss-2023b-CUDA-12.4.0/lib/python3.11/site-packages/torch/_compile.py:32\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     30\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/appl/software/PyTorch/2.6.0-foss-2023b-CUDA-12.4.0/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:745\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    741\u001b[0m prior_skip_guard_eval_unsafe \u001b[38;5;241m=\u001b[39m set_skip_guard_eval_unsafe(\n\u001b[1;32m    742\u001b[0m     _is_skip_guard_eval_unsafe_stance()\n\u001b[1;32m    743\u001b[0m )\n\u001b[1;32m    744\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    747\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/unsloth_venv/lib/python3.11/site-packages/unsloth/models/llama.py:1200\u001b[0m, in \u001b[0;36mPeftModelForCausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, num_logits_to_keep, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_dynamo\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mPeftModelForCausalLM_fast_forward\u001b[39m(\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1199\u001b[0m ):\n\u001b[0;32m-> 1200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/appl/software/PyTorch/2.6.0-foss-2023b-CUDA-12.4.0/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/appl/software/PyTorch/2.6.0-foss-2023b-CUDA-12.4.0/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/unsloth_venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:193\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/unsloth_venv/lib/python3.11/site-packages/unsloth/models/llama.py:1097\u001b[0m, in \u001b[0;36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001b[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, logits_to_keep, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m RETURN_LOGITS \u001b[38;5;129;01mand\u001b[39;00m HAS_CUT_CROSS_ENTROPY \u001b[38;5;129;01mand\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1096\u001b[0m     n_items \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_items\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1097\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mfused_linear_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlm_weight\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlm_head\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m             \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogit_softcapping\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlogit_softcapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1105\u001b[0m         output \u001b[38;5;241m=\u001b[39m (logits,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/unsloth_venv/lib/python3.11/site-packages/unsloth_zoo/loss_utils.py:157\u001b[0m, in \u001b[0;36mfused_linear_cross_entropy\u001b[0;34m(hidden_states, lm_weight, labels, num_items_in_batch, ignore_index, reduction, logit_softcapping, accuracy_threshold)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfused_linear_cross_entropy\u001b[39m(\n\u001b[1;32m    158\u001b[0m     hidden_states      : torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    159\u001b[0m     lm_weight          : torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    160\u001b[0m     labels             : torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    161\u001b[0m     num_items_in_batch : \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    162\u001b[0m     ignore_index       : \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m    163\u001b[0m     reduction          : \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    164\u001b[0m     logit_softcapping  : \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    165\u001b[0m     accuracy_threshold : \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    166\u001b[0m ):\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# All Unsloth Zoo code licensed under LGPLv3\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m logit_softcapping \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: logit_softcapping \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with torch.profiler.profile(\n",
    "    activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
    "    profile_memory=True,\n",
    "    record_shapes=True,\n",
    "    #with_stack=True\n",
    ") as prof:\n",
    "    # Run training step\n",
    "    trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in ./venv/master_venv/lib/python3.12/site-packages (6.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 64 | Gradient accumulation steps = 16\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (64 x 16 x 1) = 1,024\n",
      " \"-____-\"     Trainable parameters = 1,721,761,792/8,000,000,000 (21.52% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/mlynatom/cp-exp/e/CPEX-1\n",
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:315\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                           aten::matmul         0.03%      63.298ms        25.69%       50.175s       3.287ms       0.000us         0.00%      119.106s       7.803ms           0 b           0 b    3987.01 Gb           0 b         15264  \n",
      "                                               aten::mm         0.32%     632.331ms        25.65%       50.112s       3.283ms      119.106s        62.72%      119.106s       7.803ms           0 b           0 b    3987.01 Gb    3987.01 Gb         15264  \n",
      "                                            aten::empty         0.05%     101.479ms         0.05%     103.347ms      12.872us       0.000us         0.00%       0.000us       0.000us      29.57 Mb      29.57 Mb    2192.99 Gb    2192.99 Gb          8029  \n",
      "                                               LoRA_MLP         0.13%     249.492ms         9.39%       18.336s      31.834ms        1.785s         0.94%       75.183s     130.526ms           0 b           0 b    1296.00 Gb   -2085.19 Gb           576  \n",
      "                                       aten::empty_like         0.01%      21.271ms         0.09%     174.854ms      24.965us       0.000us         0.00%       0.000us       0.000us          32 b           0 b    1035.96 Gb           0 b          7004  \n",
      "                                    aten::empty_strided         0.14%     279.059ms         0.14%     279.824ms      12.213us       0.000us         0.00%       0.000us       0.000us     517.35 Kb     517.35 Kb     804.17 Gb     804.17 Gb         22912  \n",
      "                                              aten::add         0.01%      25.070ms         0.20%     386.601ms     261.041us        1.325s         0.70%        1.325s     894.441us       1.49 Kb       1.49 Kb     720.00 Gb     720.00 Gb          1481  \n",
      "                                     Fast_RMS_Layernorm         0.26%     516.089ms         1.46%        2.848s       2.453ms     710.787ms         0.37%     710.787ms     612.219us           0 b           0 b     580.64 Gb    -144.00 Mb          1161  \n",
      "                                               LoRA_QKV         0.06%     116.704ms        10.55%       20.609s      35.779ms       0.000us         0.00%       11.396s      19.785ms           0 b           0 b     432.01 Gb     -59.06 Gb           576  \n",
      "                                          _fMHABackward         0.03%      56.390ms         0.27%     526.538ms       1.828ms       0.000us         0.00%        4.525s      15.713ms           0 b           0 b     432.00 Gb      -2.25 Gb           288  \n",
      "                              xformers_flash::flash_bwd         0.02%      30.593ms         0.23%     446.709ms       1.551ms       0.000us         0.00%        4.523s      15.704ms           0 b           0 b     432.00 Gb           0 b           288  \n",
      "                        aten::_flash_attention_backward         0.00%       9.426ms         0.21%     416.116ms       1.445ms        4.523s         2.38%        4.523s      15.704ms           0 b           0 b     432.00 Gb    -290.25 Gb           288  \n",
      "                              xformers_flash::flash_fwd         0.03%      59.871ms         0.31%     595.825ms       1.034ms       0.000us         0.00%        2.586s       4.489ms       9.00 Kb      -9.00 Kb     292.50 Gb           0 b           576  \n",
      "                         aten::_flash_attention_forward         0.01%      17.337ms         0.27%     521.689ms     905.709us        2.586s         1.36%        2.586s       4.489ms       9.00 Kb          16 b     292.50 Gb           0 b           576  \n",
      "                                            aten::clone         0.00%       4.964ms         4.21%        8.229s      13.337ms       0.000us         0.00%     573.418ms     929.364us       4.98 Kb           0 b     288.00 Gb           0 b           617  \n",
      "                                          aten::reshape         0.01%      21.054ms         4.23%        8.268s     868.234us       0.000us         0.00%     573.370ms      60.209us           0 b           0 b     288.00 Gb           0 b          9523  \n",
      "                                                 LoRA_W         0.02%      46.267ms         1.66%        3.248s       5.639ms       0.000us         0.00%        7.116s      12.355ms           0 b           0 b     288.00 Gb     -20.25 Gb           576  \n",
      "                      UnslothCheckpointFunctionBackward         0.19%     362.632ms        53.66%      104.824s     363.970ms       0.000us         0.00%      119.494s     414.908ms           0 b           0 b     166.51 Gb    -504.00 Gb           288  \n",
      "                                                  _fMHA         0.04%      68.973ms         0.31%     603.616ms       2.096ms       0.000us         0.00%        1.189s       4.129ms           0 b      -4.50 Kb     146.25 Gb           0 b           288  \n",
      "    autograd::engine::evaluate_function: LoRA_WBackward         0.00%       5.163ms         1.68%        3.278s      11.383ms       0.000us         0.00%        4.863s      16.886ms           0 b           0 b     146.25 Gb      -1.12 Gb           288  \n",
      "                                         LoRA_WBackward         0.03%      53.330ms         1.58%        3.081s      10.698ms       0.000us         0.00%        4.855s      16.857ms           0 b           0 b     145.12 Gb     -28.12 Gb           288  \n",
      "                              UnslothCheckpointFunction         0.10%     194.128ms        11.96%       23.367s      81.134ms       0.000us         0.00%       57.105s     198.282ms           0 b      -4.50 Kb     144.50 Gb   -1082.25 Gb           288  \n",
      "                                              aten::sum         0.01%      11.639ms         0.01%      23.236ms      38.534us     328.071ms         0.17%     328.175ms     544.238us           0 b           0 b      72.00 Gb      71.99 Gb           603  \n",
      "                                        ExpandBackward0         0.00%       2.061ms         0.01%      18.612ms      32.312us       0.000us         0.00%     327.690ms     568.906us           0 b           0 b      72.00 Gb           0 b           576  \n",
      "                                               aten::to         0.02%      29.402ms        16.80%       32.812s       1.442ms       0.000us         0.00%     271.542ms      11.930us     512.40 Kb           0 b      56.34 Gb           0 b         22761  \n",
      "                                         aten::_to_copy         0.04%      84.152ms        16.78%       32.782s       2.017ms       0.000us         0.00%     271.542ms      16.703us     512.40 Kb           0 b      56.34 Gb           0 b         16257  \n",
      "                                       aten::zeros_like         0.00%       2.953ms         0.07%     128.157ms     139.604us       0.000us         0.00%      22.481ms      24.489us           0 b           0 b      16.58 Gb           0 b           918  \n",
      "                     LinearCrossEntropyFunctionBackward         0.01%      25.828ms         1.31%        2.562s     284.619ms        9.046s         4.76%        9.063s        1.007s           0 b           0 b      13.36 Gb     -11.06 Mb             9  \n",
      "                                            aten::zeros         0.00%       5.045ms         0.02%      37.547ms      30.903us       0.000us         0.00%      13.447ms      11.067us           0 b           0 b      11.16 Gb           0 b          1215  \n",
      "                                     EmbeddingBackward0         0.00%      76.675us         2.26%        4.415s     490.535ms       0.000us         0.00%      46.835ms       5.204ms           0 b           0 b       8.86 Gb           0 b             9  \n",
      "                               aten::embedding_backward         0.00%      50.302us         2.26%        4.415s     490.526ms       0.000us         0.00%      46.835ms       5.204ms           0 b           0 b       8.86 Gb           0 b             9  \n",
      "                         aten::embedding_dense_backward         0.00%       1.585ms         2.26%        4.415s     490.521ms      37.491ms         0.02%      46.835ms       5.204ms           0 b           0 b       8.86 Gb     -10.16 Gb             9  \n",
      "autograd::engine::evaluate_function: LinearCrossEntr...         0.00%     196.869us         1.31%        2.562s     284.640ms       0.000us         0.00%        9.063s        1.007s           0 b           0 b       8.85 Gb      -4.51 Gb             9  \n",
      "                                       LoRA_MLPBackward         0.17%     329.829ms        13.37%       26.110s      90.660ms        1.943s         1.02%       45.666s     158.563ms           0 b           0 b       7.59 Gb    -592.59 Gb           288  \n",
      "                                          aten::resize_         0.01%      17.897ms         5.77%       11.280s     156.660ms       6.815us         0.00%       6.815us       0.095us       1.00 Mb       1.00 Mb       5.01 Gb       5.01 Gb            72  \n",
      "                                        aten::embedding         0.00%     303.897us         0.03%      58.383ms       6.487ms       0.000us         0.00%      28.463ms       3.163ms           0 b           0 b       4.50 Gb           0 b             9  \n",
      "                                     aten::index_select         0.00%     344.285us         0.03%      58.020ms       6.447ms      28.463ms         0.01%      28.463ms       3.163ms           0 b           0 b       4.50 Gb           0 b             9  \n",
      "autograd::engine::evaluate_function: EmbeddingBackwa...         0.00%     266.961us         2.26%        4.415s     490.565ms       0.000us         0.00%      46.835ms       5.204ms           0 b           0 b       4.36 Gb      -4.50 Gb             9  \n",
      "                              Optimizer.step#AdamW.step         0.18%     359.574ms         0.60%        1.180s     131.113ms       0.000us         0.00%        1.686s     187.371ms      -2.00 Kb      -2.00 Kb       3.27 Gb           0 b             9  \n",
      "                                       LoRA_QKVBackward         0.06%     109.171ms        11.26%       21.991s      76.357ms       0.000us         0.00%        8.019s      27.845ms           0 b           0 b       2.53 Gb     -83.53 Gb           288  \n",
      "autograd::engine::evaluate_function: UnslothCheckpoi...         0.00%       3.202ms        53.66%      104.827s     363.982ms       0.000us         0.00%      119.494s     414.908ms           0 b           0 b       2.51 Gb    -164.00 Gb           288  \n",
      "                                        aten::new_empty         0.00%     824.254us         0.00%       1.290ms      23.041us       0.000us         0.00%       0.000us       0.000us           0 b           0 b      15.57 Mb           0 b            56  \n",
      "                                             aten::sort         0.00%       6.863ms         0.01%      17.648ms       1.103ms       1.042ms         0.00%       1.079ms      67.410us           0 b           0 b      13.21 Mb      13.21 Mb            16  \n",
      "                                           aten::arange         0.00%     356.386us         2.95%        5.769s     262.244ms      35.168us         0.00%      70.336us       3.197us       2.00 Mb           0 b       9.00 Mb           0 b            22  \n",
      "                                          aten::nonzero         0.00%     586.777us         0.00%       1.506ms     167.329us     249.638us         0.00%     249.638us      27.738us           0 b           0 b       9.00 Mb           0 b             9  \n",
      "                                        aten::new_zeros         0.00%     178.653us         0.00%     850.004us      31.482us       0.000us         0.00%      88.448us       3.276us           0 b           0 b       8.90 Mb           0 b            27  \n",
      "                                         aten::new_full         0.00%     121.870us         0.00%     792.250us      29.343us       0.000us         0.00%      74.208us       2.748us           0 b           0 b       6.67 Mb           0 b            27  \n",
      "                             LinearCrossEntropyFunction         0.19%     369.745ms         0.19%     371.704ms      41.300ms        3.380s         1.78%        3.380s     375.562ms           0 b           0 b       6.66 Mb      -2.27 Mb             9  \n",
      "                                  Torch-Compiled Region         0.00%       2.058ms         0.00%       8.438ms     937.557us       0.000us         0.00%       1.115ms     123.915us           0 b           0 b       4.40 Mb      -8.81 Mb             9  \n",
      "                                    aten::_foreach_norm         0.01%      13.350ms         0.01%      18.881ms       1.049ms      31.563ms         0.02%      31.602ms       1.756ms           0 b           0 b       1.98 Mb     576.00 Kb            18  \n",
      "                                               aten::ne         0.00%       1.404ms         0.00%       8.531ms     315.965us     149.729us         0.00%     149.729us       5.546us           0 b           0 b       1.13 Mb       1.13 Mb            27  \n",
      "                                              aten::div         0.00%     831.780us         0.00%       1.593ms      31.230us     135.902us         0.00%     135.902us       2.665us       1.74 Kb       1.68 Kb      18.00 Kb      18.00 Kb            51  \n",
      "                                              aten::cat         0.02%      37.714ms         0.04%      80.480ms     137.339us       9.701ms         0.01%       9.701ms      16.555us      64.01 Mb      64.01 Mb      18.00 Kb      18.00 Kb           586  \n",
      "                                            aten::stack         0.00%       8.876ms         0.05%      89.516ms     153.019us       0.000us         0.00%       9.701ms      16.584us       9.00 Kb           0 b      18.00 Kb           0 b           585  \n",
      "                                       aten::lift_fresh         0.01%      11.498ms         0.01%      13.788ms     128.864us       0.000us         0.00%      47.807us       0.447us           0 b           0 b      12.00 Kb           0 b           107  \n",
      "                                           DivBackward0         0.00%     161.741us         0.00%     980.970us      54.498us       0.000us         0.00%      61.440us       3.413us           0 b           0 b       9.00 Kb           0 b            18  \n",
      "                                              aten::abs         0.00%     406.901us         0.00%       1.468ms      81.547us      26.368us         0.00%      52.736us       2.930us           0 b           0 b       9.00 Kb           0 b            18  \n",
      "                                              aten::mul         0.00%       5.194ms         0.00%       5.420ms     169.375us      19.107us         0.00%      19.107us       0.597us      32.00 Mb      32.00 Mb       4.50 Kb       4.50 Kb            32  \n",
      "                                       aten::reciprocal         0.00%     151.051us         0.00%       6.651ms     665.111us      21.568us         0.00%      21.568us       2.157us         256 b         256 b       4.50 Kb       4.50 Kb            10  \n",
      "                                        aten::ones_like         0.00%      37.770us         0.00%     304.769us      33.863us       0.000us         0.00%      22.880us       2.542us           0 b           0 b       4.50 Kb           0 b             9  \n",
      "      autograd::engine::evaluate_function: DivBackward0         0.00%     253.043us         0.00%       1.234ms      68.556us       0.000us         0.00%      61.440us       3.413us         -72 b         -72 b       4.50 Kb      -4.50 Kb            18  \n",
      "                                            aten::isnan         0.00%      60.747us         0.00%       2.952ms     327.996us       0.000us         0.00%      31.488us       3.499us           0 b           0 b       4.50 Kb           0 b             9  \n",
      "                                            aten::isinf         0.00%      95.938us         0.00%       1.372ms     152.443us       0.000us         0.00%      52.608us       5.845us           0 b           0 b       4.50 Kb      -4.50 Kb             9  \n",
      "                                               aten::eq         0.00%     307.652us         0.00%     406.398us      45.155us      26.240us         0.00%      26.240us       2.916us           0 b           0 b       4.50 Kb       4.50 Kb             9  \n",
      "                               aten::linalg_vector_norm         0.00%     339.142us         0.01%      20.421ms       2.269ms      49.889us         0.00%      49.889us       5.543us           0 b           0 b       4.50 Kb       4.50 Kb             9  \n",
      "                                            aten::clamp         0.00%     237.550us         0.01%      10.497ms       1.166ms      20.927us         0.00%      20.927us       2.325us           0 b           0 b       4.50 Kb       4.50 Kb             9  \n",
      "                                             aten::mean         0.00%     737.187us         0.00%       7.844ms     871.562us      43.008us         0.00%      43.008us       4.779us           0 b           0 b       4.50 Kb       4.50 Kb             9  \n",
      "                aten::_has_compatible_shallow_copy_type         0.00%       1.023ms         0.00%       1.023ms       0.185us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b          5532  \n",
      "                                         aten::linspace         0.00%     102.272us         0.00%     165.897us       5.925us       0.000us         0.00%       0.000us       0.000us       1.54 Kb           0 b           0 b           0 b            28  \n",
      "                                            aten::slice         0.01%      23.439ms         0.02%      31.997ms       4.448us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b          7194  \n",
      "                                       aten::as_strided         0.03%      54.043ms         0.03%      54.144ms       0.881us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b         61437  \n",
      "                                            aten::copy_         0.07%     127.405ms        20.89%       40.808s       2.343ms       12.821s         6.75%       12.833s     736.710us           0 b           0 b           0 b    -128.00 Mb         17419  \n",
      "                                     aten::resolve_conj         0.00%       4.839us         0.00%       4.839us       0.220us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            22  \n",
      "                                      aten::resolve_neg         0.00%       2.565us         0.00%       2.565us       0.117us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            22  \n",
      "                                          aten::detach_         0.00%       1.316ms         0.00%       1.400ms      19.996us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            70  \n",
      "                                                detach_         0.00%      84.059us         0.00%      84.059us       1.201us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            70  \n",
      "                                        cudaMemcpyAsync         0.03%      51.629ms         0.03%      51.629ms      11.044us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b          4675  \n",
      "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      11.057ms         0.01%      11.057ms     381.267us           0 b           0 b           0 b           0 b            29  \n",
      "                                  cudaStreamSynchronize        28.57%       55.807s        28.57%       55.807s     502.767ms       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b           111  \n",
      "                                          aten::random_         0.00%      26.449us         0.00%      26.449us      26.449us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             1  \n",
      "                                             aten::item         0.00%     292.798us        28.56%       55.799s        1.213s       0.000us         0.00%     123.582us       2.687us           0 b           0 b           0 b           0 b            46  \n",
      "                              aten::_local_scalar_dense         0.00%     955.083us        28.56%       55.799s        1.213s     123.582us         0.00%     123.582us       2.687us           0 b           0 b           0 b           0 b            46  \n",
      "enumerate(DataLoader)#_SingleProcessDataLoaderIter._...         0.73%        1.423s         0.74%        1.450s     131.817ms       0.000us         0.00%       0.000us       0.000us           0 b     -29.56 Mb           0 b           0 b            11  \n",
      "                                         aten::randperm         0.01%      25.238ms         0.03%      49.606ms       7.087ms       0.000us         0.00%       0.000us       0.000us      14.56 Mb          -8 b           0 b           0 b             7  \n",
      "                                    aten::scalar_tensor         0.00%       4.559us         0.00%       4.559us       4.559us       0.000us         0.00%       0.000us       0.000us           8 b           8 b           0 b           0 b             1  \n",
      "                                       aten::pin_memory         0.00%     122.338us         0.00%       2.579ms      85.979us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            30  \n",
      "                                        aten::is_pinned         0.00%     353.867us         0.00%     479.686us      15.990us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            30  \n",
      "                               cudaPointerGetAttributes         0.00%     125.819us         0.00%     125.819us       4.194us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            30  \n",
      "                                      aten::_pin_memory         0.00%     302.792us         0.00%       1.977ms      65.911us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            30  \n",
      "                                          cudaHostAlloc         5.77%       11.262s         5.77%       11.262s     321.761ms       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            35  \n",
      "                                             aten::set_         0.00%     193.133us         0.00%     193.133us       4.491us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b            43  \n",
      "                         Memcpy HtoD (Pinned -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us        5.385s         2.84%        5.385s      19.371ms           0 b           0 b           0 b           0 b           278  \n",
      "                                       cudaLaunchKernel        53.20%      103.917s        53.20%      103.917s       1.886ms     541.533ms         0.29%     541.533ms       9.827us           0 b           0 b           0 b           0 b         55109  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     118.241us         0.00%     118.241us       6.569us           0 b           0 b           0 b           0 b            18  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us     104.384us         0.00%     104.384us       5.799us           0 b           0 b           0 b           0 b            18  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     129.891us         0.00%     129.891us      14.432us           0 b           0 b           0 b           0 b             9  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      21.055us         0.00%      21.055us       2.339us           0 b           0 b           0 b           0 b             9  \n",
      "                                             aten::view         0.04%      68.519ms         0.04%      68.519ms       2.226us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b         30787  \n",
      "void at::native::(anonymous namespace)::indexSelectL...         0.00%       0.000us         0.00%       0.000us       0.000us      28.463ms         0.01%      28.463ms       3.163ms           0 b           0 b           0 b           0 b             9  \n",
      "                                              aten::pow         0.00%      24.325us         0.00%      53.981us      26.990us       0.000us         0.00%       0.000us       0.000us         256 b         252 b           0 b           0 b             2  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 195.342s\n",
      "Self CUDA time total: 189.899s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cuda_memory_usage\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#64\n",
    "print(prof.key_averages().table(sort_by=\"cuda_memory_usage\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#16\n",
    "print(prof.key_averages().table(sort_by=\"cuda_memory_usage\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "print(prof.key_averages().table(sort_by=\"cuda_memory_usage\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "print(prof.key_averages().table(sort_by=\"cuda_memory_usage\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#16*16 - a100 40GB\n",
    "print(prof.key_averages().table(sort_by=\"cuda_memory_usage\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#16*16 - 80GB\n",
    "print(prof.key_averages().table(sort_by=\"cuda_memory_usage\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#128 *2 - 40 + group by length\n",
    "print(prof.key_averages().table(sort_by=\"cuda_memory_usage\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#128 *2 - 40 + packing\n",
    "print(prof.key_averages().table(sort_by=\"cuda_memory_usage\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#128 *2 - 40\n",
    "print(prof.key_averages().table(sort_by=\"cuda_memory_usage\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#256\n",
    "print(prof.key_averages().table(sort_by=\"cuda_memory_usage\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cuda_memory_usage\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot spent time\n",
    "\n",
    "prof.export_memory_timeline(\"memory_timeline.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cpu_memory_usage\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cuda_memory_usage\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cuda_memory_usage\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
