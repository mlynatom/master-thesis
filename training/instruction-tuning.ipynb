{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFTT (Instruction Tuning) using Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: unsloth 2024.12.4\n",
      "Uninstalling unsloth-2024.12.4:\n",
      "  Successfully uninstalled unsloth-2024.12.4\n",
      "Collecting git+https://github.com/unslothai/unsloth.git\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /data/temporary/pip-req-build-8_wldsqp\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /data/temporary/pip-req-build-8_wldsqp\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit 85f1fa096afde5efe2fb8521d8ceec8d13a00715\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: unsloth\n",
      "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unsloth: filename=unsloth-2024.12.4-py3-none-any.whl size=173746 sha256=d24c513cc390ff39d829f5174b34357c59e8c03ef4692f8c0fba31bc8bf26730\n",
      "  Stored in directory: /data/temporary/pip-ephem-wheel-cache-fmx16g_e/wheels/60/3e/1f/e576c07051d90cf64b6a41434d87ccf4db33fafd5343bf5de0\n",
      "Successfully built unsloth\n",
      "Installing collected packages: unsloth\n",
      "Successfully installed unsloth-2024.12.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: unsloth_zoo in ./venv/master_venv/lib/python3.12/site-packages (2024.12.1)\n",
      "Requirement already satisfied: torch in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (2.5.1)\n",
      "Requirement already satisfied: triton in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (3.1.0)\n",
      "Requirement already satisfied: packaging in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (24.1)\n",
      "Requirement already satisfied: tyro in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (0.9.2)\n",
      "Requirement already satisfied: transformers>=4.46.1 in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (4.46.3)\n",
      "Requirement already satisfied: datasets>=2.16.0 in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (3.0.1)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (0.2.0)\n",
      "Requirement already satisfied: tqdm in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (4.66.5)\n",
      "Requirement already satisfied: psutil in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (6.1.0)\n",
      "Requirement already satisfied: wheel>=0.42.0 in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (0.45.1)\n",
      "Requirement already satisfied: numpy in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (2.1.2)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (1.0.1)\n",
      "Requirement already satisfied: trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (0.12.1)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (0.13.2)\n",
      "Requirement already satisfied: protobuf<4.0.0 in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (3.20.3)\n",
      "Requirement already satisfied: huggingface_hub in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (0.26.1)\n",
      "Requirement already satisfied: hf_transfer in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (0.1.8)\n",
      "Requirement already satisfied: cut_cross_entropy in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (24.11.4)\n",
      "Requirement already satisfied: pillow in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (11.0.0)\n",
      "Requirement already satisfied: pyyaml in ./venv/master_venv/lib/python3.12/site-packages (from accelerate>=0.34.1->unsloth_zoo) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./venv/master_venv/lib/python3.12/site-packages (from accelerate>=0.34.1->unsloth_zoo) (0.4.5)\n",
      "Requirement already satisfied: filelock in ./venv/master_venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth_zoo) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./venv/master_venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth_zoo) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./venv/master_venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth_zoo) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./venv/master_venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth_zoo) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./venv/master_venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth_zoo) (2.32.3)\n",
      "Requirement already satisfied: xxhash in ./venv/master_venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth_zoo) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in ./venv/master_venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth_zoo) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in ./venv/master_venv/lib/python3.12/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.16.0->unsloth_zoo) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in ./venv/master_venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth_zoo) (3.10.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/master_venv/lib/python3.12/site-packages (from huggingface_hub->unsloth_zoo) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (12.4.127)\n",
      "Requirement already satisfied: setuptools in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (75.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/master_venv/lib/python3.12/site-packages (from sympy==1.13.1->torch->unsloth_zoo) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/master_venv/lib/python3.12/site-packages (from transformers>=4.46.1->unsloth_zoo) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./venv/master_venv/lib/python3.12/site-packages (from transformers>=4.46.1->unsloth_zoo) (0.20.1)\n",
      "Requirement already satisfied: rich in ./venv/master_venv/lib/python3.12/site-packages (from trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth_zoo) (13.9.4)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in ./venv/master_venv/lib/python3.12/site-packages (from tyro->unsloth_zoo) (0.16)\n",
      "Requirement already satisfied: shtab>=1.5.6 in ./venv/master_venv/lib/python3.12/site-packages (from tyro->unsloth_zoo) (1.7.1)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in ./venv/master_venv/lib/python3.12/site-packages (from tyro->unsloth_zoo) (4.4.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./venv/master_venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/master_venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/master_venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/master_venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/master_venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./venv/master_venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (1.15.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/master_venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth_zoo) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/master_venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth_zoo) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/master_venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth_zoo) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/master_venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth_zoo) (2024.8.30)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/master_venv/lib/python3.12/site-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth_zoo) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/master_venv/lib/python3.12/site-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth_zoo) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/master_venv/lib/python3.12/site-packages (from jinja2->torch->unsloth_zoo) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/master_venv/lib/python3.12/site-packages (from pandas->datasets>=2.16.0->unsloth_zoo) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/master_venv/lib/python3.12/site-packages (from pandas->datasets>=2.16.0->unsloth_zoo) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/master_venv/lib/python3.12/site-packages (from pandas->datasets>=2.16.0->unsloth_zoo) (2024.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venv/master_venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth_zoo) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/master_venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth_zoo) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./venv/master_venv/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.16.0->unsloth_zoo) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.19.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting click!=8.0.0,>=7.1 (from wandb)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Using cached GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in ./venv/master_venv/lib/python3.12/site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in ./venv/master_venv/lib/python3.12/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in ./venv/master_venv/lib/python3.12/site-packages (from wandb) (6.1.0)\n",
      "Collecting pydantic<3,>=2.6 (from wandb)\n",
      "  Downloading pydantic-2.10.3-py3-none-any.whl.metadata (172 kB)\n",
      "Requirement already satisfied: pyyaml in ./venv/master_venv/lib/python3.12/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in ./venv/master_venv/lib/python3.12/site-packages (from wandb) (2.32.3)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.19.2-py2.py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in ./venv/master_venv/lib/python3.12/site-packages (from wandb) (75.2.0)\n",
      "Requirement already satisfied: six>=1.4.0 in ./venv/master_venv/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=2.6->wandb)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.1 (from pydantic<3,>=2.6->wandb)\n",
      "  Downloading pydantic_core-2.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./venv/master_venv/lib/python3.12/site-packages (from pydantic<3,>=2.6->wandb) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/master_venv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/master_venv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/master_venv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/master_venv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading wandb-0.19.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Using cached GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "Downloading pydantic-2.10.3-py3-none-any.whl (456 kB)\n",
      "Downloading pydantic_core-2.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-2.19.2-py2.py3-none-any.whl (322 kB)\n",
      "Downloading setproctitle-1.3.4-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, setproctitle, sentry-sdk, pydantic-core, docker-pycreds, click, annotated-types, pydantic, gitdb, gitpython, wandb\n",
      "Successfully installed annotated-types-0.7.0 click-8.1.7 docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 pydantic-2.10.3 pydantic-core-2.27.1 sentry-sdk-2.19.2 setproctitle-1.3.4 smmap-5.0.1 wandb-0.19.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#install latest version of unsloth\n",
    "#%pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "#%pip install --upgrade unsloth\n",
    "%pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
    "%pip install --upgrade unsloth_zoo\n",
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from unsloth.chat_templates import get_chat_template, train_on_responses_only\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "\n",
    "# random state\n",
    "SEED = 42\n",
    "\n",
    "# reproducibility\n",
    "## torch\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "## python\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "## numpy\n",
    "import numpy as np\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.4: Fast Llama patching. Transformers:4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.381 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7407fa5a550444169b37677c386cb117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f4b1a300f843d6a8aab81671d0c562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef6f035081d481e8d71dc25dcf32405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e848fad6a443199709790eeb564b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20020d9092824ca5b401033631e2d136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_seq_length = 8192 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.12.4 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = SEED,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "dataset = load_dataset(\"ctu-aic/ask_library_cs\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_ask_library(dataset):\n",
    "    \"\"\"Standardize dataset to hugginface conversations format\"\"\"\n",
    "    # filter cs language only\n",
    "    dataset = dataset.filter(lambda x: x[\"language\"] == \"cs\")\n",
    "\n",
    "    questions = dataset[\"question\"]\n",
    "    answers = dataset[\"answer\"]\n",
    "\n",
    "\n",
    "    conversations = []\n",
    "    for q, a in zip(questions, answers):\n",
    "        user = {\"role\": \"user\", \"content\": q}\n",
    "        assistant = {\"role\": \"assistant\", \"content\": a}\n",
    "        conversations.append([user, assistant])\n",
    "\n",
    "    dataset = dataset.add_column(\"conversations\", conversations)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = standardize_ask_library(dataset)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'DobrÃ½ den, nemohu se prosÃ­m pÅ™ihlÃ¡sit pÅ™es zadanÃ© Ãºdaje, hlÃ¡sÃ­ to chybnÄ› zadanÃ½ pin nebo ÄtenÃ¡Å™skÃ© ÄÃ­slo, delÅ¡Ã­ obdobÃ­ jsem knihy nevypÅ¯jÄovala, chci teÄ v dobÄ› koronaviru zkusit e-lerning. DÄ›kuji za odpovÄ›Ä.',\n",
       "  'role': 'user'},\n",
       " {'content': 'VÃ¡Å¾enÃ¡ panÃ­,  moc mÄ› to mrzÃ­, ale v databÃ¡zi ÄtenÃ¡Å™Å¯ VÃ¡s jiÅ¾ nemÃ¡me. Podle NaÅ™Ã­zenÃ­ na ochranu osobnÃ­ch ÃºdajÅ¯ nemÅ¯Å¾eme spravovat Ãºdaje ÄtenÃ¡Å™Å¯ dÃ©le neÅ¾ Å¡est mÄ›sÃ­cÅ¯ od propadlÃ© registrace. Potom musÃ­me vÅ¡echny VaÅ¡e Ãºdaje zarchivovat. NabÃ­dnout VÃ¡m mohu zaloÅ¾enÃ­ novÃ©ho ÄtenÃ¡Å™skÃ©ho konta, toto lze vÅ¡ak uÄinit aÅ¾ v okamÅ¾iku znovuotevÅ™enÃ­ knihovny.  MomentÃ¡lnÄ› mÅ¯Å¾ete zkusit e-knihy, kterÃ© jdou stahovat zdarma, vÃ­ce informacÃ­ zÃ­skÃ¡te na tomto webu -https://protiviru.knihovny.cz/  DÄ›kuji za pochopenÃ­ a pÅ™eji pevnÃ© zdravÃ­',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"conversations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nDobrÃ½ den, nemohu se prosÃ­m pÅ™ihlÃ¡sit pÅ™es zadanÃ© Ãºdaje, hlÃ¡sÃ­ to chybnÄ› zadanÃ½ pin nebo ÄtenÃ¡Å™skÃ© ÄÃ­slo, delÅ¡Ã­ obdobÃ­ jsem knihy nevypÅ¯jÄovala, chci teÄ v dobÄ› koronaviru zkusit e-lerning. DÄ›kuji za odpovÄ›Ä.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nVÃ¡Å¾enÃ¡ panÃ­,  moc mÄ› to mrzÃ­, ale v databÃ¡zi ÄtenÃ¡Å™Å¯ VÃ¡s jiÅ¾ nemÃ¡me. Podle NaÅ™Ã­zenÃ­ na ochranu osobnÃ­ch ÃºdajÅ¯ nemÅ¯Å¾eme spravovat Ãºdaje ÄtenÃ¡Å™Å¯ dÃ©le neÅ¾ Å¡est mÄ›sÃ­cÅ¯ od propadlÃ© registrace. Potom musÃ­me vÅ¡echny VaÅ¡e Ãºdaje zarchivovat. NabÃ­dnout VÃ¡m mohu zaloÅ¾enÃ­ novÃ©ho ÄtenÃ¡Å™skÃ©ho konta, toto lze vÅ¡ak uÄinit aÅ¾ v okamÅ¾iku znovuotevÅ™enÃ­ knihovny.  MomentÃ¡lnÄ› mÅ¯Å¾ete zkusit e-knihy, kterÃ© jdou stahovat zdarma, vÃ­ce informacÃ­ zÃ­skÃ¡te na tomto webu -https://protiviru.knihovny.cz/  DÄ›kuji za pochopenÃ­ a pÅ™eji pevnÃ© zdravÃ­<|eot_id|>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdb8e8f7156f42d590a74fb2a8ff9e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/15789 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.other:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 16,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        #max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = SEED,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"wandb\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29be72cdd84044f0aeadd11a1741d5ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15789 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nBohuÅ¾el jsme se dopracovali jen k protektorÃ¡tu a tam jsme ÃºplnÄ› samostatnÃ­ nebyli.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nDobrÃ½ den, mÃ¡te pravdu, ProtektorÃ¡t ÄŒechy a Morava za samostatnÃ½ ÄeskÃ½ stÃ¡t opravdu povaÅ¾ovat nelze. PodÃ­vÃ¡me-li se na Äasovou osu, tak nejblÃ­Å¾e k\\xa0naÅ¡Ã­ dobÄ› lze o samostatnÃ©m ÄeskÃ©m stÃ¡tÄ› hovoÅ™it v\\xa0pÅ™Ã­padÄ› ZemÃ­ Koruny ÄeskÃ©, kterÃ© byly ustanoveny Karlem IV. 7. dubna 1348 a zanikly aÅ¾ vznikem ÄŒeskoslovenskÃ© republiky na podzim\\xa0r. 1918. NÃ¡stupem Ferdinanda I. HabsburskÃ©ho na ÄeskÃ½ trÅ¯n (23.Å™Ã­jna 1526) vÅ¡ak o svoji bÃ½valou mocenskou autonomii pÅ™iÅ¡ly (VOREL, s. 81-87), stÃ¡tnÃ­ Ãºtvar z roku 1526 bychom tedy v\\xa0pÅ™Ã­padÄ› VaÅ¡Ã­ otÃ¡zky mohli povaÅ¾ovat za nejpozdÄ›jÅ¡Ã­, kterÃ½ pÅ™ichÃ¡zÃ­ v Ãºvahu. Naopak prvnÃ­ zmÃ­nky o ÄeskÃ©m stÃ¡tÄ›, kterÃ½ vystupoval navenek jako samostatnÃ½, jsou datovÃ¡ny do\\xa0pÅ™emyslovskÃ© Ã©ry, tedy do doby sjednocovÃ¡nÃ­ kmenÅ¯ Å¾ijÃ­cÃ­ch na ÃºzemÃ­ ÄŒech a nÃ¡slednÃ© expanze na\\xa0ÃºzemÃ­ Moravy, tedy pÅ™ibliÅ¾nÄ› mezi konec 9. stoletÃ­ a zaÄÃ¡tek 11. stoletÃ­. V\\xa0literatuÅ™e hovoÅ™Ã­me o\\xa0tzv.\\xa0ÄŒeskÃ©m knÃ­Å¾ectvÃ­, pÅ™Ã­padnÄ› o PÅ™emyslovskÃ©m stÃ¡tÄ›. Mezi vÃ½Å¡e uvedenÃ½mi ÄasovÃ½mi meznÃ­ky ÄeskÃ½ stÃ¡t sice mÄ›nil nÃ¡zev, panovnickÃ© dynastie a ÃºzemnÃ­ rozlohu, o jeho samostatnosti vÅ¡ak nenÃ­ pochyb (mÄ›l sice nÄ›jakÃ© povinnosti vÅ¯Äi cÃ­saÅ™i SvatÃ© Å™Ã­Å¡e Å™Ã­mskÃ©, ty vÅ¡ak nezasahovaly do jeho autonomie v\\xa0takovÃ© mÃ­Å™e, jako tomu bylo za HabsburkÅ¯ nebo bÄ›hem 2. svÄ›tovÃ© vÃ¡lky. PÅ™imhouÅ™Ã­me-li oÄi, mohli bychom tento vztah pÅ™irovnat ke ÄlenstvÃ­ v EU). Å˜ada historikÅ¯ (mezi nimi napÅ™. Martin Wihoda Äi JiÅ™Ã­ Rak) zastÃ¡vÃ¡ nÃ¡zor, Å¾e nÃ¡zev stÃ¡tnÃ­ho svÃ¡tku \"Den obnovy samostatnÃ©ho ÄeskÃ©ho stÃ¡tu\" byl zvolen neÅ¡Å¥astnÄ›. (WIHODA, s. 13: \"â€¦mÄ›l bÃ½t obnoven ÄeskÃ½ stÃ¡t, ÄÃ­mÅ¾ je Å™eÄeno, Å¾e pÅ™ed rokem 1993 buÄ nebyl, nebo mu vlÃ¡dl kdosi jinÃ½. Å½e by republikÃ¡nskÃ© ÄŒeskoslovensko, jehoÅ¾ samostatnost se stÃ¡le slavÃ­ 28. Å™Ã­jna? VzÃ¡jemnÄ› se popÃ­rajÃ­cÃ­ stÃ¡tnÃ­ svÃ¡tky vzeÅ¡ly ze smutnÄ› proslulÃ©ho zÃ¡konodÃ¡rskÃ©ho kutilstvÃ­, kterÃ© se Å™Ã­dilo prostoduchÃ½m pravidlem, Å¾e daÅˆovÃ­ poplatnÃ­ci oslavy neproÅ¾Ã­vajÃ­ a Å¾e je v\\xa0podstatÄ› jedno, kdo, kdy a proÄ se zaslouÅ¾il o prodlouÅ¾enÃ© vÃ­kendy.\") OtÃ¡zka nÃ¡zvu stÃ¡tnÃ­ho svÃ¡tku je rozebÃ­rÃ¡na rovnÄ›Å¾ na strÃ¡nkÃ¡chhttps://cadil.blog.idnes.cz/blog.aspx?c=313928. Zde autor poukazuje na preambuli Ãšstavy ÄŒR, kterÃ¡ se historicky odvolÃ¡vÃ¡ na zemÄ› Koruny ÄeskÃ©: \"My, obÄanÃ© ÄŒeskÃ© republiky v ÄŒechÃ¡ch, na MoravÄ› a ve Slezsku, v Äase obnovy samostatnÃ©ho ÄeskÃ©ho stÃ¡tu, vÄ›rni vÅ¡em dobrÃ½m tradicÃ­m dÃ¡vnÃ© stÃ¡tnosti zemÃ­ Koruny ÄeskÃ© i stÃ¡tnosti ÄeskoslovenskÃ©â€¦.\" S\\xa0pozdravem Mgr. Pavel Å evÄÃ­k<|eot_id|>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                                        \\n\\nDobrÃ½ den, mÃ¡te pravdu, ProtektorÃ¡t ÄŒechy a Morava za samostatnÃ½ ÄeskÃ½ stÃ¡t opravdu povaÅ¾ovat nelze. PodÃ­vÃ¡me-li se na Äasovou osu, tak nejblÃ­Å¾e k\\xa0naÅ¡Ã­ dobÄ› lze o samostatnÃ©m ÄeskÃ©m stÃ¡tÄ› hovoÅ™it v\\xa0pÅ™Ã­padÄ› ZemÃ­ Koruny ÄeskÃ©, kterÃ© byly ustanoveny Karlem IV. 7. dubna 1348 a zanikly aÅ¾ vznikem ÄŒeskoslovenskÃ© republiky na podzim\\xa0r. 1918. NÃ¡stupem Ferdinanda I. HabsburskÃ©ho na ÄeskÃ½ trÅ¯n (23.Å™Ã­jna 1526) vÅ¡ak o svoji bÃ½valou mocenskou autonomii pÅ™iÅ¡ly (VOREL, s. 81-87), stÃ¡tnÃ­ Ãºtvar z roku 1526 bychom tedy v\\xa0pÅ™Ã­padÄ› VaÅ¡Ã­ otÃ¡zky mohli povaÅ¾ovat za nejpozdÄ›jÅ¡Ã­, kterÃ½ pÅ™ichÃ¡zÃ­ v Ãºvahu. Naopak prvnÃ­ zmÃ­nky o ÄeskÃ©m stÃ¡tÄ›, kterÃ½ vystupoval navenek jako samostatnÃ½, jsou datovÃ¡ny do\\xa0pÅ™emyslovskÃ© Ã©ry, tedy do doby sjednocovÃ¡nÃ­ kmenÅ¯ Å¾ijÃ­cÃ­ch na ÃºzemÃ­ ÄŒech a nÃ¡slednÃ© expanze na\\xa0ÃºzemÃ­ Moravy, tedy pÅ™ibliÅ¾nÄ› mezi konec 9. stoletÃ­ a zaÄÃ¡tek 11. stoletÃ­. V\\xa0literatuÅ™e hovoÅ™Ã­me o\\xa0tzv.\\xa0ÄŒeskÃ©m knÃ­Å¾ectvÃ­, pÅ™Ã­padnÄ› o PÅ™emyslovskÃ©m stÃ¡tÄ›. Mezi vÃ½Å¡e uvedenÃ½mi ÄasovÃ½mi meznÃ­ky ÄeskÃ½ stÃ¡t sice mÄ›nil nÃ¡zev, panovnickÃ© dynastie a ÃºzemnÃ­ rozlohu, o jeho samostatnosti vÅ¡ak nenÃ­ pochyb (mÄ›l sice nÄ›jakÃ© povinnosti vÅ¯Äi cÃ­saÅ™i SvatÃ© Å™Ã­Å¡e Å™Ã­mskÃ©, ty vÅ¡ak nezasahovaly do jeho autonomie v\\xa0takovÃ© mÃ­Å™e, jako tomu bylo za HabsburkÅ¯ nebo bÄ›hem 2. svÄ›tovÃ© vÃ¡lky. PÅ™imhouÅ™Ã­me-li oÄi, mohli bychom tento vztah pÅ™irovnat ke ÄlenstvÃ­ v EU). Å˜ada historikÅ¯ (mezi nimi napÅ™. Martin Wihoda Äi JiÅ™Ã­ Rak) zastÃ¡vÃ¡ nÃ¡zor, Å¾e nÃ¡zev stÃ¡tnÃ­ho svÃ¡tku \"Den obnovy samostatnÃ©ho ÄeskÃ©ho stÃ¡tu\" byl zvolen neÅ¡Å¥astnÄ›. (WIHODA, s. 13: \"â€¦mÄ›l bÃ½t obnoven ÄeskÃ½ stÃ¡t, ÄÃ­mÅ¾ je Å™eÄeno, Å¾e pÅ™ed rokem 1993 buÄ nebyl, nebo mu vlÃ¡dl kdosi jinÃ½. Å½e by republikÃ¡nskÃ© ÄŒeskoslovensko, jehoÅ¾ samostatnost se stÃ¡le slavÃ­ 28. Å™Ã­jna? VzÃ¡jemnÄ› se popÃ­rajÃ­cÃ­ stÃ¡tnÃ­ svÃ¡tky vzeÅ¡ly ze smutnÄ› proslulÃ©ho zÃ¡konodÃ¡rskÃ©ho kutilstvÃ­, kterÃ© se Å™Ã­dilo prostoduchÃ½m pravidlem, Å¾e daÅˆovÃ­ poplatnÃ­ci oslavy neproÅ¾Ã­vajÃ­ a Å¾e je v\\xa0podstatÄ› jedno, kdo, kdy a proÄ se zaslouÅ¾il o prodlouÅ¾enÃ© vÃ­kendy.\") OtÃ¡zka nÃ¡zvu stÃ¡tnÃ­ho svÃ¡tku je rozebÃ­rÃ¡na rovnÄ›Å¾ na strÃ¡nkÃ¡chhttps://cadil.blog.idnes.cz/blog.aspx?c=313928. Zde autor poukazuje na preambuli Ãšstavy ÄŒR, kterÃ¡ se historicky odvolÃ¡vÃ¡ na zemÄ› Koruny ÄeskÃ©: \"My, obÄanÃ© ÄŒeskÃ© republiky v ÄŒechÃ¡ch, na MoravÄ› a ve Slezsku, v Äase obnovy samostatnÃ©ho ÄeskÃ©ho stÃ¡tu, vÄ›rni vÅ¡em dobrÃ½m tradicÃ­m dÃ¡vnÃ© stÃ¡tnosti zemÃ­ Koruny ÄeskÃ© i stÃ¡tnosti ÄeskoslovenskÃ©â€¦.\" S\\xa0pozdravem Mgr. Pavel Å evÄÃ­k<|eot_id|>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA A100-SXM4-40GB. Max memory = 39.381 GB.\n",
      "2.635 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 15,789 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 64 | Total steps = 246\n",
      " \"-____-\"     Number of trainable parameters = 24,313,856\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398def6e7a22450dac07742754b3425f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111231544572446, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mlynatom/master-thesis-repository-tomas-mlynar/wandb/run-20241216_180947-eptc8ndo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlynatom/huggingface/runs/eptc8ndo' target=\"_blank\">outputs</a></strong> to <a href='https://wandb.ai/mlynatom/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlynatom/huggingface' target=\"_blank\">https://wandb.ai/mlynatom/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlynatom/huggingface/runs/eptc8ndo' target=\"_blank\">https://wandb.ai/mlynatom/huggingface/runs/eptc8ndo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='246' max='246' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [246/246 1:11:40, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.850800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.916100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.783200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.789200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.815600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.791700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.781400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.646200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.659600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.766400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.516200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.588000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.856800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.757800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.656800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.760600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.643900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.656400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.713000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.729300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.726700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.783600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.767300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.956100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.776500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.671000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.878000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.618100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.796100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.679200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.485400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.689000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.793500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.576500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.708900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.761000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.665500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.601300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.497600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.726600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>2.359900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>2.651500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>2.672300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>2.683400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.199600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>2.660300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>2.578900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>2.650300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>2.758700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.591400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>2.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>2.820100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>2.586600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>2.685500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>2.625200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>2.465300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>2.629200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>2.700800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>2.705400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.634600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>2.681200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>2.584800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>2.640800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>2.651600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>2.559100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>2.622500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>2.641600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>2.557600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>2.619300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.824400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>2.629800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>2.829100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>2.551900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>2.418600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.537700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>2.587900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>2.596900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>2.601500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>2.640900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.631900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>2.413000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>2.675200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>2.659100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>2.629700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>2.676400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>2.721900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>2.541500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>2.599100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>2.564900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.642100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>2.669300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>2.614700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>2.746100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>2.937200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>2.335200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>2.526200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>2.754000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>2.527200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>2.703500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.689900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>2.531000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>2.591100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>2.560300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>2.494800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>2.713300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>2.669600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>2.585000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>2.547000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>2.436600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.786300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>2.547500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>2.453900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>2.502600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>2.647700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>2.607200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>2.211600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>2.445600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>2.648900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>2.623500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.512000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>2.585200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>2.565000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>2.578100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>2.395100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>2.371200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>2.549400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>2.537200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>2.657500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>2.490100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.542300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>2.462400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>2.369500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>2.533700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>2.558900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>2.581800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>2.634900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>2.436000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>2.589100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>2.629800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.537300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>2.467900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>2.879900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>2.576000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>2.482300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>2.564500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>2.592400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>2.669800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>2.628200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>2.691600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.577500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>2.641000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>2.617100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>2.538400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>2.521100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>2.687400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>2.467300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>2.766700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>2.422800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>2.585900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.655800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>2.556500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>2.624800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>2.438300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>2.493300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>2.687200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>2.385500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>2.606300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>2.673700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>2.465900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.551200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>2.454600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>2.510000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>2.494000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>2.430400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>2.531600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>2.627600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>2.459600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>2.631800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>2.408900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.493400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>2.265200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>2.368100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>2.534700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>2.426400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>2.538300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>2.383000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>2.660600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>2.319100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>2.539700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.478100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>2.299200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>2.569400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>2.542000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>2.387600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>2.504000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>2.465800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>2.359500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>2.539000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>2.605700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.540200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>2.662000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>2.637100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>2.663400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>2.387500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>2.530400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>2.501200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>2.713300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>2.767500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>2.513300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.525900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>2.421300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>2.528800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>2.575400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>2.426600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>2.585500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>2.418300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>2.518100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>2.616400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>2.573700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.564700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>2.426800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>2.622200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>2.386900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>2.659000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>2.455500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>2.511100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>2.446600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>2.625800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>2.383500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.435500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>2.507800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>2.463300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>2.232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>2.500300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>2.383300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>2.502700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>2.566300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>2.439800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>2.427000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.464400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>2.423000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>2.547300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>2.534500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>2.672700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>2.511200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>2.473200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4329.0353 seconds used for training.\n",
      "72.15 minutes used for training.\n",
      "Peak reserved memory = 19.932 GB.\n",
      "Peak reserved memory for training = 17.297 GB.\n",
      "Peak reserved memory % of max memory = 50.613 %.\n",
      "Peak reserved memory for training % of max memory = 43.922 %.\n"
     ]
    }
   ],
   "source": [
    "#Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nKdo je nejlepÅ¡Ã­ fotbalista na svÄ›tÄ›?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nTato otÃ¡zka je vÅ¾dy aktuÃ¡lnÃ­ a zpÅ¯sobuje mnoho debat. NejlepÅ¡Ã­ fotbalista na svÄ›tÄ› je subjektivnÃ­ a mÅ¯Å¾e se liÅ¡it podle osobnÃ­ch preferencÃ­, statistik a individuÃ¡lnÃ­ch vÃ½konÅ¯. NicmÃ©nÄ›, podle mnoha odbornÃ­kÅ¯ a fanouÅ¡kÅ¯, jsou mezi nejlepÅ¡Ã­mi fotbalisty na svÄ›tÄ›:\\n\\n1. **Lionel Messi**: ArgentinskÃ½ ÃºtoÄnÃ­k, kterÃ½ hraje za Paris Saint-Germain, je Äasto povaÅ¾ovÃ¡n za jednoho z nejlepÅ¡Ã­ch fotbalistÅ¯ vÅ¡ech dob. Jeho vÃ½jimeÄnÃ© technickÃ© dovednosti, rychlost, driblovÃ¡nÃ­ a stÅ™eleckÃ© schopnosti ho ÄinÃ­ ÃºÅ¾asnÃ½m hrÃ¡Äem.\\n2. **Cristiano Ronaldo**: PortugalskÃ½ ÃºtoÄnÃ­k, kterÃ½ hraje za Al-Nassr, je dalÅ¡Ã­ velkÃ½m jmÃ©nem ve fotbalovÃ©m svÄ›tÄ›. Jeho neuvÄ›Å™itelnÃ¡ sÃ­la, rychlost, stÅ™eleckÃ© schopnosti a vedenÃ­ na hÅ™iÅ¡ti ho ÄinÃ­ jednÃ­m z nejlepÅ¡Ã­ch hrÃ¡ÄÅ¯ souÄasnosti.\\n3. **Kylian MbappÃ©**: FrancouzskÃ½ ÃºtoÄnÃ­k, kterÃ½ hraje za Paris Saint-Germain, je mladÃ½m a talentovanÃ½m hrÃ¡Äem, kterÃ½ jiÅ¾ dosÃ¡hl mnoha ÃºspÄ›chÅ¯ ve fotbalovÃ©m svÄ›tÄ›. Jeho rychlost, technickÃ© dovednosti a stÅ™eleckÃ© schopnosti ho ÄinÃ­ jednÃ­m z nejÅ¾Ã¡danÄ›jÅ¡Ã­ch hrÃ¡ÄÅ¯ souÄasnosti.\\n\\nSamozÅ™ejmÄ›, Å¾e existujÃ­ i jinÃ­ vÃ½bornÃ­ fotbalistÃ©, jako napÅ™Ã­klad **Robert Lewandowski**, **Neymar**, **Mohamed Salah** a dalÅ¡Ã­, kteÅ™Ã­ takÃ© patÅ™Ã­ mezi nejlepÅ¡Ã­ hrÃ¡Äe souÄasnosti.\\n\\nKterÃ½ fotbalista je podle tebe nejlepÅ¡Ã­?<|eot_id|>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Kdo je nejlepÅ¡Ã­ fotbalista na svÄ›tÄ›?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 2048, use_cache = True,\n",
    "                         temperature = 1, min_p = 0.1)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nMarie Terezie je<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nDobrÃ½ den, Marie Terezie se v ÄeskÃ©m jazyce nazÃ½vÃ¡ Marie TerÃ©zie, je Äeskou krÃ¡lovskou krÃ¡lovnou. PozdÄ›jÃ­ se na dvÄ› ÄÃ¡sti: Marie - z Å™eckÃ©ho word Marie (krÃ¡l, prince) a Terezie - z Å™eckÃ©ho word TerÃ©sia (krÃ¡l) a to v dobÄ›, kdy se krÃ¡lovna pozdÃ­vala jako krÃ¡lovna a pozdÃ­vala se jako krÃ¡lovna. DÃ¡le se podepisovala jako Marie Terezie, kde se pozdÃ­vala jako krÃ¡lovna a']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
