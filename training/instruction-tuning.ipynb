{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFTT (Instruction Tuning) using Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: unsloth 2024.12.4\n",
      "Uninstalling unsloth-2024.12.4:\n",
      "  Successfully uninstalled unsloth-2024.12.4\n",
      "Collecting git+https://github.com/unslothai/unsloth.git\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /data/temporary/pip-req-build-cpa8fvu0\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /data/temporary/pip-req-build-cpa8fvu0\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit 7accbe89914be05b7de653943918f12527d213d6\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: unsloth\n",
      "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unsloth: filename=unsloth-2024.12.12-py3-none-any.whl size=175163 sha256=e47f034a69d1017908e329220fb7a95cb5169733e30b2fcf2ccf56c3a7ea6a38\n",
      "  Stored in directory: /data/temporary/pip-ephem-wheel-cache-6vm7dd6j/wheels/60/3e/1f/e576c07051d90cf64b6a41434d87ccf4db33fafd5343bf5de0\n",
      "Successfully built unsloth\n",
      "Installing collected packages: unsloth\n",
      "Successfully installed unsloth-2024.12.12\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: unsloth_zoo in ./venv/master_venv/lib/python3.12/site-packages (2024.12.1)\n",
      "Collecting unsloth_zoo\n",
      "  Downloading unsloth_zoo-2024.12.7-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: torch in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (2.5.1)\n",
      "Requirement already satisfied: triton in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (3.1.0)\n",
      "Requirement already satisfied: packaging in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (24.1)\n",
      "Requirement already satisfied: tyro in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (0.9.2)\n",
      "Requirement already satisfied: transformers>=4.46.1 in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (4.46.3)\n",
      "Requirement already satisfied: datasets>=2.16.0 in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (3.0.1)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (0.2.0)\n",
      "Requirement already satisfied: tqdm in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (4.66.5)\n",
      "Requirement already satisfied: psutil in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (6.1.0)\n",
      "Requirement already satisfied: wheel>=0.42.0 in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (0.45.1)\n",
      "Requirement already satisfied: numpy in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (2.1.2)\n",
      "Requirement already satisfied: accelerate>=0.34.1 in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (1.0.1)\n",
      "Requirement already satisfied: trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (0.12.1)\n",
      "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (0.13.2)\n",
      "Requirement already satisfied: protobuf<4.0.0 in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (3.20.3)\n",
      "Requirement already satisfied: huggingface_hub in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (0.26.1)\n",
      "Requirement already satisfied: hf_transfer in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (0.1.8)\n",
      "Requirement already satisfied: cut_cross_entropy in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (24.11.4)\n",
      "Requirement already satisfied: pillow in ./venv/master_venv/lib/python3.12/site-packages (from unsloth_zoo) (11.0.0)\n",
      "Requirement already satisfied: pyyaml in ./venv/master_venv/lib/python3.12/site-packages (from accelerate>=0.34.1->unsloth_zoo) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./venv/master_venv/lib/python3.12/site-packages (from accelerate>=0.34.1->unsloth_zoo) (0.4.5)\n",
      "Requirement already satisfied: filelock in ./venv/master_venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth_zoo) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./venv/master_venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth_zoo) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./venv/master_venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth_zoo) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./venv/master_venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth_zoo) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./venv/master_venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth_zoo) (2.32.3)\n",
      "Requirement already satisfied: xxhash in ./venv/master_venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth_zoo) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in ./venv/master_venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth_zoo) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in ./venv/master_venv/lib/python3.12/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.16.0->unsloth_zoo) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in ./venv/master_venv/lib/python3.12/site-packages (from datasets>=2.16.0->unsloth_zoo) (3.10.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/master_venv/lib/python3.12/site-packages (from huggingface_hub->unsloth_zoo) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (12.4.127)\n",
      "Requirement already satisfied: setuptools in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (75.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./venv/master_venv/lib/python3.12/site-packages (from torch->unsloth_zoo) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/master_venv/lib/python3.12/site-packages (from sympy==1.13.1->torch->unsloth_zoo) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/master_venv/lib/python3.12/site-packages (from transformers>=4.46.1->unsloth_zoo) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./venv/master_venv/lib/python3.12/site-packages (from transformers>=4.46.1->unsloth_zoo) (0.20.1)\n",
      "Requirement already satisfied: rich in ./venv/master_venv/lib/python3.12/site-packages (from trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth_zoo) (13.9.4)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in ./venv/master_venv/lib/python3.12/site-packages (from tyro->unsloth_zoo) (0.16)\n",
      "Requirement already satisfied: shtab>=1.5.6 in ./venv/master_venv/lib/python3.12/site-packages (from tyro->unsloth_zoo) (1.7.1)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in ./venv/master_venv/lib/python3.12/site-packages (from tyro->unsloth_zoo) (4.4.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./venv/master_venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/master_venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/master_venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/master_venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/master_venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./venv/master_venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.16.0->unsloth_zoo) (1.15.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/master_venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth_zoo) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/master_venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth_zoo) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/master_venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth_zoo) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/master_venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth_zoo) (2024.8.30)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/master_venv/lib/python3.12/site-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth_zoo) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/master_venv/lib/python3.12/site-packages (from rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth_zoo) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/master_venv/lib/python3.12/site-packages (from jinja2->torch->unsloth_zoo) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/master_venv/lib/python3.12/site-packages (from pandas->datasets>=2.16.0->unsloth_zoo) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/master_venv/lib/python3.12/site-packages (from pandas->datasets>=2.16.0->unsloth_zoo) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/master_venv/lib/python3.12/site-packages (from pandas->datasets>=2.16.0->unsloth_zoo) (2024.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venv/master_venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->trl!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9->unsloth_zoo) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/master_venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth_zoo) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./venv/master_venv/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.16.0->unsloth_zoo) (0.2.0)\n",
      "Downloading unsloth_zoo-2024.12.7-py3-none-any.whl (71 kB)\n",
      "Installing collected packages: unsloth_zoo\n",
      "  Attempting uninstall: unsloth_zoo\n",
      "    Found existing installation: unsloth_zoo 2024.12.1\n",
      "    Uninstalling unsloth_zoo-2024.12.1:\n",
      "      Successfully uninstalled unsloth_zoo-2024.12.1\n",
      "Successfully installed unsloth_zoo-2024.12.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: wandb in ./venv/master_venv/lib/python3.12/site-packages (0.19.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in ./venv/master_venv/lib/python3.12/site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in ./venv/master_venv/lib/python3.12/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in ./venv/master_venv/lib/python3.12/site-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in ./venv/master_venv/lib/python3.12/site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in ./venv/master_venv/lib/python3.12/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in ./venv/master_venv/lib/python3.12/site-packages (from wandb) (6.1.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in ./venv/master_venv/lib/python3.12/site-packages (from wandb) (2.10.3)\n",
      "Requirement already satisfied: pyyaml in ./venv/master_venv/lib/python3.12/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in ./venv/master_venv/lib/python3.12/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in ./venv/master_venv/lib/python3.12/site-packages (from wandb) (2.19.2)\n",
      "Requirement already satisfied: setproctitle in ./venv/master_venv/lib/python3.12/site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in ./venv/master_venv/lib/python3.12/site-packages (from wandb) (75.2.0)\n",
      "Requirement already satisfied: six>=1.4.0 in ./venv/master_venv/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./venv/master_venv/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/master_venv/lib/python3.12/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in ./venv/master_venv/lib/python3.12/site-packages (from pydantic<3,>=2.6->wandb) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./venv/master_venv/lib/python3.12/site-packages (from pydantic<3,>=2.6->wandb) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/master_venv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/master_venv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/master_venv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/master_venv/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in ./venv/master_venv/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#install latest version of unsloth\n",
    "#%pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "\n",
    "#%pip install --upgrade unsloth\n",
    "%pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
    "%pip install --upgrade unsloth_zoo\n",
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from unsloth.chat_templates import get_chat_template, train_on_responses_only\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "\n",
    "# random state\n",
    "SEED = 42\n",
    "\n",
    "# reproducibility\n",
    "## torch\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "## python\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "## numpy\n",
    "import numpy as np\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.12: Fast Llama patching. Transformers: 4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.381 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 8192 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.12.12 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = SEED,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2f34eefa1e4f8486a1b77baad3adcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6456d9b263944bfb8b002c6ec6603c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/82.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6833d2a25c349139bf7e645273356ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/3.57M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b366a4de9f04ab49cccda4c7926cf96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/3.56M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e59682959b944848b3d441f0ea7c5ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/103582 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51cf5c514c34199ae0e6335abfa03e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3451 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f14859b07f34c6e8a1436e82d30e79f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3549 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "dataset = load_dataset(\"ctu-aic/cs_instruction_tuning_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_ask_library(dataset):\n",
    "    \"\"\"Standardize dataset to hugginface conversations format\"\"\"\n",
    "    # filter cs language only\n",
    "    dataset = dataset.filter(lambda x: x[\"language\"] == \"cs\")\n",
    "\n",
    "    questions = dataset[\"question\"]\n",
    "    answers = dataset[\"answer\"]\n",
    "\n",
    "\n",
    "    conversations = []\n",
    "    for q, a in zip(questions, answers):\n",
    "        user = {\"role\": \"user\", \"content\": q}\n",
    "        assistant = {\"role\": \"assistant\", \"content\": a}\n",
    "        conversations.append([user, assistant])\n",
    "\n",
    "    dataset = dataset.add_column(\"conversations\", conversations)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2df947ed7c64e218046f645ca911b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/103557 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811e94a216fe4fe28d0921d9767dcd8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e37f703d3ee4d2381388a9fdc3fb06e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3548 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#dataset = standardize_ask_library(dataset)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Dostanete 3 kl√≠ƒçov√° slova, vygenerujte p≈ôesvƒõdƒçivou vƒõtu na dan√© t√©ma.\\n\\nPsychologie, komunikace, vztahy',\n",
       "  'role': 'user'},\n",
       " {'content': 'Spr√°vn√° komunikace je nezbytn√° pro zlep≈°en√≠ vztah≈Ø, a proto je d≈Øle≈æit√© se sezn√°mit s psychologick√Ωmi aspekty v t√©to oblasti.',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][100][\"conversations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nKlasifikujte tento ƒçl√°nek jako n√°zor, mƒõs√≠ƒçn√≠ zpr√°vu, ekonomickou anal√Ωzu.\\n\\nTento ƒçl√°nek pojedn√°v√° o tom, jak by ned√°vn√© vl√°dn√≠ reformy mohly v√©st ke zv√Ω≈°en√≠ ekonomick√Ωch p≈ô√≠le≈æitost√≠ v zemi.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nEkonomick√° anal√Ωza.<|eot_id|>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][100][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd280251dce48ae9f17d93b257ddc71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/103557 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cee4dc4e3894861a7126a8b1d7ebf44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/3450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.other:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset[\"train\"],\n",
    "    eval_dataset = dataset[\"validation\"],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 32,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_ratio = 0.01,\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        #max_steps = 60,\n",
    "        learning_rate = 1e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 5,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = SEED,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"wandb\", # Use this for WandB etc\n",
    "        run_name=\"llama3.2-3b-instruct-1epoch-32batch-4gradacc-1e-4lr\",\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = 50,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8c446c322541c88785a0d8a9ca49b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/103557 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2bad8256ed4d60b0e2ff2165c95c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nKartavya Path byla naz√Ωv√°na ve jm√©nu<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nzodpovƒõdnosti a povinnosti. Tento term√≠n se obvykle pou≈æ√≠v√° v hinduistick√© filozofii a kultu≈ôe a zd≈Øraz≈àuje d≈Øle≈æitost plnƒõn√≠ sv√Ωch povinnost√≠ a zodpovƒõdnost√≠ v≈Øƒçi rodinƒõ, spoleƒçnosti a celkovƒõ v≈Øƒçi ≈æivotu. Je to filozofie, kter√° n√°s vede k tomu, abychom neuplat≈àovali sv√° pr√°va, ale abychom se zamƒõ≈ôili na to, co mus√≠me udƒõlat pro ostatn√≠ a pro svƒõt jako celek.<|eot_id|>'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                              \\n\\nzodpovƒõdnosti a povinnosti. Tento term√≠n se obvykle pou≈æ√≠v√° v hinduistick√© filozofii a kultu≈ôe a zd≈Øraz≈àuje d≈Øle≈æitost plnƒõn√≠ sv√Ωch povinnost√≠ a zodpovƒõdnost√≠ v≈Øƒçi rodinƒõ, spoleƒçnosti a celkovƒõ v≈Øƒçi ≈æivotu. Je to filozofie, kter√° n√°s vede k tomu, abychom neuplat≈àovali sv√° pr√°va, ale abychom se zamƒõ≈ôili na to, co mus√≠me udƒõlat pro ostatn√≠ a pro svƒõt jako celek.<|eot_id|>'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA A100-SXM4-40GB. Max memory = 39.381 GB.\n",
      "2.635 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 103,557 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 32 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 128 | Total steps = 809\n",
      " \"-____-\"     Number of trainable parameters = 24,313,856\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmlynatom\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mlynatom/master-thesis-repository-tomas-mlynar/wandb/run-20250106_212639-to9pz44r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mlynatom/huggingface/runs/to9pz44r' target=\"_blank\">llama3.2-3b-instruct-1epoch-32batch-4gradacc-1e-4lr</a></strong> to <a href='https://wandb.ai/mlynatom/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mlynatom/huggingface' target=\"_blank\">https://wandb.ai/mlynatom/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mlynatom/huggingface/runs/to9pz44r' target=\"_blank\">https://wandb.ai/mlynatom/huggingface/runs/to9pz44r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='809' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  9/809 04:41 < 8:57:00, 0.02 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.513900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:157\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m<string>:380\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m<string>:64\u001b[0m, in \u001b[0;36m_unsloth_training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/accelerate/accelerator.py:2246\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2244\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2245\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2246\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/master_venv/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4329.0353 seconds used for training.\n",
      "72.15 minutes used for training.\n",
      "Peak reserved memory = 19.932 GB.\n",
      "Peak reserved memory for training = 17.297 GB.\n",
      "Peak reserved memory % of max memory = 50.613 %.\n",
      "Peak reserved memory for training % of max memory = 43.922 %.\n"
     ]
    }
   ],
   "source": [
    "#Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nKdo je nejlep≈°√≠ fotbalista na svƒõtƒõ?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nTato ot√°zka je v≈ædy aktu√°ln√≠ a zp≈Øsobuje mnoho debat. Nejlep≈°√≠ fotbalista na svƒõtƒõ je subjektivn√≠ a m≈Ø≈æe se li≈°it podle osobn√≠ch preferenc√≠, statistik a individu√°ln√≠ch v√Ωkon≈Ø. Nicm√©nƒõ, podle mnoha odborn√≠k≈Ø a fanou≈°k≈Ø, jsou mezi nejlep≈°√≠mi fotbalisty na svƒõtƒõ:\\n\\n1. **Lionel Messi**: Argentinsk√Ω √∫toƒçn√≠k, kter√Ω hraje za Paris Saint-Germain, je ƒçasto pova≈æov√°n za jednoho z nejlep≈°√≠ch fotbalist≈Ø v≈°ech dob. Jeho v√Ωjimeƒçn√© technick√© dovednosti, rychlost, driblov√°n√≠ a st≈ôeleck√© schopnosti ho ƒçin√≠ √∫≈æasn√Ωm hr√°ƒçem.\\n2. **Cristiano Ronaldo**: Portugalsk√Ω √∫toƒçn√≠k, kter√Ω hraje za Al-Nassr, je dal≈°√≠ velk√Ωm jm√©nem ve fotbalov√©m svƒõtƒõ. Jeho neuvƒõ≈ôiteln√° s√≠la, rychlost, st≈ôeleck√© schopnosti a veden√≠ na h≈ôi≈°ti ho ƒçin√≠ jedn√≠m z nejlep≈°√≠ch hr√°ƒç≈Ø souƒçasnosti.\\n3. **Kylian Mbapp√©**: Francouzsk√Ω √∫toƒçn√≠k, kter√Ω hraje za Paris Saint-Germain, je mlad√Ωm a talentovan√Ωm hr√°ƒçem, kter√Ω ji≈æ dos√°hl mnoha √∫spƒõch≈Ø ve fotbalov√©m svƒõtƒõ. Jeho rychlost, technick√© dovednosti a st≈ôeleck√© schopnosti ho ƒçin√≠ jedn√≠m z nej≈æ√°danƒõj≈°√≠ch hr√°ƒç≈Ø souƒçasnosti.\\n\\nSamoz≈ôejmƒõ, ≈æe existuj√≠ i jin√≠ v√Ωborn√≠ fotbalist√©, jako nap≈ô√≠klad **Robert Lewandowski**, **Neymar**, **Mohamed Salah** a dal≈°√≠, kte≈ô√≠ tak√© pat≈ô√≠ mezi nejlep≈°√≠ hr√°ƒçe souƒçasnosti.\\n\\nKter√Ω fotbalista je podle tebe nejlep≈°√≠?<|eot_id|>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Kdo je nejlep≈°√≠ fotbalista na svƒõtƒõ?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 2048, use_cache = True,\n",
    "                         temperature = 1, min_p = 0.1)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nMarie Terezie je<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nDobr√Ω den, Marie Terezie se v ƒçesk√©m jazyce naz√Ωv√° Marie Ter√©zie, je ƒçeskou kr√°lovskou kr√°lovnou. Pozdƒõj√≠ se na dvƒõ ƒç√°sti: Marie - z ≈ôeck√©ho word Marie (kr√°l, prince) a Terezie - z ≈ôeck√©ho word Ter√©sia (kr√°l) a to v dobƒõ, kdy se kr√°lovna pozd√≠vala jako kr√°lovna a pozd√≠vala se jako kr√°lovna. D√°le se podepisovala jako Marie Terezie, kde se pozd√≠vala jako kr√°lovna a']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
