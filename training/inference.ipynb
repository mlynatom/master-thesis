{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./venv/unsloth_venv/lib/python3.11/site-packages (4.47.1)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in /mnt/appl/software/Python-bundle-PyPI/2023.10-GCCcore-13.2.0/lib/python3.11/site-packages (from transformers) (3.13.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./venv/unsloth_venv/lib/python3.11/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/appl/software/SciPy-bundle/2023.11-gfbf-2023b/lib/python3.11/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/appl/software/hatchling/1.18.0-GCCcore-13.2.0/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/appl/software/PyYAML/6.0.1-GCCcore-13.2.0/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/appl/software/Python-bundle-PyPI/2023.10-GCCcore-13.2.0/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /mnt/appl/software/Python-bundle-PyPI/2023.10-GCCcore-13.2.0/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./venv/unsloth_venv/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./venv/unsloth_venv/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venv/unsloth_venv/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /mnt/appl/software/Python-bundle-PyPI/2023.10-GCCcore-13.2.0/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/appl/software/typing-extensions/4.11.0-GCCcore-13.2.0/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/appl/software/Python-bundle-PyPI/2023.10-GCCcore-13.2.0/lib/python3.11/site-packages (from requests->transformers) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/appl/software/Python-bundle-PyPI/2023.10-GCCcore-13.2.0/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/appl/software/Python-bundle-PyPI/2023.10-GCCcore-13.2.0/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/appl/software/Python-bundle-PyPI/2023.10-GCCcore-13.2.0/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.47.1\n",
      "    Uninstalling transformers-4.47.1:\n",
      "      Successfully uninstalled transformers-4.47.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autoawq 0.2.8 requires transformers<=4.47.1,>=4.45.0, but you have transformers 4.51.3 which is incompatible.\n",
      "unsloth 2025.4.7 requires protobuf<4.0.0, but you have protobuf 4.25.3 which is incompatible.\n",
      "unsloth-zoo 2025.4.4 requires protobuf<4.0.0, but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed transformers-4.51.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextStreamer\n",
    "\n",
    "\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tested_model = \"/mnt/personal/mlynatom/thesis_models/cp_Llama-3.1-8B-cs_expand_5M_subword_resizing-full_fineweb-2_seed42_samples500000/merge_16bit\"\n",
    "#tested_model = \"meta-llama/Llama-3.1-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.1.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.254 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270ef72fd33b44c59c8d0a684a0719ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = tested_model, # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " udƒõlat takov√Ω ƒçl√°nek abych mohl vidƒõt jak√© vƒõci dƒõl√°te v≈°imli jste nƒõjak√© zaj√≠mavƒõj≈°√≠ vƒõci ne≈æ m≈Øj ƒçl√°nek urƒçitƒõ jste mƒõli nƒõjak√© zaj√≠mavƒõj≈°√≠ vƒõci ne≈æ m≈Øj ƒçl√°nek urƒçitƒõ jste mƒõli nƒõjak√© zaj√≠mavƒõj≈°√≠ vƒõci ne≈æ m≈Øj ƒçl√°nek urƒçitƒõ jste mƒõli nƒõjak√© zaj√≠mavƒõj≈°√≠ vƒõci ne≈æ m≈Øj ƒçl√°nek urƒçitƒõ jste mƒõli nƒõjak√© zaj√≠mavƒõj≈°√≠ vƒõci ne≈æ m≈Øj ƒçl√°nek urƒçitƒõ jste mƒõli nƒõjak√© zaj√≠mavƒõj≈°√≠ vƒõci ne≈æ m≈Øj ƒçl√°nek urƒçitƒõ jste mƒõli nƒõjak√© zaj√≠mavƒõj≈°√≠ vƒõci ne≈æ m≈Øj ƒçl√°nek urƒçitƒõ jste mƒõli nƒõjak√© zaj√≠mavƒõj≈°√≠ vƒõci ne≈æ m≈Øj ƒçl√°nek urƒçitƒõ jste mƒõli nƒõjak√© zaj√≠mavƒõj≈°√≠ vƒõci ne≈æ m≈Øj ƒçl√°nek urƒçitƒõ jste mƒõli nƒõjak√© zaj√≠mavƒõj≈°√≠ vƒõci ne≈æ m≈Øj ƒçl√°nek urƒçitƒõ jste mƒõli nƒõjak√© zaj√≠mavƒõj≈°√≠ vƒõci ne≈æ m≈Øj ƒçl√°nek urƒçitƒõ jste mƒõli nƒõjak√© zaj√≠mavƒõj≈°√≠ vƒõci ne≈æ\n"
     ]
    }
   ],
   "source": [
    "input_text = \"J√° jsem se rozhodl\"\n",
    "inputs = tokenizer(\n",
    "    input_text,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=max_seq_length,\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Set up the text streamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "# Generate the output\n",
    "_ = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=128,\n",
    "    use_cache=True,\n",
    "    temperature=0.1,\n",
    "    min_p=0.1,\n",
    "    repetition_penalty=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", ≈æe si na blogu zalo≈æ√≠m novou rubriku. A to je \"Zpr√°vy ze svƒõta\". V n√≠ bych r√°d p≈ôin√°≈°el informace o tom co se dƒõje v r≈Øzn√Ωch ƒç√°stech svƒõta a hlavnƒõ ty kter√© nejsou tak ƒçasto hl√°≈°eny jako t≈ôeba teroristick√© √∫toky nebo v√°lka.\n",
      "V tomto d√≠le Zpr√°v ze svƒõta v√°m p≈ôedstavuji 5 zaj√≠mav√Ωch informac√≠:\n",
      "1)¬†Nejvƒõt≈°√≠ d√°lnice svƒõta\n",
      "Dnes u≈æ nen√≠ tajemstv√≠m, ≈æe nejvƒõt≈°√≠ d√°lnici svƒõta m√° Indie. Je dlouh√° p≈ôes 9 tis√≠c kilometr≈Ø a vede od severn√≠ho\n"
     ]
    }
   ],
   "source": [
    "input_text = \"J√° jsem se rozhodl\"\n",
    "inputs = tokenizer(\n",
    "    input_text,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=max_seq_length,\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Set up the text streamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "# Generate the output\n",
    "_ = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=128,\n",
    "    use_cache=True,\n",
    "    temperature=0.1,\n",
    "    min_p=0.1,\n",
    "    repetition_penalty=1.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " udƒõlat nƒõco jin√©ho ne≈æ ostatn√≠ lid√© dƒõlaj√≠ teƒè tady kolem n√°s v≈°ichni ≈ôe≈°√≠ probl√©my svƒõta ale nikdo nedok√°≈æe pomoci tomu ≈æe svƒõt jde dolu proto≈æe ka≈æd√Ω chce m√≠t svoji vƒõc lep≈°√≠ vƒõt≈°√≠ rychlej≈°√≠ atd prostƒõ nikdo nem√° ƒças pom√Ω≈°let t≈ôeba jenom trochu jinak j√° chtƒõl uk√°zat v≈°em hlavnƒõ mlad√Ωm lidem kdo mysl√≠ stejnƒõ jako mnƒõ jak√Ω m√°me velk√Ω v√Ωbƒõr pr√°ce vlastnƒõ ≈æ√°dn√Ω takov√Ω ƒçlovƒõk nikdy nebude st√°t √∫plnƒõ s√°m v≈ædy m≈Ø≈æeme naj√≠t nƒõkoho dal≈°√≠ho p≈ôesnƒõ toho ƒçlovƒõka kter√Ω hled√°me a≈• u≈æ t√≠m mysl√≠me lidi kte≈ô√≠ maj√≠ nƒõjak√© zku≈°enosti nebo naopak pr√°vƒõ dƒõti kter√Ωm je≈°tƒõ nen√≠ ani ≈°estn√°ct let opravdu m√°te mo≈ænost zvolit pr√°ci kter√° v√°m vyhovuje\n"
     ]
    }
   ],
   "source": [
    "input_text = \"J√° jsem se rozhodl\"\n",
    "inputs = tokenizer(\n",
    "    input_text,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=max_seq_length,\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Set up the text streamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "# Generate the output\n",
    "_ = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=128,\n",
    "    use_cache=True,\n",
    "    temperature=0.1,\n",
    "    min_p=0.1,\n",
    "    repetition_penalty=1.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAhoj, pot≈ôebuju pomoci s v√Ωbƒõrem nov√©ho telefonu.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      3\u001b[0m ]\n\u001b[0;32m----> 4\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Must add for generation\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(input_ids \u001b[38;5;241m=\u001b[39m inputs, streamer \u001b[38;5;241m=\u001b[39m text_streamer, max_new_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m     14\u001b[0m                    use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m, min_p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m, repetition_penalty \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.2\u001b[39m,)\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/unsloth_venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1629\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1627\u001b[0m     tokenizer_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1629\u001b[0m chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_assistant_tokens_mask \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*generation\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m, chat_template):\n\u001b[1;32m   1632\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   1633\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_assistant_tokens_mask==True but chat template does not contain `\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;132;01m% g\u001b[39;00m\u001b[38;5;124meneration \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m}` keyword.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1634\u001b[0m     )\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/unsloth_venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1822\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.get_chat_template\u001b[0;34m(self, chat_template, tools)\u001b[0m\n\u001b[1;32m   1820\u001b[0m         chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_template\n\u001b[1;32m   1821\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1823\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use chat template functions because tokenizer.chat_template is not set and no template \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1824\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument was passed! For information about writing templates and setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1825\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer.chat_template attribute, please see the documentation at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1826\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/chat_templating\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1827\u001b[0m         )\n\u001b[1;32m   1829\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chat_template\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Ahoj, pot≈ôebuju pomoci s v√Ωbƒõrem nov√©ho telefonu.\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 0.7, min_p = 0.1, repetition_penalty = 1.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dnes je pondƒõl√≠. Na dne≈°n√≠m cviƒçen√≠ se nauƒç√≠me, jak pracovat s daty a ƒç√≠seln√Ωmi operacemi v jazyce Python.\n",
      "\n",
      "Vytvo≈ôte si nov√Ω soubor ve sv√©m editoru (nebo na webu https://repl.it/languages/python3) a p≈ôejmenujte ho jako \"cviceni1.py\". Vlo≈æte do nƒõj n√°sleduj√≠c√≠ k√≥d:\n",
      "\n",
      "```python\n",
      "print(\"Hello World!\")\n",
      "```\n",
      "\n",
      "Spus≈•te jej pomoc√≠ p≈ô√≠kazu `run` nebo stisknƒõte tlaƒç√≠tko \"Run\" v horn√≠m menu replitu.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Kde domov m≈Øj?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 0.1, min_p = 0.1, repetition_penalty = 1.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dobr√Ω den! J√° jsem AI, tak≈æe nem√°m emoce jako lid√©. M√°m b√Ωt p≈ôipraven√Ω pomoci v√°m s jak√©koli ot√°zkou nebo probl√©mem, kter√Ω m√°te. Jak m≈Ø≈æete mƒõ potrestovat?<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Jak se m√°≈°?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 0.1, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoj! Vyberanie nov√©ho telefonu je ƒçasto obt√≠≈æn√©, ale budem v√°m r√°dn√Ω pomoci.\n",
      "\n",
      "M≈Ø≈æete mi ≈ô√≠ct, co v√°s zaj√≠m√° v nov√©m telefonu? M√°te nƒõjak√© specifick√© po≈æadavky nebo preference? Nap≈ô√≠klad:\n",
      "\n",
      "- Jak√Ω typ telefonu chcete (Android, iOS, hybridn√≠)?\n",
      "- Jak√Ω rozpoƒçet m√°te?\n",
      "- Chcete telefon s kamerou, nebo je to pro v√°s nƒõco jin√©ho?\n",
      "- M√°te nƒõjak√© specifick√© funkce nebo funkce, kter√© chcete m√≠t v telefonu?\n",
      "- Chcete telefon s velkou bateri√≠, nebo je\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Ahoj, pot≈ôebuju pomoci s v√Ωbƒõrem nov√©ho telefonu.\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 0.1, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V ƒçe≈°tinƒõ se pou≈æ√≠vaj√≠ dvƒõ varianty, ale spr√°vnou variantu je zpo≈ædƒõn√≠.\n",
      "\n",
      "Spo≈ædƒõn√≠ je neplatn√©, proto≈æe se v nƒõm pou≈æ√≠vaj√≠ dvƒõ slova, kter√© se v ƒçe≈°tinƒõ cannot be combined.\n",
      "\n",
      "Zpo≈ædƒõn√≠ je spr√°vn√©, proto≈æe se v nƒõm pou≈æ√≠vaj√≠ dvƒõ slova, kter√© se v ƒçe≈°tinƒõ mohou b√Ωt spojeny.\n",
      "\n",
      "V ƒçe≈°tinƒõ se zpo≈ædƒõn√≠ pou≈æ√≠v√° v mnoha situac√≠ch, jako je zpo≈ædƒõn√≠ pr√°ce, zpo≈ædƒõn√≠ transportu, zpo≈ædƒõn√≠ dopr\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"P√≠≈°eme zpo≈ædƒõn√≠, nebo spo≈ædƒõn√≠?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 0.1, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tested_it_models = [\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-cs_expand_5M_subword_resizing-full_cs_fineweb2-cs_finewebedu-en_31_500k_seed42_samples500000-full_cs_instruction_tuning_collection/merge_16bit\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-cs_expand_5M_subword_resizing-full_cs_fineweb2-cs_finewebedu-en_31_500k_seed42_samples500000-full_mix_11_cs_en/merge_16bit\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-cs_expand_5M_subword_resizing-full_fineweb-2_seed42_samples500000-full_cs_instruction_tuning_collection/merge_16bit\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-cs_expand_5M_subword_resizing-full_fineweb-2_seed42_samples500000-full_mix_11_cs_en/merge_16bit\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_cs_fineweb2_seed42_neptune_bs128_samples500000-full_cs_instruction_tuning_collection/merge_16bit\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_cs_fineweb2_seed42_neptune_bs128_samples500000-full_mix_11_cs_en/merge_16bit\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_fineweb2-cs_finewebedu-en_31_500k_seed42_samples500000-full_cs_instruction_tuning_collection/merge_16bit\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_fineweb2-cs_finewebedu-en_31_500k_seed42_samples500000-full_mix_11_cs_en/merge_16bit\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-Llama-3.1-8B-full_cs_instruction_tuning_collection/merge_16bit\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-Llama-3.1-8B-full_mix_11_cs_en/merge_16bit\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-Llama-3.1-8B-Instruct-full_cs_instruction_tuning_collection/merge_16bit\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-Llama-3.1-8B-Instruct-full_mix_11_cs_en/merge_16bit\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_fineweb2-cs_finewebedu-en_31_500k_seed42_samples500000-mix_11_cs_en_alpaca_dolly/merge_16bit\",\n",
    "    \"/mnt/personal/mlynatom/thesis_models/it-Llama-3.1-8B-Instruct-mix_11_cs_en_alpaca_dolly/merge_16bit\",\n",
    "\n",
    "    #\"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_fineweb2-cs_finewebedu-en_31_500k_seed42_samples500000-mix_11_cs_en_alpaca_dolly/final\",\n",
    "    #\"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_fineweb2-cs_finewebedu-en_31_500k_seed42_samples500000-full_mix_11_cs_en/final\",\n",
    "    #\"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_cs_fineweb2_seed42_neptune_bs128_samples500000-full_cs_instruction_tuning_collection/final\",\n",
    "    #\"/mnt/personal/mlynatom/thesis_models/it-Llama-3.1-8B-full_mix_11_cs_en/merge_16bit\",\n",
    "    #\"/mnt/personal/mlynatom/thesis_models/it-Llama-3.1-8B-full_mix_11_cs_en/merge_16bitv2\",\n",
    "    #\"/mnt/personal/mlynatom/thesis_models/it-Llama-3.1-8B-full_mix_11_cs_en/final\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_fineweb2-cs_finewebedu-en_31_500k_seed42_samples500000-full_mix_11_cs_en/final\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_fineweb2-cs_finewebedu-en_31_500k_seed42_samples500000-full_mix_11_cs_en/merge_16bit\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_fineweb2-cs_finewebedu-en_31_500k_seed42_samples500000-full_mix_11_cs_en/merge_16bit_v2\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_fineweb2-cs_finewebedu-en_31_500k_seed42_samples500000_v2-mix_11_cs_en/final\",\n",
    "    #\"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_fineweb2-cs_finewebedu-en_31_500k_seed42_samples500000_v2-mix_11_cs_en/merge_16bit_v2\"\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_cs_fineweb2_seed42_neptune_bs128_samples500000-full_cs_instruction_tuning_collection/merge_16bit\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_cs_fineweb2_seed42_neptune_bs128_samples500000-full_cs_instruction_tuning_collection/final\"\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_cs_fineweb2_seed42_neptune_bs128_samples500000-full_cs_instruction_tuning_collection/merge_16bit\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_cs_fineweb2_seed42_neptune_bs128_samples500000-full_cs_instruction_tuning_collection/merge_16bit_v2\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_cs_fineweb2_seed42_neptune_bs128_samples500000-full_cs_instruction_tuning_collection/final\"\n",
    "]\n",
    "\n",
    "prompts = [\n",
    "    \"Oznaƒçte entity ve vƒõtƒõ.\\n\\nBill Gates je zakladatelem spoleƒçnosti Microsoft.\", #directly from Alpaca dataset\n",
    "    \"Jak√© kapitoly by mƒõla m√≠t diplomov√° pr√°ce na t√©ma adaptace LLM?\", #creative\n",
    "    \"Co je to 6. nemoc?\", #info seeking\n",
    "    \"Napi≈° mi pl√°n uƒçen√≠ na st√°tnice z ƒçe≈°tiny.\", #reasoning\n",
    "    \"Napi≈° mi k√≥d v Pythonu, kter√Ω spoƒç√≠t√° pr≈Ømƒõr zadan√Ωch ƒç√≠sel.\", #code generation\n",
    "    \"Kolik je 2+2*2?\", #math\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "Testing model: meta-llama/Llama-3.1-8B-Instruct\n",
      "=========================================\n",
      "==((====))==  Unsloth 2025.4.7: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.254 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e0245473c8402baffb373c8a978c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Oznaƒçte entity ve vƒõtƒõ.\n",
      "\n",
      "Bill Gates je zakladatelem spoleƒçnosti Microsoft.\n",
      "=========================================\n",
      "V t√©to vƒõtƒõ jsou dvƒõ entity:\n",
      "\n",
      "1. Bill Gates - osoba (lid√©)\n",
      "2. Microsoft - organizace (spoleƒçnost)<|eot_id|>\n",
      "=========================================\n",
      "\n",
      "\n",
      "\n",
      "Prompt: Jak√© kapitoly by mƒõla m√≠t diplomov√° pr√°ce na t√©ma adaptace LLM?\n",
      "=========================================\n",
      "Diplomov√° pr√°ce na t√©ma adaptace Large Language Model≈Ø (LLM) by mƒõla m√≠t n√°sleduj√≠c√≠ kapitoly:\n",
      "\n",
      "**1. √övod**\n",
      "\n",
      "* Vyhled√°n√≠ t√©matu a jeho v√Ωznamnosti\n",
      "* C√≠l diplomov√© pr√°ce\n",
      "* Struƒçn√© shrnut√≠ obsahu pr√°ce\n",
      "\n",
      "**2. Teoretick√© pozad√≠**\n",
      "\n",
      "* Z√°kladn√≠ informace o Large Language Model≈Ø (LLM)\n",
      "* Historie v√Ωvoje LLM\n",
      "* Principy fungov√°n√≠ LLM\n",
      "* Typy LLM (transformer, LSTM, GRU, atd.)\n",
      "\n",
      "**3. Adaptace LLM**\n",
      "\n",
      "* Definice adaptace LLM\n",
      "* Typy adaptace LLM (fine-tuning, transfer learning, atd.)\n",
      "* Postupy adaptace LLM (data preparation, model tuning, atd.)\n",
      "\n",
      "**4. Metodologie**\n",
      "\n",
      "* Popis pou≈æit√Ωch metod a technik\n",
      "* Popis datov√© sady a jej√≠ p≈ô√≠pravƒõ\n",
      "* Popis model≈Ø LLM a jejich parametr≈Ø\n",
      "* Popis procesu adaptace LLM\n",
      "\n",
      "**5. V√Ωsledky**\n",
      "\n",
      "* Shrnut√≠ v√Ωsledk≈Ø adaptace LLM\n",
      "* Porovn√°n√≠ v√Ωsledk≈Ø s p≈Øvodn√≠mi modely LLM\n",
      "* Diskuze o v√Ωsledc√≠ch a jejich v√Ωznamnosti\n",
      "\n",
      "**6. Diskuze a z√°vƒõry**\n",
      "\n",
      "* Diskuze o v√Ωsledc√≠ch a jejich v√Ωznamnosti\n",
      "* Shrnut√≠ hlavn√≠ch z√°vƒõr≈Ø diplomov√© pr√°ce\n",
      "* Doporuƒçen√≠ pro budouc√≠ v√Ωzkum\n",
      "\n",
      "**7. Ochrana osobn√≠ch √∫daj≈Ø a etika**\n",
      "\n",
      "* Shrnut√≠ postup≈Ø ochrany osobn√≠ch √∫daj≈Ø\n",
      "* Shrnut√≠ etick√Ωch princip≈Ø diplomov√© pr√°ce\n",
      "\n",
      "**8. Seznam pou≈æit√© literatury**\n",
      "\n",
      "* Seznam citovan√Ωch zdroj≈Ø\n",
      "\n",
      "**9. Seznam pou≈æit√Ωch zkratek**\n",
      "\n",
      "* Seznam zkratek pou≈æit√© v diplomov√© pr√°ci\n",
      "\n",
      "**10. P≈ô√≠loha**\n",
      "\n",
      "* P≈ô√≠loha obsahuje dal≈°√≠ informace, kter√© nejsou relevantn√≠ pro hlavn√≠ text diplomov√© pr√°ce, ale kter√© mohou b√Ωt u≈æiteƒçn√© pro ƒçten√°≈ôe.<|eot_id|>\n",
      "=========================================\n",
      "\n",
      "\n",
      "\n",
      "Prompt: Co je to 6. nemoc?\n",
      "=========================================\n",
      "6. nemoc je term√≠n, kter√Ω se pou≈æ√≠v√° v souvislosti s epidemi√≠ ch≈ôipky. V roce 1957 se vyskytla pandemie ch≈ôipky, kter√° byla oznaƒçov√°na jako 2. svƒõtov√° ch≈ôipkov√° pandemie. Dal≈°√≠ pandemie ch≈ôipky se vyskytly v letech 1968 (3. svƒõtov√° ch≈ôipkov√° pandemie) a 2009 (pandemie A/H1N1).\n",
      "\n",
      "6. nemoc je nƒõkdy pou≈æ√≠v√°na jako synonymum pro pandemii ch≈ôipky, kter√° se vyskytla v roce 2019 a byla zp≈Øsobena nov√Ωm typem ch≈ôipkov√© viru HCoV-19 (dnes zn√°m√Ωm jako SARS-CoV-2). Tato pandemie byla oznaƒçov√°na jako COVID-19.\n",
      "\n",
      "Je d≈Øle≈æit√© poznamenat, ≈æe term√≠n \"6. nemoc\" nen√≠ ofici√°lnƒõ uz√°konƒõn a nen√≠ pou≈æ√≠v√°n mezin√°rodn√≠mi zdravotnick√Ωmi organizacemi.<|eot_id|>\n",
      "=========================================\n",
      "\n",
      "\n",
      "\n",
      "Prompt: Napi≈° mi pl√°n uƒçen√≠ na st√°tnice z ƒçe≈°tiny.\n",
      "=========================================\n",
      "Pl√°n uƒçen√≠ na st√°tnice z ƒçe≈°tiny je komplexn√≠ a vy≈æaduje systematickou p≈ô√≠pravu. Zde je jeden mo≈æn√Ω pl√°n uƒçen√≠:\n",
      "\n",
      "**F√°ze 1: Z√°kladn√≠ znalosti (1-3 mƒõs√≠ce)**\n",
      "\n",
      "1. **Z√°kladn√≠ gramatika**:\n",
      " * Rozd√≠ly mezi slovesn√Ωmi a p≈ô√≠slovkov√Ωmi tvarovan√≠mi\n",
      " * Pou≈æ√≠v√°n√≠ kategori√≠ ƒçasu (p≈ô√≠tomn√Ω, minul√Ω, budouc√≠)\n",
      " * Pou≈æ√≠v√°n√≠ kategori√≠ zp≈Øsobu (p≈ô√≠tomn√Ω, minul√Ω, budouc√≠)\n",
      " * Pou≈æ√≠v√°n√≠ kategori√≠ ƒç√≠sla (singul√°r, plur√°l)\n",
      "2. **Slovn√≠ z√°soba**:\n",
      " * Z√°kladn√≠ slovn√≠ z√°soba (p≈ô√≠slovce, p≈ô√≠davn√° jm√©na, podstatn√° jm√©na)\n",
      " * Pou≈æ√≠v√°n√≠ slovn√≠ z√°soby v r≈Øzn√Ωch kontextech\n",
      "3. **ƒåten√≠ a psan√≠**:\n",
      " * Z√°kladn√≠ ƒçten√≠ a psan√≠ (p≈ô√≠slovce, p≈ô√≠davn√° jm√©na, podstatn√° jm√©na)\n",
      " * Pou≈æ√≠v√°n√≠ ƒçten√≠ a psan√≠ v r≈Øzn√Ωch kontextech\n",
      "\n",
      "**F√°ze 2: Roz≈°√≠≈ôen√© znalosti (3-6 mƒõs√≠c≈Ø)**\n",
      "\n",
      "1. **Roz≈°√≠≈ôen√° gramatika**:\n",
      " * Pou≈æ√≠v√°n√≠ kategori√≠ ƒçasu a zp≈Øsobu v komplexn√≠ch vƒõt√°ch\n",
      " * Pou≈æ√≠v√°n√≠ kategori√≠ ƒç√≠sla v komplexn√≠ch vƒõt√°ch\n",
      " * Pou≈æ√≠v√°n√≠ kategori√≠ osob (j√°, ty, on, my, vy, oni)\n",
      "2. **Slovn√≠ z√°soba**:\n",
      " * Roz≈°√≠≈ôen√° slovn√≠ z√°soba (slovesa, p≈ô√≠slovce, p≈ô√≠davn√° jm√©na, podstatn√° jm√©na)\n",
      " * Pou≈æ√≠v√°n√≠ slovn√≠ z√°soby v r≈Øzn√Ωch kontextech\n",
      "3. **ƒåten√≠ a psan√≠**:\n",
      " * Roz≈°√≠≈ôen√© ƒçten√≠ a psan√≠ (slovesa, p≈ô√≠slovce, p≈ô√≠davn√° jm√©na, podstatn√° jm√©na)\n",
      " * Pou≈æ√≠v√°n√≠ ƒçten√≠ a psan√≠ v r≈Øzn√Ωch kontextech\n",
      "\n",
      "**F√°ze 3: Specifick√© znalosti (6-12 mƒõs√≠c≈Ø)**\n",
      "\n",
      "1. **Specifick√° gramatika**:\n",
      " * Pou≈æ√≠v√°n√≠ kategori√≠ ƒçasu a zp≈Øsobu v komplexn√≠ch vƒõt√°ch s p≈ô√≠slovci\n",
      " * Pou≈æ√≠v√°n√≠ kategori√≠ ƒç√≠sla v komplexn√≠ch vƒõt√°ch s p≈ô√≠slovci\n",
      " * Pou≈æ√≠v√°n√≠ kategori√≠ osob v komplexn√≠ch vƒõt√°ch s p≈ô√≠slovci\n",
      "2. **Slovn√≠ z√°soba**:\n",
      " * Specifick√° slovn√≠ z√°soba (slovesa, p≈ô√≠slovce, p≈ô√≠davn√° jm√©na, podstatn√° jm√©na)\n",
      " * Pou≈æ√≠v√°n√≠ slovn√≠ z√°soby v r≈Øzn√Ωch kontextech\n",
      "3. **ƒåten√≠ a psan√≠**:\n",
      " * Specifick√© ƒçten√≠ a psan√≠ (slovesa, p≈ô√≠slovce, p≈ô√≠davn√° jm√©na, podstatn√° jm√©na)\n",
      " * Pou≈æ√≠v√°n√≠ ƒçten√≠ a psan√≠ v r≈Øzn√Ωch kontextech\n",
      "\n",
      "**F√°ze 4: √öpln√© znalosti (1-2 roky)**\n",
      "\n",
      "1. **√öpln√° gramatika**:\n",
      " * Pou≈æ√≠v√°n√≠ kategori√≠ ƒçasu a zp≈Øsobu v komplexn√≠ch vƒõt√°ch s p≈ô√≠slovci a p≈ô√≠davn√Ωmi jm√©ny\n",
      " * Pou≈æ√≠v√°n√≠ kategori√≠ ƒç√≠sla v komplexn√≠ch vƒõt√°ch s p≈ô√≠slovci a p≈ô√≠davn√Ωmi jm√©ny\n",
      " * Pou≈æ√≠v√°n√≠ kategori√≠ osob v komplexn√≠ch vƒõt√°ch s p≈ô√≠slovci a p≈ô√≠davn√Ωmi jm√©ny\n",
      "2. **Slovn√≠ z√°soba**:\n",
      " * √öpln√° slovn√≠ z√°soba (slovesa, p≈ô√≠slovce, p≈ô√≠davn√° jm√©na, podstatn√° jm√©na)\n",
      " * Pou≈æ√≠v√°n√≠ slovn√≠ z√°soby v r≈Øzn√Ωch kontextech\n",
      "3. **ƒåten√≠ a psan√≠**:\n",
      " * √öpln√© ƒçten√≠ a psan√≠ (slovesa, p≈ô√≠slovce, p≈ô√≠davn√° jm√©na, podstatn√° jm√©na)\n",
      " * Pou≈æ√≠v√°n√≠ ƒçten√≠ a psan√≠ v r≈Øzn√Ωch kontextech\n",
      "\n",
      "Tento pl√°n uƒçen√≠ je pouze orientaƒçn√≠ a m≈Ø≈æe se li≈°it v z√°vislosti na individu√°ln√≠ch pot≈ôeb√°ch a schopnostech.<|eot_id|>\n",
      "=========================================\n",
      "\n",
      "\n",
      "\n",
      "Prompt: Napi≈° mi k√≥d v Pythonu, kter√Ω spoƒç√≠t√° pr≈Ømƒõr zadan√Ωch ƒç√≠sel.\n",
      "=========================================\n",
      "**Spoƒç√≠t√°n√≠ pr≈Ømƒõru zadan√Ωch ƒç√≠sel v Pythonu**\n",
      "\n",
      "N√°sleduj√≠c√≠ k√≥d spoƒç√≠t√° pr≈Ømƒõr zadan√Ωch ƒç√≠sel. K√≥d pou≈æ√≠v√° seznam ƒç√≠sel a funkce `sum()` a `len()` pro v√Ωpoƒçet pr≈Ømƒõru.\n",
      "\n",
      "```python\n",
      "def spocti_prumer(cisla):\n",
      "    \"\"\"\n",
      "    Spoƒç√≠t√° pr≈Ømƒõr zadan√Ωch ƒç√≠sel.\n",
      "\n",
      "    Args:\n",
      "        cisla (list): Seznam ƒç√≠sel.\n",
      "\n",
      "    Returns:\n",
      "        float: Pr≈Ømƒõr zadan√Ωch ƒç√≠sel.\n",
      "    \"\"\"\n",
      "    if not cisla:\n",
      "        raise ValueError(\"Seznam ƒç√≠sel je pr√°zdn√Ω.\")\n",
      "\n",
      "    return sum(cisla) / len(cisla)\n",
      "\n",
      "# Pou≈æit√≠ funkce\n",
      "cisla = [1, 2, 3, 4, 5]\n",
      "prumer = spocti_prumer(cisla)\n",
      "print(f\"Pr≈Ømƒõr zadan√Ωch ƒç√≠sel je: {prumer}\")\n",
      "```\n",
      "\n",
      "**Jak k√≥d funguje:**\n",
      "\n",
      "1. Definujeme funkci `spocti_prumer()` s jedn√≠m argumentem `cisla`, kter√Ω je seznamem ƒç√≠sel.\n",
      "2. V r√°mci funkce ovƒõ≈ôujeme, zda seznam ƒç√≠sel nen√≠ pr√°zdn√Ω. Pokud je pr√°zdn√Ω, vrac√≠ v√Ωjimku `ValueError`.\n",
      "3. Pokud seznam ƒç√≠sel nen√≠ pr√°zdn√Ω, spoƒç√≠t√°v√°me pr≈Ømƒõr pomoc√≠ funkce `sum()` a `len()`.\n",
      "4. Funkci pou≈æ√≠v√°me s seznamem ƒç√≠sel a tiskneme v√Ωsledek.\n",
      "\n",
      "**P≈ô√≠klad pou≈æit√≠:**\n",
      "\n",
      "* Seznam ƒç√≠sel: `[1, 2, 3, 4, 5]`\n",
      "* V√Ωsledek: `3.0`<|eot_id|>\n",
      "=========================================\n",
      "\n",
      "\n",
      "\n",
      "Prompt: Kolik je 2+2*2?\n",
      "=========================================\n",
      "≈ò√°dkov√°n√≠ operac√≠ se prov√°d√≠ podle n√°sleduj√≠c√≠ch pravidel:\n",
      "\n",
      "1. Zvl√°≈°≈•\n",
      "2. Vnit≈ôn√≠ z√°vorky\n",
      "3. Exponentiace\n",
      "4. Mno≈æstv√≠\n",
      "5. Dƒõlen√≠\n",
      "6. P≈ôiƒç√≠t√°n√≠ a odƒç√≠t√°n√≠\n",
      "\n",
      "V na≈°em p≈ô√≠padƒõ tedy nejprve provedeme n√°soben√≠ (2*2), pot√© p≈ôiƒçt√≠me 2.\n",
      "\n",
      "2 + 2*2 = 2 + 4 = 6<|eot_id|>\n",
      "=========================================\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "\n",
      "\n",
      "\n",
      "Testing model: /mnt/personal/mlynatom/thesis_models/it-Llama-3.1-8B-Instruct-mix_11_cs_en_alpaca_dolly/merge_16bit\n",
      "=========================================\n",
      "==((====))==  Unsloth 2025.4.7: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.254 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a6d7db43c604e5da09676910de0ca73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Oznaƒçte entity ve vƒõtƒõ.\n",
      "\n",
      "Bill Gates je zakladatelem spoleƒçnosti Microsoft.\n",
      "=========================================\n",
      "Bill Gates - osoba\n",
      "spoleƒçnost Microsoft - organizace\n",
      "zakladatelem - funkce\n",
      "spoleƒçnost - organizace\n",
      "Microsoft - n√°zev organizace\n",
      "zakladatelem - funkce\n",
      "spoleƒçnost - organizace<|eot_id|>\n",
      "=========================================\n",
      "\n",
      "\n",
      "\n",
      "Prompt: Jak√© kapitoly by mƒõla m√≠t diplomov√° pr√°ce na t√©ma adaptace LLM?\n",
      "=========================================\n",
      "1. √övod - vysvƒõtlen√≠ t√©matu, c√≠le diplomov√© pr√°ce a v√Ωznam adaptace LLM\n",
      "2. Teoretick√° ƒç√°st - popis adaptace LLM, v√Ωhody a nev√Ωhody, r≈Øzn√© metody adaptace a jejich porovn√°n√≠\n",
      "3. Empirick√° ƒç√°st - popis experiment≈Ø, kter√© byly provedeny k ovƒõ≈ôen√≠ √∫ƒçinnosti r≈Øzn√Ωch metod adaptace LLM\n",
      "4. V√Ωsledky - prezentace v√Ωsledk≈Ø experiment≈Ø, porovn√°n√≠ √∫ƒçinnosti r≈Øzn√Ωch metod adaptace LLM\n",
      "5. Diskuze - diskuse o v√Ωsledc√≠ch, porovn√°n√≠ s teoretick√Ωmi p≈ôedpoklady a doporuƒçen√≠ pro budouc√≠ v√Ωzkum\n",
      "6. Shrnut√≠ - shrnut√≠ hlavn√≠ch bod≈Ø diplomov√© pr√°ce a doporuƒçen√≠ pro budouc√≠ pou≈æit√≠ adaptace LLM\n",
      "7. Literatura - seznam literatur, kter√© byly pou≈æity p≈ôi psan√≠ diplomov√© pr√°ce.<|eot_id|>\n",
      "=========================================\n",
      "\n",
      "\n",
      "\n",
      "Prompt: Co je to 6. nemoc?\n",
      "=========================================\n",
      "Jako AI jsem programov√°n, abych poskytoval fakta a informace, kter√© jsou ovƒõ≈ôen√© a d≈Øvƒõryhodn√©. Nicm√©nƒõ, term√≠n \"6. nemoc\" nen√≠ standardn√≠m l√©ka≈ôsk√Ωm term√≠nem a nen√≠ uvedeno v ≈æ√°dn√© l√©ka≈ôsk√© literatu≈ôe. Proto nemohu poskytnout p≈ôesnou odpovƒõƒè na tuto ot√°zku.<|eot_id|>\n",
      "=========================================\n",
      "\n",
      "\n",
      "\n",
      "Prompt: Napi≈° mi pl√°n uƒçen√≠ na st√°tnice z ƒçe≈°tiny.\n",
      "=========================================\n",
      "1. Z√≠sk√°n√≠ z√°kladn√≠ch znalost√≠ z oblasti ƒçe≈°tiny, jako jsou gramatika, slovn√≠ z√°soba a literatura.\n",
      "\n",
      "2. Pravideln√© ƒçten√≠ a poslechov√°n√≠ ƒçesk√Ωch text≈Ø, jako jsou knihy, noviny a rozhlasov√© po≈ôady.\n",
      "\n",
      "3. Praktick√© cviƒçen√≠ v psan√≠ a mluven√≠ v ƒçe≈°tinƒõ, jako jsou rozhovory, prezentace a psan√≠ esej√≠.\n",
      "\n",
      "4. Studium historie a kultury ƒåesk√© republiky, jako jsou v√Ωznamn√© osobnosti, historick√© ud√°losti a kulturn√≠ tradice.\n",
      "\n",
      "5. Pravideln√© testov√°n√≠ a hodnocen√≠ znalost√≠ a dovednost√≠ v ƒçe≈°tinƒõ.\n",
      "\n",
      "6. Praktick√© vyu≈æit√≠ znalost√≠ a dovednost√≠ v ƒçe≈°tinƒõ v r≈Øzn√Ωch oblastech, jako jsou vzdƒõl√°v√°n√≠, pr√°ce a cestov√°n√≠.\n",
      "\n",
      "7. Pravideln√© setk√°v√°n√≠ s uƒçiteli a spolu≈æ√°ky, aby se mohli vymƒõ≈àovat zku≈°enostmi a z√≠skat dal≈°√≠ informace.\n",
      "\n",
      "8. Pravideln√© sledov√°n√≠ novinek a aktu√°ln√≠ch ud√°lost√≠ v ƒåesk√© republice, aby se z√≠skaly aktu√°ln√≠ informace a znalosti.\n",
      "\n",
      "9. Pravideln√© cviƒçen√≠ v psan√≠ a mluven√≠ v ƒçe≈°tinƒõ, aby se zlep≈°ily dovednosti a znalosti.\n",
      "\n",
      "10. Pravideln√© hodnocen√≠ a zlep≈°ov√°n√≠ znalost√≠ a dovednost√≠ v ƒçe≈°tinƒõ, aby se dos√°hlo c√≠le st√°tnice z ƒçe≈°tiny.<|eot_id|>\n",
      "=========================================\n",
      "\n",
      "\n",
      "\n",
      "Prompt: Napi≈° mi k√≥d v Pythonu, kter√Ω spoƒç√≠t√° pr≈Ømƒõr zadan√Ωch ƒç√≠sel.\n",
      "=========================================\n",
      "def spocti_prumer(cisla):\n",
      "    if not cisla:\n",
      "        return None\n",
      "    return sum(cisla) / len(cisla)\n",
      "\n",
      "cisla = [1, 2, 3, 4, 5]\n",
      "prumer = spocti_prumer(cisla)\n",
      "if prumer is not None:\n",
      "    print(\"Pr≈Ømƒõr zadan√Ωch ƒç√≠sel je:\", prumer)\n",
      "else:\n",
      "    print(\"Nen√≠ mo≈æn√© spoƒç√≠tat pr≈Ømƒõr pr√°zdn√©ho seznamu.\")<|eot_id|>\n",
      "=========================================\n",
      "\n",
      "\n",
      "\n",
      "Prompt: Kolik je 2+2*2?\n",
      "=========================================\n",
      "2+2*2 = 2+4 = 6.<|eot_id|>\n",
      "=========================================\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextStreamer\n",
    "\n",
    "for tested_model in tested_it_models:\n",
    "    print(f\"Testing model: {tested_model}\")\n",
    "    print(\"=========================================\")\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = tested_model, # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = 1024,\n",
    "        dtype = torch.bfloat16,\n",
    "        load_in_4bit = False,\n",
    "    )\n",
    "\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "    \n",
    "\n",
    "    for prompt in prompts:\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(\"=========================================\")\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize = True,\n",
    "            add_generation_prompt = True, # Must add for generation\n",
    "            return_tensors = \"pt\",\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "\n",
    "        _ = model.generate(input_ids = inputs, \n",
    "                           streamer = text_streamer, \n",
    "                           max_new_tokens = 1024,\n",
    "                           use_cache = True, \n",
    "                           temperature = 0.1, \n",
    "                           min_p = 0.1, \n",
    "                           #repetition_penalty = 1.2,\n",
    "                           )\n",
    "        \n",
    "        print(\"=========================================\")\n",
    "        print(\"\\n\\n\")\n",
    "    print(\"=========================================\")\n",
    "    print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
