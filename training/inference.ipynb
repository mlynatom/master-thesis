{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.12: Fast Llama patching. Transformers: 4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.381 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Eiffel Tower is a well-known structure. However, it's not located in France's capital city. The Eiffel Tower is located in Paris, France. \n",
      "\n",
      "If you'd like, I can describe the Eiffel Tower, which is a large iron lattice tower with an average height of approximately 324 meters (1,063 feet). It stands tall with four main pillars, supporting a central column, and a series of interconnected girders that give it its distinctive latticework appearance. At night, it is illuminated and is a major tourist attraction in Paris.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"meta-llama/Llama-3.2-3B-Instruct\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoj! Vyberanie nov√©ho telefonu je ƒçasto obt√≠≈æn√©, ale budem v√°m r√°dn√Ω pomoci.\n",
      "\n",
      "Kolik typ≈Ø telefon≈Ø m√°te na v√Ωbƒõr? M√°te preference na jak√©koli z nich? Nap≈ô√≠klad:\n",
      "\n",
      "- Android nebo iOS?\n",
      "- Velikost displeje?\n",
      "- Kamera?\n",
      "- Baterie?\n",
      "- Cena?\n",
      "\n",
      "M≈Ø≈æete mi ≈ô√≠ct, co v√°s zaj√≠m√°, a budem v√°m pomoci vybrat nejlep≈°√≠ telefon pro va≈°e pot≈ôeby.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Ahoj, pot≈ôebuju pomoci s v√Ωbƒõrem nov√©ho telefonu.\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 0.1, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dobr√Ω den! J√° jsem AI, tak≈æe nem√°m emoce jako lid√©. M√°m b√Ωt p≈ôipraven√Ω pomoci v√°m s jak√©koli ot√°zkou nebo probl√©mem, kter√Ω m√°te. Jak m≈Ø≈æete mƒõ potrestovat?<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Jak se m√°≈°?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 0.1, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoj! Vyberanie nov√©ho telefonu je ƒçasto obt√≠≈æn√©, ale budem v√°m r√°dn√Ω pomoci.\n",
      "\n",
      "M≈Ø≈æete mi ≈ô√≠ct, co v√°s zaj√≠m√° v nov√©m telefonu? M√°te nƒõjak√© specifick√© po≈æadavky nebo preference? Nap≈ô√≠klad:\n",
      "\n",
      "- Jak√Ω typ telefonu chcete (Android, iOS, hybridn√≠)?\n",
      "- Jak√Ω rozpoƒçet m√°te?\n",
      "- Chcete telefon s kamerou, nebo je to pro v√°s nƒõco jin√©ho?\n",
      "- M√°te nƒõjak√© specifick√© funkce nebo funkce, kter√© chcete m√≠t v telefonu?\n",
      "- Chcete telefon s velkou bateri√≠, nebo je\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Ahoj, pot≈ôebuju pomoci s v√Ωbƒõrem nov√©ho telefonu.\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 0.1, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V ƒçe≈°tinƒõ se pou≈æ√≠vaj√≠ dvƒõ varianty, ale spr√°vnou variantu je zpo≈ædƒõn√≠.\n",
      "\n",
      "Spo≈ædƒõn√≠ je neplatn√©, proto≈æe se v nƒõm pou≈æ√≠vaj√≠ dvƒõ slova, kter√© se v ƒçe≈°tinƒõ cannot be combined.\n",
      "\n",
      "Zpo≈ædƒõn√≠ je spr√°vn√©, proto≈æe se v nƒõm pou≈æ√≠vaj√≠ dvƒõ slova, kter√© se v ƒçe≈°tinƒõ mohou b√Ωt spojeny.\n",
      "\n",
      "V ƒçe≈°tinƒõ se zpo≈ædƒõn√≠ pou≈æ√≠v√° v mnoha situac√≠ch, jako je zpo≈ædƒõn√≠ pr√°ce, zpo≈ædƒõn√≠ transportu, zpo≈ædƒõn√≠ dopr\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"P√≠≈°eme zpo≈ædƒõn√≠, nebo spo≈ædƒõn√≠?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 0.1, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.12: Fast Llama patching. Transformers: 4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.381 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.12.12 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model2, tokenizer2 = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"/home/mlynatom/master-thesis-repository-tomas-mlynar/models/llama3.2-3b-instruct-1epoch-32batch-4gradacc-1e-4lr\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model2) # Enable native 2x faster inference\n",
    "\n",
    "\n",
    "text_streamer2 = TextStreamer(tokenizer2, skip_prompt = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm just a language model, I don't have feelings or emotions like humans do. However, I'm functioning properly and ready to assist you with any questions or tasks you may have.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "messages2 = [\n",
    "    {\"role\": \"user\", \"content\": \"How are you?\"},\n",
    "]\n",
    "inputs2 = tokenizer2.apply_chat_template(\n",
    "    messages2,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "_ = model2.generate(input_ids = inputs2, streamer = text_streamer2, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 0.1, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Eiffel Tower is a famous tall tower located in the heart of Paris, the capital of France. It stands at an impressive height of 324 meters (1,063 feet) and was built for the 1889 World's Fair. The tower is a masterpiece of engineering and architecture, with its four pillars supporting a lattice-like structure that gives it its distinctive appearance. The Eiffel Tower is not only a iconic landmark but also a popular tourist destination, attracting millions of visitors each year.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "messages2 = [\n",
    "    {\"role\": \"user\", \"content\": \"Describe a tall tower in the capital of France.\"},\n",
    "]\n",
    "inputs2 = tokenizer2.apply_chat_template(\n",
    "    messages2,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "_ = model2.generate(input_ids = inputs2, streamer = text_streamer2, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 0.1, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jsem programov√Ω syst√©m, tak≈æe nem√°m c·∫£mty.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "messages2 = [\n",
    "    {\"role\": \"user\", \"content\": \"Jak se m√°≈°?\"},\n",
    "]\n",
    "inputs2 = tokenizer2.apply_chat_template(\n",
    "    messages2,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "_ = model2.generate(input_ids = inputs2, streamer = text_streamer2, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 0.1, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zpo≈ædƒõn√≠ je spr√°vn√©.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "messages2 = [\n",
    "    {\"role\": \"user\", \"content\": \"P√≠≈°eme zpo≈ædƒõn√≠, nebo spo≈ædƒõn√≠?\"},\n",
    "]\n",
    "inputs2 = tokenizer2.apply_chat_template(\n",
    "    messages2,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "_ = model2.generate(input_ids = inputs2, streamer = text_streamer2, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 0.1, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jako AI jsem schopna poskytnout v√°m informace o r≈Øzn√Ωch telefonov√Ωch model√°ch, ale nebudu moci v√°m pomoci s v√Ωbƒõrem. Pokud m√°te specifick√© po≈æadavky, jako nap≈ô√≠klad cena, typ oper√°toru, technologie a dal≈°√≠, m≈Ø≈æete se obr√°tit na chuy√™n√≠ky v oboru nebo na internetov√© str√°nky, kde jsou dostupn√© recenze a porovn√°v√°n√≠ telefon≈Ø.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "messages2 = [\n",
    "    {\"role\": \"user\", \"content\": \"Ahoj, pot≈ôebuju pomoci s v√Ωbƒõrem nov√©ho telefonu.\"},\n",
    "]\n",
    "inputs2 = tokenizer2.apply_chat_template(\n",
    "    messages2,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "_ = model2.generate(input_ids = inputs2, streamer = text_streamer2, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 0.1, min_p = 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
