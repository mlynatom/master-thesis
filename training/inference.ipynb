{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./venv/unsloth_venv/lib/python3.11/site-packages (4.47.1)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in /mnt/appl/software/Python-bundle-PyPI/2023.10-GCCcore-13.2.0/lib/python3.11/site-packages (from transformers) (3.13.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./venv/unsloth_venv/lib/python3.11/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/appl/software/SciPy-bundle/2023.11-gfbf-2023b/lib/python3.11/site-packages (from transformers) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/appl/software/hatchling/1.18.0-GCCcore-13.2.0/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/appl/software/PyYAML/6.0.1-GCCcore-13.2.0/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/appl/software/Python-bundle-PyPI/2023.10-GCCcore-13.2.0/lib/python3.11/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /mnt/appl/software/Python-bundle-PyPI/2023.10-GCCcore-13.2.0/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./venv/unsloth_venv/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./venv/unsloth_venv/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venv/unsloth_venv/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /mnt/appl/software/Python-bundle-PyPI/2023.10-GCCcore-13.2.0/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/appl/software/typing-extensions/4.11.0-GCCcore-13.2.0/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/appl/software/Python-bundle-PyPI/2023.10-GCCcore-13.2.0/lib/python3.11/site-packages (from requests->transformers) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/appl/software/Python-bundle-PyPI/2023.10-GCCcore-13.2.0/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/appl/software/Python-bundle-PyPI/2023.10-GCCcore-13.2.0/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/appl/software/Python-bundle-PyPI/2023.10-GCCcore-13.2.0/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.47.1\n",
      "    Uninstalling transformers-4.47.1:\n",
      "      Successfully uninstalled transformers-4.47.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autoawq 0.2.8 requires transformers<=4.47.1,>=4.45.0, but you have transformers 4.51.3 which is incompatible.\n",
      "unsloth 2025.4.7 requires protobuf<4.0.0, but you have protobuf 4.25.3 which is incompatible.\n",
      "unsloth-zoo 2025.4.4 requires protobuf<4.0.0, but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed transformers-4.51.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextStreamer\n",
    "\n",
    "\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tested_model = \"/mnt/personal/mlynatom/thesis_models/cp_Llama-3.1-8B-cs_expand_5M_subword_resizing-full_fineweb-2_seed42_samples500000/merge_16bit\"\n",
    "#tested_model = \"meta-llama/Llama-3.1-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.1.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.254 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270ef72fd33b44c59c8d0a684a0719ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = tested_model, # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " udělat takový článek abych mohl vidět jaké věci děláte všimli jste nějaké zajímavější věci než můj článek určitě jste měli nějaké zajímavější věci než můj článek určitě jste měli nějaké zajímavější věci než můj článek určitě jste měli nějaké zajímavější věci než můj článek určitě jste měli nějaké zajímavější věci než můj článek určitě jste měli nějaké zajímavější věci než můj článek určitě jste měli nějaké zajímavější věci než můj článek určitě jste měli nějaké zajímavější věci než můj článek určitě jste měli nějaké zajímavější věci než můj článek určitě jste měli nějaké zajímavější věci než můj článek určitě jste měli nějaké zajímavější věci než můj článek určitě jste měli nějaké zajímavější věci než\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Já jsem se rozhodl\"\n",
    "inputs = tokenizer(\n",
    "    input_text,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=max_seq_length,\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Set up the text streamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "# Generate the output\n",
    "_ = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=128,\n",
    "    use_cache=True,\n",
    "    temperature=0.1,\n",
    "    min_p=0.1,\n",
    "    repetition_penalty=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", že si na blogu založím novou rubriku. A to je \"Zprávy ze světa\". V ní bych rád přinášel informace o tom co se děje v různých částech světa a hlavně ty které nejsou tak často hlášeny jako třeba teroristické útoky nebo válka.\n",
      "V tomto díle Zpráv ze světa vám představuji 5 zajímavých informací:\n",
      "1) Největší dálnice světa\n",
      "Dnes už není tajemstvím, že největší dálnici světa má Indie. Je dlouhá přes 9 tisíc kilometrů a vede od severního\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Já jsem se rozhodl\"\n",
    "inputs = tokenizer(\n",
    "    input_text,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=max_seq_length,\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Set up the text streamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "# Generate the output\n",
    "_ = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=128,\n",
    "    use_cache=True,\n",
    "    temperature=0.1,\n",
    "    min_p=0.1,\n",
    "    repetition_penalty=1.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " udělat něco jiného než ostatní lidé dělají teď tady kolem nás všichni řeší problémy světa ale nikdo nedokáže pomoci tomu že svět jde dolu protože každý chce mít svoji věc lepší větší rychlejší atd prostě nikdo nemá čas pomýšlet třeba jenom trochu jinak já chtěl ukázat všem hlavně mladým lidem kdo myslí stejně jako mně jaký máme velký výběr práce vlastně žádný takový člověk nikdy nebude stát úplně sám vždy můžeme najít někoho dalšího přesně toho člověka který hledáme ať už tím myslíme lidi kteří mají nějaké zkušenosti nebo naopak právě děti kterým ještě není ani šestnáct let opravdu máte možnost zvolit práci která vám vyhovuje\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Já jsem se rozhodl\"\n",
    "inputs = tokenizer(\n",
    "    input_text,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=max_seq_length,\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Set up the text streamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "# Generate the output\n",
    "_ = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=128,\n",
    "    use_cache=True,\n",
    "    temperature=0.1,\n",
    "    min_p=0.1,\n",
    "    repetition_penalty=1.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAhoj, potřebuju pomoci s výběrem nového telefonu.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      3\u001b[0m ]\n\u001b[0;32m----> 4\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Must add for generation\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(input_ids \u001b[38;5;241m=\u001b[39m inputs, streamer \u001b[38;5;241m=\u001b[39m text_streamer, max_new_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m     14\u001b[0m                    use_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, temperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m, min_p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m, repetition_penalty \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.2\u001b[39m,)\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/unsloth_venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1629\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1627\u001b[0m     tokenizer_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 1629\u001b[0m chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_assistant_tokens_mask \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*generation\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms*-?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m\"\u001b[39m, chat_template):\n\u001b[1;32m   1632\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[1;32m   1633\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_assistant_tokens_mask==True but chat template does not contain `\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;132;01m% g\u001b[39;00m\u001b[38;5;124meneration \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m}` keyword.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1634\u001b[0m     )\n",
      "File \u001b[0;32m~/master-thesis-repository-tomas-mlynar/venv/unsloth_venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1822\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.get_chat_template\u001b[0;34m(self, chat_template, tools)\u001b[0m\n\u001b[1;32m   1820\u001b[0m         chat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_template\n\u001b[1;32m   1821\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1823\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use chat template functions because tokenizer.chat_template is not set and no template \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1824\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument was passed! For information about writing templates and setting the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1825\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer.chat_template attribute, please see the documentation at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1826\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/chat_templating\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1827\u001b[0m         )\n\u001b[1;32m   1829\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chat_template\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Ahoj, potřebuju pomoci s výběrem nového telefonu.\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 0.7, min_p = 0.1, repetition_penalty = 1.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dnes je pondělí. Na dnešním cvičení se naučíme, jak pracovat s daty a číselnými operacemi v jazyce Python.\n",
      "\n",
      "Vytvořte si nový soubor ve svém editoru (nebo na webu https://repl.it/languages/python3) a přejmenujte ho jako \"cviceni1.py\". Vložte do něj následující kód:\n",
      "\n",
      "```python\n",
      "print(\"Hello World!\")\n",
      "```\n",
      "\n",
      "Spusťte jej pomocí příkazu `run` nebo stiskněte tlačítko \"Run\" v horním menu replitu.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Kde domov můj?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 0.1, min_p = 0.1, repetition_penalty = 1.2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dobrý den! Já jsem AI, takže nemám emoce jako lidé. Mám být připravený pomoci vám s jakékoli otázkou nebo problémem, který máte. Jak můžete mě potrestovat?<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Jak se máš?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 0.1, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoj! Vyberanie nového telefonu je často obtížné, ale budem vám rádný pomoci.\n",
      "\n",
      "Můžete mi říct, co vás zajímá v novém telefonu? Máte nějaké specifické požadavky nebo preference? Například:\n",
      "\n",
      "- Jaký typ telefonu chcete (Android, iOS, hybridní)?\n",
      "- Jaký rozpočet máte?\n",
      "- Chcete telefon s kamerou, nebo je to pro vás něco jiného?\n",
      "- Máte nějaké specifické funkce nebo funkce, které chcete mít v telefonu?\n",
      "- Chcete telefon s velkou baterií, nebo je\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Ahoj, potřebuju pomoci s výběrem nového telefonu.\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 0.1, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V češtině se používají dvě varianty, ale správnou variantu je zpoždění.\n",
      "\n",
      "Spoždění je neplatné, protože se v něm používají dvě slova, které se v češtině cannot be combined.\n",
      "\n",
      "Zpoždění je správné, protože se v něm používají dvě slova, které se v češtině mohou být spojeny.\n",
      "\n",
      "V češtině se zpoždění používá v mnoha situacích, jako je zpoždění práce, zpoždění transportu, zpoždění dopr\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Píšeme zpoždění, nebo spoždění?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 0.1, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tested_it_models = [\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-cs_expand_5M_subword_resizing-full_cs_fineweb2-cs_finewebedu-en_31_500k_seed42_samples500000-full_cs_instruction_tuning_collection/merge_16bit\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-cs_expand_5M_subword_resizing-full_cs_fineweb2-cs_finewebedu-en_31_500k_seed42_samples500000-full_mix_11_cs_en/merge_16bit\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-cs_expand_5M_subword_resizing-full_fineweb-2_seed42_samples500000-full_cs_instruction_tuning_collection/merge_16bit\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-cs_expand_5M_subword_resizing-full_fineweb-2_seed42_samples500000-full_mix_11_cs_en/merge_16bit\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_cs_fineweb2_seed42_neptune_bs128_samples500000-full_cs_instruction_tuning_collection/merge_16bit\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_cs_fineweb2_seed42_neptune_bs128_samples500000-full_mix_11_cs_en/merge_16bit\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_fineweb2-cs_finewebedu-en_31_500k_seed42_samples500000-full_cs_instruction_tuning_collection/merge_16bit\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_fineweb2-cs_finewebedu-en_31_500k_seed42_samples500000-full_mix_11_cs_en/merge_16bit\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-Llama-3.1-8B-full_cs_instruction_tuning_collection/merge_16bit\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-Llama-3.1-8B-full_mix_11_cs_en/merge_16bit\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-Llama-3.1-8B-Instruct-full_cs_instruction_tuning_collection/merge_16bit\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-Llama-3.1-8B-Instruct-full_mix_11_cs_en/merge_16bit\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_fineweb2-cs_finewebedu-en_31_500k_seed42_samples500000-mix_11_cs_en_alpaca_dolly/merge_16bit\",\n",
    "    \"/mnt/personal/mlynatom/thesis_models/it-Llama-3.1-8B-Instruct-mix_11_cs_en_alpaca_dolly/merge_16bit\",\n",
    "\n",
    "    #\"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_fineweb2-cs_finewebedu-en_31_500k_seed42_samples500000-mix_11_cs_en_alpaca_dolly/final\",\n",
    "    #\"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_fineweb2-cs_finewebedu-en_31_500k_seed42_samples500000-full_mix_11_cs_en/final\",\n",
    "    #\"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_cs_fineweb2_seed42_neptune_bs128_samples500000-full_cs_instruction_tuning_collection/final\",\n",
    "    #\"/mnt/personal/mlynatom/thesis_models/it-Llama-3.1-8B-full_mix_11_cs_en/merge_16bit\",\n",
    "    #\"/mnt/personal/mlynatom/thesis_models/it-Llama-3.1-8B-full_mix_11_cs_en/merge_16bitv2\",\n",
    "    #\"/mnt/personal/mlynatom/thesis_models/it-Llama-3.1-8B-full_mix_11_cs_en/final\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_fineweb2-cs_finewebedu-en_31_500k_seed42_samples500000-full_mix_11_cs_en/final\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_fineweb2-cs_finewebedu-en_31_500k_seed42_samples500000-full_mix_11_cs_en/merge_16bit\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_fineweb2-cs_finewebedu-en_31_500k_seed42_samples500000-full_mix_11_cs_en/merge_16bit_v2\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_fineweb2-cs_finewebedu-en_31_500k_seed42_samples500000_v2-mix_11_cs_en/final\",\n",
    "    #\"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_fineweb2-cs_finewebedu-en_31_500k_seed42_samples500000_v2-mix_11_cs_en/merge_16bit_v2\"\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_cs_fineweb2_seed42_neptune_bs128_samples500000-full_cs_instruction_tuning_collection/merge_16bit\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_cs_fineweb2_seed42_neptune_bs128_samples500000-full_cs_instruction_tuning_collection/final\"\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_cs_fineweb2_seed42_neptune_bs128_samples500000-full_cs_instruction_tuning_collection/merge_16bit\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_cs_fineweb2_seed42_neptune_bs128_samples500000-full_cs_instruction_tuning_collection/merge_16bit_v2\",\n",
    "    # \"/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_cs_fineweb2_seed42_neptune_bs128_samples500000-full_cs_instruction_tuning_collection/final\"\n",
    "]\n",
    "\n",
    "prompts = [\n",
    "    \"Označte entity ve větě.\\n\\nBill Gates je zakladatelem společnosti Microsoft.\", #directly from Alpaca dataset\n",
    "    \"Jaké kapitoly by měla mít diplomová práce na téma adaptace LLM?\", #creative\n",
    "    \"Co je to 6. nemoc?\", #info seeking\n",
    "    \"Napiš mi plán učení na státnice z češtiny.\", #reasoning\n",
    "    \"Napiš mi kód v Pythonu, který spočítá průměr zadaných čísel.\", #code generation\n",
    "    \"Kolik je 2+2*2?\", #math\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "Testing model: meta-llama/Llama-3.1-8B-Instruct\n",
      "=========================================\n",
      "==((====))==  Unsloth 2025.4.7: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.254 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e0245473c8402baffb373c8a978c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Označte entity ve větě.\n",
      "\n",
      "Bill Gates je zakladatelem společnosti Microsoft.\n",
      "=========================================\n",
      "V této větě jsou dvě entity:\n",
      "\n",
      "1. Bill Gates - osoba (lidé)\n",
      "2. Microsoft - organizace (společnost)<|eot_id|>\n",
      "=========================================\n",
      "\n",
      "\n",
      "\n",
      "Prompt: Jaké kapitoly by měla mít diplomová práce na téma adaptace LLM?\n",
      "=========================================\n",
      "Diplomová práce na téma adaptace Large Language Modelů (LLM) by měla mít následující kapitoly:\n",
      "\n",
      "**1. Úvod**\n",
      "\n",
      "* Vyhledání tématu a jeho významnosti\n",
      "* Cíl diplomové práce\n",
      "* Stručné shrnutí obsahu práce\n",
      "\n",
      "**2. Teoretické pozadí**\n",
      "\n",
      "* Základní informace o Large Language Modelů (LLM)\n",
      "* Historie vývoje LLM\n",
      "* Principy fungování LLM\n",
      "* Typy LLM (transformer, LSTM, GRU, atd.)\n",
      "\n",
      "**3. Adaptace LLM**\n",
      "\n",
      "* Definice adaptace LLM\n",
      "* Typy adaptace LLM (fine-tuning, transfer learning, atd.)\n",
      "* Postupy adaptace LLM (data preparation, model tuning, atd.)\n",
      "\n",
      "**4. Metodologie**\n",
      "\n",
      "* Popis použitých metod a technik\n",
      "* Popis datové sady a její přípravě\n",
      "* Popis modelů LLM a jejich parametrů\n",
      "* Popis procesu adaptace LLM\n",
      "\n",
      "**5. Výsledky**\n",
      "\n",
      "* Shrnutí výsledků adaptace LLM\n",
      "* Porovnání výsledků s původními modely LLM\n",
      "* Diskuze o výsledcích a jejich významnosti\n",
      "\n",
      "**6. Diskuze a závěry**\n",
      "\n",
      "* Diskuze o výsledcích a jejich významnosti\n",
      "* Shrnutí hlavních závěrů diplomové práce\n",
      "* Doporučení pro budoucí výzkum\n",
      "\n",
      "**7. Ochrana osobních údajů a etika**\n",
      "\n",
      "* Shrnutí postupů ochrany osobních údajů\n",
      "* Shrnutí etických principů diplomové práce\n",
      "\n",
      "**8. Seznam použité literatury**\n",
      "\n",
      "* Seznam citovaných zdrojů\n",
      "\n",
      "**9. Seznam použitých zkratek**\n",
      "\n",
      "* Seznam zkratek použité v diplomové práci\n",
      "\n",
      "**10. Příloha**\n",
      "\n",
      "* Příloha obsahuje další informace, které nejsou relevantní pro hlavní text diplomové práce, ale které mohou být užitečné pro čtenáře.<|eot_id|>\n",
      "=========================================\n",
      "\n",
      "\n",
      "\n",
      "Prompt: Co je to 6. nemoc?\n",
      "=========================================\n",
      "6. nemoc je termín, který se používá v souvislosti s epidemií chřipky. V roce 1957 se vyskytla pandemie chřipky, která byla označována jako 2. světová chřipková pandemie. Další pandemie chřipky se vyskytly v letech 1968 (3. světová chřipková pandemie) a 2009 (pandemie A/H1N1).\n",
      "\n",
      "6. nemoc je někdy používána jako synonymum pro pandemii chřipky, která se vyskytla v roce 2019 a byla způsobena novým typem chřipkové viru HCoV-19 (dnes známým jako SARS-CoV-2). Tato pandemie byla označována jako COVID-19.\n",
      "\n",
      "Je důležité poznamenat, že termín \"6. nemoc\" není oficiálně uzákoněn a není používán mezinárodními zdravotnickými organizacemi.<|eot_id|>\n",
      "=========================================\n",
      "\n",
      "\n",
      "\n",
      "Prompt: Napiš mi plán učení na státnice z češtiny.\n",
      "=========================================\n",
      "Plán učení na státnice z češtiny je komplexní a vyžaduje systematickou přípravu. Zde je jeden možný plán učení:\n",
      "\n",
      "**Fáze 1: Základní znalosti (1-3 měsíce)**\n",
      "\n",
      "1. **Základní gramatika**:\n",
      " * Rozdíly mezi slovesnými a příslovkovými tvarovaními\n",
      " * Používání kategorií času (přítomný, minulý, budoucí)\n",
      " * Používání kategorií způsobu (přítomný, minulý, budoucí)\n",
      " * Používání kategorií čísla (singulár, plurál)\n",
      "2. **Slovní zásoba**:\n",
      " * Základní slovní zásoba (příslovce, přídavná jména, podstatná jména)\n",
      " * Používání slovní zásoby v různých kontextech\n",
      "3. **Čtení a psaní**:\n",
      " * Základní čtení a psaní (příslovce, přídavná jména, podstatná jména)\n",
      " * Používání čtení a psaní v různých kontextech\n",
      "\n",
      "**Fáze 2: Rozšířené znalosti (3-6 měsíců)**\n",
      "\n",
      "1. **Rozšířená gramatika**:\n",
      " * Používání kategorií času a způsobu v komplexních větách\n",
      " * Používání kategorií čísla v komplexních větách\n",
      " * Používání kategorií osob (já, ty, on, my, vy, oni)\n",
      "2. **Slovní zásoba**:\n",
      " * Rozšířená slovní zásoba (slovesa, příslovce, přídavná jména, podstatná jména)\n",
      " * Používání slovní zásoby v různých kontextech\n",
      "3. **Čtení a psaní**:\n",
      " * Rozšířené čtení a psaní (slovesa, příslovce, přídavná jména, podstatná jména)\n",
      " * Používání čtení a psaní v různých kontextech\n",
      "\n",
      "**Fáze 3: Specifické znalosti (6-12 měsíců)**\n",
      "\n",
      "1. **Specifická gramatika**:\n",
      " * Používání kategorií času a způsobu v komplexních větách s příslovci\n",
      " * Používání kategorií čísla v komplexních větách s příslovci\n",
      " * Používání kategorií osob v komplexních větách s příslovci\n",
      "2. **Slovní zásoba**:\n",
      " * Specifická slovní zásoba (slovesa, příslovce, přídavná jména, podstatná jména)\n",
      " * Používání slovní zásoby v různých kontextech\n",
      "3. **Čtení a psaní**:\n",
      " * Specifické čtení a psaní (slovesa, příslovce, přídavná jména, podstatná jména)\n",
      " * Používání čtení a psaní v různých kontextech\n",
      "\n",
      "**Fáze 4: Úplné znalosti (1-2 roky)**\n",
      "\n",
      "1. **Úplná gramatika**:\n",
      " * Používání kategorií času a způsobu v komplexních větách s příslovci a přídavnými jmény\n",
      " * Používání kategorií čísla v komplexních větách s příslovci a přídavnými jmény\n",
      " * Používání kategorií osob v komplexních větách s příslovci a přídavnými jmény\n",
      "2. **Slovní zásoba**:\n",
      " * Úplná slovní zásoba (slovesa, příslovce, přídavná jména, podstatná jména)\n",
      " * Používání slovní zásoby v různých kontextech\n",
      "3. **Čtení a psaní**:\n",
      " * Úplné čtení a psaní (slovesa, příslovce, přídavná jména, podstatná jména)\n",
      " * Používání čtení a psaní v různých kontextech\n",
      "\n",
      "Tento plán učení je pouze orientační a může se lišit v závislosti na individuálních potřebách a schopnostech.<|eot_id|>\n",
      "=========================================\n",
      "\n",
      "\n",
      "\n",
      "Prompt: Napiš mi kód v Pythonu, který spočítá průměr zadaných čísel.\n",
      "=========================================\n",
      "**Spočítání průměru zadaných čísel v Pythonu**\n",
      "\n",
      "Následující kód spočítá průměr zadaných čísel. Kód používá seznam čísel a funkce `sum()` a `len()` pro výpočet průměru.\n",
      "\n",
      "```python\n",
      "def spocti_prumer(cisla):\n",
      "    \"\"\"\n",
      "    Spočítá průměr zadaných čísel.\n",
      "\n",
      "    Args:\n",
      "        cisla (list): Seznam čísel.\n",
      "\n",
      "    Returns:\n",
      "        float: Průměr zadaných čísel.\n",
      "    \"\"\"\n",
      "    if not cisla:\n",
      "        raise ValueError(\"Seznam čísel je prázdný.\")\n",
      "\n",
      "    return sum(cisla) / len(cisla)\n",
      "\n",
      "# Použití funkce\n",
      "cisla = [1, 2, 3, 4, 5]\n",
      "prumer = spocti_prumer(cisla)\n",
      "print(f\"Průměr zadaných čísel je: {prumer}\")\n",
      "```\n",
      "\n",
      "**Jak kód funguje:**\n",
      "\n",
      "1. Definujeme funkci `spocti_prumer()` s jedním argumentem `cisla`, který je seznamem čísel.\n",
      "2. V rámci funkce ověřujeme, zda seznam čísel není prázdný. Pokud je prázdný, vrací výjimku `ValueError`.\n",
      "3. Pokud seznam čísel není prázdný, spočítáváme průměr pomocí funkce `sum()` a `len()`.\n",
      "4. Funkci používáme s seznamem čísel a tiskneme výsledek.\n",
      "\n",
      "**Příklad použití:**\n",
      "\n",
      "* Seznam čísel: `[1, 2, 3, 4, 5]`\n",
      "* Výsledek: `3.0`<|eot_id|>\n",
      "=========================================\n",
      "\n",
      "\n",
      "\n",
      "Prompt: Kolik je 2+2*2?\n",
      "=========================================\n",
      "Řádkování operací se provádí podle následujících pravidel:\n",
      "\n",
      "1. Zvlášť\n",
      "2. Vnitřní závorky\n",
      "3. Exponentiace\n",
      "4. Množství\n",
      "5. Dělení\n",
      "6. Přičítání a odčítání\n",
      "\n",
      "V našem případě tedy nejprve provedeme násobení (2*2), poté přičtíme 2.\n",
      "\n",
      "2 + 2*2 = 2 + 4 = 6<|eot_id|>\n",
      "=========================================\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "\n",
      "\n",
      "\n",
      "Testing model: /mnt/personal/mlynatom/thesis_models/it-Llama-3.1-8B-Instruct-mix_11_cs_en_alpaca_dolly/merge_16bit\n",
      "=========================================\n",
      "==((====))==  Unsloth 2025.4.7: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.254 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a6d7db43c604e5da09676910de0ca73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Označte entity ve větě.\n",
      "\n",
      "Bill Gates je zakladatelem společnosti Microsoft.\n",
      "=========================================\n",
      "Bill Gates - osoba\n",
      "společnost Microsoft - organizace\n",
      "zakladatelem - funkce\n",
      "společnost - organizace\n",
      "Microsoft - název organizace\n",
      "zakladatelem - funkce\n",
      "společnost - organizace<|eot_id|>\n",
      "=========================================\n",
      "\n",
      "\n",
      "\n",
      "Prompt: Jaké kapitoly by měla mít diplomová práce na téma adaptace LLM?\n",
      "=========================================\n",
      "1. Úvod - vysvětlení tématu, cíle diplomové práce a význam adaptace LLM\n",
      "2. Teoretická část - popis adaptace LLM, výhody a nevýhody, různé metody adaptace a jejich porovnání\n",
      "3. Empirická část - popis experimentů, které byly provedeny k ověření účinnosti různých metod adaptace LLM\n",
      "4. Výsledky - prezentace výsledků experimentů, porovnání účinnosti různých metod adaptace LLM\n",
      "5. Diskuze - diskuse o výsledcích, porovnání s teoretickými předpoklady a doporučení pro budoucí výzkum\n",
      "6. Shrnutí - shrnutí hlavních bodů diplomové práce a doporučení pro budoucí použití adaptace LLM\n",
      "7. Literatura - seznam literatur, které byly použity při psaní diplomové práce.<|eot_id|>\n",
      "=========================================\n",
      "\n",
      "\n",
      "\n",
      "Prompt: Co je to 6. nemoc?\n",
      "=========================================\n",
      "Jako AI jsem programován, abych poskytoval fakta a informace, které jsou ověřené a důvěryhodné. Nicméně, termín \"6. nemoc\" není standardním lékařským termínem a není uvedeno v žádné lékařské literatuře. Proto nemohu poskytnout přesnou odpověď na tuto otázku.<|eot_id|>\n",
      "=========================================\n",
      "\n",
      "\n",
      "\n",
      "Prompt: Napiš mi plán učení na státnice z češtiny.\n",
      "=========================================\n",
      "1. Získání základních znalostí z oblasti češtiny, jako jsou gramatika, slovní zásoba a literatura.\n",
      "\n",
      "2. Pravidelné čtení a poslechování českých textů, jako jsou knihy, noviny a rozhlasové pořady.\n",
      "\n",
      "3. Praktické cvičení v psaní a mluvení v češtině, jako jsou rozhovory, prezentace a psaní esejí.\n",
      "\n",
      "4. Studium historie a kultury České republiky, jako jsou významné osobnosti, historické události a kulturní tradice.\n",
      "\n",
      "5. Pravidelné testování a hodnocení znalostí a dovedností v češtině.\n",
      "\n",
      "6. Praktické využití znalostí a dovedností v češtině v různých oblastech, jako jsou vzdělávání, práce a cestování.\n",
      "\n",
      "7. Pravidelné setkávání s učiteli a spolužáky, aby se mohli vyměňovat zkušenostmi a získat další informace.\n",
      "\n",
      "8. Pravidelné sledování novinek a aktuálních událostí v České republice, aby se získaly aktuální informace a znalosti.\n",
      "\n",
      "9. Pravidelné cvičení v psaní a mluvení v češtině, aby se zlepšily dovednosti a znalosti.\n",
      "\n",
      "10. Pravidelné hodnocení a zlepšování znalostí a dovedností v češtině, aby se dosáhlo cíle státnice z češtiny.<|eot_id|>\n",
      "=========================================\n",
      "\n",
      "\n",
      "\n",
      "Prompt: Napiš mi kód v Pythonu, který spočítá průměr zadaných čísel.\n",
      "=========================================\n",
      "def spocti_prumer(cisla):\n",
      "    if not cisla:\n",
      "        return None\n",
      "    return sum(cisla) / len(cisla)\n",
      "\n",
      "cisla = [1, 2, 3, 4, 5]\n",
      "prumer = spocti_prumer(cisla)\n",
      "if prumer is not None:\n",
      "    print(\"Průměr zadaných čísel je:\", prumer)\n",
      "else:\n",
      "    print(\"Není možné spočítat průměr prázdného seznamu.\")<|eot_id|>\n",
      "=========================================\n",
      "\n",
      "\n",
      "\n",
      "Prompt: Kolik je 2+2*2?\n",
      "=========================================\n",
      "2+2*2 = 2+4 = 6.<|eot_id|>\n",
      "=========================================\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import TextStreamer\n",
    "\n",
    "for tested_model in tested_it_models:\n",
    "    print(f\"Testing model: {tested_model}\")\n",
    "    print(\"=========================================\")\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = tested_model, # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = 1024,\n",
    "        dtype = torch.bfloat16,\n",
    "        load_in_4bit = False,\n",
    "    )\n",
    "\n",
    "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "    \n",
    "\n",
    "    for prompt in prompts:\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(\"=========================================\")\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize = True,\n",
    "            add_generation_prompt = True, # Must add for generation\n",
    "            return_tensors = \"pt\",\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "\n",
    "        _ = model.generate(input_ids = inputs, \n",
    "                           streamer = text_streamer, \n",
    "                           max_new_tokens = 1024,\n",
    "                           use_cache = True, \n",
    "                           temperature = 0.1, \n",
    "                           min_p = 0.1, \n",
    "                           #repetition_penalty = 1.2,\n",
    "                           )\n",
    "        \n",
    "        print(\"=========================================\")\n",
    "        print(\"\\n\\n\")\n",
    "    print(\"=========================================\")\n",
    "    print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
