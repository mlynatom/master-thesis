{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fec0f52",
   "metadata": {},
   "source": [
    "# Merge LoRA Adapter Weights and aply AWQ quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "436aefcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/mnt/personal/mlynatom/thesis_models/it-Llama-3.1-8B-Instruct-mix_11_cs_en_alpaca_dolly\"\n",
    "#base_model_path = \"/mnt/personal/mlynatom/thesis_models/cp_Llama-3.1-8B-full_cs_fineweb2_seed42_neptune_bs128_samples500000/merge_16bit\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37939449",
   "metadata": {},
   "source": [
    "## 16 bit merge v3\n",
    "\n",
    "https://colab.research.google.com/drive/12c_sx8pIwiStqKr_7CF5BVwyyJpXmMTf?usp=sharing#scrollTo=c9yLWqKRKKyd\n",
    "\n",
    "https://kaitchup.substack.com/p/lora-adapters-when-a-naive-merge\n",
    "\n",
    "https://huggingface.co/docs/peft/developer_guides/lora#a-more-convenient-way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e89cbea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f29a6808df4d5c85c5e8352eef2c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import replace_lora_weights_loftq, get_peft_model, LoraConfig\n",
    "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_path, device_map={\"\": 0}, quantization_config=bnb_config, torch_dtype=torch.bfloat16)\n",
    "# note: don't pass init_lora_weights=\"loftq\" or loftq_config!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a6e4e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Benjamin Marie's code #### https://kaitchup.substack.com/p/lora-adapters-when-a-naive-merge ###\n",
    "import torch\n",
    "import peft\n",
    "import json\n",
    "import shutil\n",
    "from peft.utils import _get_submodules\n",
    "import os\n",
    "import bitsandbytes as bnb\n",
    "from bitsandbytes.functional import dequantize_4bit\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "\n",
    "def dequantize_model(model, to='./dequantized_model', dtype=torch.bfloat16, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    'model': the peftmodel you loaded with qlora.\n",
    "    'tokenizer': the model's corresponding hf's tokenizer.\n",
    "    'to': directory to save the dequantized model\n",
    "    'dtype': dtype that the model was trained using\n",
    "    'device': device to load the model to\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    os.makedirs(to, exist_ok=True)\n",
    "\n",
    "    cls = bnb.nn.Linear4bit\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, cls):\n",
    "                print(f\"Dequantizing `{name}`...\")\n",
    "                quant_state = copy.deepcopy(module.weight.quant_state)\n",
    "                quant_state.dtype = dtype\n",
    "\n",
    "                weights = dequantize_4bit(module.weight.data, quant_state=quant_state, quant_type=\"nf4\").to(dtype)\n",
    "\n",
    "                new_module = torch.nn.Linear(module.in_features, module.out_features, bias=None, dtype=dtype)\n",
    "                new_module.weight = torch.nn.Parameter(weights)\n",
    "                new_module.to(device=device, dtype=dtype)\n",
    "\n",
    "                parent, target, target_name = _get_submodules(model, name)\n",
    "                setattr(parent, target_name, new_module)\n",
    "\n",
    "        # a hack, setting this to avoid hf's saving error because hf\n",
    "        # itself does not support saving a model that is registered to be loaded in 4bit.\n",
    "        model.is_loaded_in_4bit = False\n",
    "\n",
    "        print(\"Saving dequantized model...\")\n",
    "        model.save_pretrained(to)\n",
    "        #tokenizer.save_pretrained(to)\n",
    "        config_data = json.loads(open(os.path.join(to, 'config.json'), 'r').read())\n",
    "        config_data.pop(\"quantization_config\", None)\n",
    "        config_data.pop(\"pretraining_tp\", None)\n",
    "        with open(os.path.join(to, 'config.json'), 'w') as config:\n",
    "            config.write(json.dumps(config_data, indent=2))\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98ede9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dequantizing `model.layers.0.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.0.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.0.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.0.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.0.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.0.mlp.up_proj`...\n",
      "Dequantizing `model.layers.0.mlp.down_proj`...\n",
      "Dequantizing `model.layers.1.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.1.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.1.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.1.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.1.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.1.mlp.up_proj`...\n",
      "Dequantizing `model.layers.1.mlp.down_proj`...\n",
      "Dequantizing `model.layers.2.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.2.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.2.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.2.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.2.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.2.mlp.up_proj`...\n",
      "Dequantizing `model.layers.2.mlp.down_proj`...\n",
      "Dequantizing `model.layers.3.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.3.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.3.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.3.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.3.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.3.mlp.up_proj`...\n",
      "Dequantizing `model.layers.3.mlp.down_proj`...\n",
      "Dequantizing `model.layers.4.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.4.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.4.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.4.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.4.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.4.mlp.up_proj`...\n",
      "Dequantizing `model.layers.4.mlp.down_proj`...\n",
      "Dequantizing `model.layers.5.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.5.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.5.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.5.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.5.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.5.mlp.up_proj`...\n",
      "Dequantizing `model.layers.5.mlp.down_proj`...\n",
      "Dequantizing `model.layers.6.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.6.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.6.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.6.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.6.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.6.mlp.up_proj`...\n",
      "Dequantizing `model.layers.6.mlp.down_proj`...\n",
      "Dequantizing `model.layers.7.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.7.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.7.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.7.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.7.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.7.mlp.up_proj`...\n",
      "Dequantizing `model.layers.7.mlp.down_proj`...\n",
      "Dequantizing `model.layers.8.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.8.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.8.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.8.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.8.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.8.mlp.up_proj`...\n",
      "Dequantizing `model.layers.8.mlp.down_proj`...\n",
      "Dequantizing `model.layers.9.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.9.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.9.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.9.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.9.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.9.mlp.up_proj`...\n",
      "Dequantizing `model.layers.9.mlp.down_proj`...\n",
      "Dequantizing `model.layers.10.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.10.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.10.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.10.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.10.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.10.mlp.up_proj`...\n",
      "Dequantizing `model.layers.10.mlp.down_proj`...\n",
      "Dequantizing `model.layers.11.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.11.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.11.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.11.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.11.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.11.mlp.up_proj`...\n",
      "Dequantizing `model.layers.11.mlp.down_proj`...\n",
      "Dequantizing `model.layers.12.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.12.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.12.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.12.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.12.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.12.mlp.up_proj`...\n",
      "Dequantizing `model.layers.12.mlp.down_proj`...\n",
      "Dequantizing `model.layers.13.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.13.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.13.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.13.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.13.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.13.mlp.up_proj`...\n",
      "Dequantizing `model.layers.13.mlp.down_proj`...\n",
      "Dequantizing `model.layers.14.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.14.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.14.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.14.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.14.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.14.mlp.up_proj`...\n",
      "Dequantizing `model.layers.14.mlp.down_proj`...\n",
      "Dequantizing `model.layers.15.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.15.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.15.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.15.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.15.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.15.mlp.up_proj`...\n",
      "Dequantizing `model.layers.15.mlp.down_proj`...\n",
      "Dequantizing `model.layers.16.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.16.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.16.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.16.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.16.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.16.mlp.up_proj`...\n",
      "Dequantizing `model.layers.16.mlp.down_proj`...\n",
      "Dequantizing `model.layers.17.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.17.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.17.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.17.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.17.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.17.mlp.up_proj`...\n",
      "Dequantizing `model.layers.17.mlp.down_proj`...\n",
      "Dequantizing `model.layers.18.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.18.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.18.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.18.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.18.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.18.mlp.up_proj`...\n",
      "Dequantizing `model.layers.18.mlp.down_proj`...\n",
      "Dequantizing `model.layers.19.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.19.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.19.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.19.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.19.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.19.mlp.up_proj`...\n",
      "Dequantizing `model.layers.19.mlp.down_proj`...\n",
      "Dequantizing `model.layers.20.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.20.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.20.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.20.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.20.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.20.mlp.up_proj`...\n",
      "Dequantizing `model.layers.20.mlp.down_proj`...\n",
      "Dequantizing `model.layers.21.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.21.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.21.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.21.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.21.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.21.mlp.up_proj`...\n",
      "Dequantizing `model.layers.21.mlp.down_proj`...\n",
      "Dequantizing `model.layers.22.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.22.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.22.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.22.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.22.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.22.mlp.up_proj`...\n",
      "Dequantizing `model.layers.22.mlp.down_proj`...\n",
      "Dequantizing `model.layers.23.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.23.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.23.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.23.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.23.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.23.mlp.up_proj`...\n",
      "Dequantizing `model.layers.23.mlp.down_proj`...\n",
      "Dequantizing `model.layers.24.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.24.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.24.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.24.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.24.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.24.mlp.up_proj`...\n",
      "Dequantizing `model.layers.24.mlp.down_proj`...\n",
      "Dequantizing `model.layers.25.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.25.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.25.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.25.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.25.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.25.mlp.up_proj`...\n",
      "Dequantizing `model.layers.25.mlp.down_proj`...\n",
      "Dequantizing `model.layers.26.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.26.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.26.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.26.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.26.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.26.mlp.up_proj`...\n",
      "Dequantizing `model.layers.26.mlp.down_proj`...\n",
      "Dequantizing `model.layers.27.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.27.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.27.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.27.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.27.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.27.mlp.up_proj`...\n",
      "Dequantizing `model.layers.27.mlp.down_proj`...\n",
      "Dequantizing `model.layers.28.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.28.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.28.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.28.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.28.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.28.mlp.up_proj`...\n",
      "Dequantizing `model.layers.28.mlp.down_proj`...\n",
      "Dequantizing `model.layers.29.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.29.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.29.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.29.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.29.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.29.mlp.up_proj`...\n",
      "Dequantizing `model.layers.29.mlp.down_proj`...\n",
      "Dequantizing `model.layers.30.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.30.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.30.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.30.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.30.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.30.mlp.up_proj`...\n",
      "Dequantizing `model.layers.30.mlp.down_proj`...\n",
      "Dequantizing `model.layers.31.self_attn.q_proj`...\n",
      "Dequantizing `model.layers.31.self_attn.k_proj`...\n",
      "Dequantizing `model.layers.31.self_attn.v_proj`...\n",
      "Dequantizing `model.layers.31.self_attn.o_proj`...\n",
      "Dequantizing `model.layers.31.mlp.gate_proj`...\n",
      "Dequantizing `model.layers.31.mlp.up_proj`...\n",
      "Dequantizing `model.layers.31.mlp.down_proj`...\n",
      "Saving dequantized model...\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n",
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=256, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=256, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=256, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=256, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=256, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=256, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=14336, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=256, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import peft\n",
    "import json\n",
    "import shutil\n",
    "from peft.utils import _get_submodules\n",
    "import os\n",
    "import bitsandbytes as bnb\n",
    "from bitsandbytes.functional import dequantize_4bit\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "try:\n",
    "    #print(f\"Starting to load the model {model_name} into memory\")\n",
    "\n",
    "    base_model = dequantize_model(base_model, to=f\"{model_path}/merge_16bit_v3\",dtype=dtype)\n",
    "    print(base_model)\n",
    "    model = PeftModel.from_pretrained(base_model, f\"{model_path}/final\")\n",
    "    print(model)\n",
    "    model = model.merge_and_unload()\n",
    "    print(model)\n",
    "\n",
    "    #print(f\"Successfully loaded the model {model_name} into memory\")\n",
    "    model.save_pretrained(f\"{model_path}/merge_16bit_v3\", safe_serialization=True)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "    # Delete the model object if it exists\n",
    "    if 'model' in locals():\n",
    "        del model\n",
    "\n",
    "    # Clear the GPU cache\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Run the garbage collection\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"Model, GPU cache, and garbage have been cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8204fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_fineweb2-cs_finewebedu-en_31_500k_seed42_samples500000-full_mix_11_cs_en/merge_16bit_v3/tokenizer_config.json',\n",
       " '/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_fineweb2-cs_finewebedu-en_31_500k_seed42_samples500000-full_mix_11_cs_en/merge_16bit_v3/special_tokens_map.json',\n",
       " '/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_fineweb2-cs_finewebedu-en_31_500k_seed42_samples500000-full_mix_11_cs_en/merge_16bit_v3/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(f\"{model_path}/final\")\n",
    "tokenizer.save_pretrained(f\"{model_path}/merge_16bit_v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1632dfe2",
   "metadata": {},
   "source": [
    "## 16 bit merge v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "253bb9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "model_name = model_path.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e27447cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7dcb8afe8a4d0b9f9f17a8300329f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_path, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "model = PeftModel.from_pretrained(base_model, f\"{model_path}/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbcbda03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "804aee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"{model_path}/merge_16bit_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a03d452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_cs_fineweb2_seed42_neptune_bs128_samples500000-full_cs_instruction_tuning_collection/merge_16bit_v2/tokenizer_config.json',\n",
       " '/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_cs_fineweb2_seed42_neptune_bs128_samples500000-full_cs_instruction_tuning_collection/merge_16bit_v2/special_tokens_map.json',\n",
       " '/mnt/personal/mlynatom/thesis_models/it-cp_Llama-3.1-8B-full_cs_fineweb2_seed42_neptune_bs128_samples500000-full_cs_instruction_tuning_collection/merge_16bit_v2/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(f\"{model_path}/final\")\n",
    "tokenizer.save_pretrained(f\"{model_path}/merge_16bit_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff8988d",
   "metadata": {},
   "source": [
    "## 16 bit merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ed78b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.4.7: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.254 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19894fdd07a48a88b4e03196ccf0e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.4.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 1024 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = torch.bfloat16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "\n",
    "model_name = model_path.split(\"/\")[-1]\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = f\"{model_path}/final\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb196941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 367.2 out of 503.53 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:00<00:00, 63.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Merge to 16bit - local\n",
    "model.save_pretrained_merged(f\"{model_path}/merge_16bit\", tokenizer, save_method = \"merged_16bit\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aad156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #merge # to 16bit - push to hub\n",
    "# model.push_to_hub_merged(f\"aic/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339e087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge to 4bit - local\n",
    "# model.save_pretrained_merged(f\"{model_path}/merge_4bit\", tokenizer, save_method = \"merged_4bit\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385b9cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge to 4bit - push to hub\n",
    "#model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae6d1bb",
   "metadata": {},
   "source": [
    "## AWQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d58bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install autoawq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef662ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = f\"{model_path}/merge_16bit\"\n",
    "quant_path = f\"{model_path}/awq\"\n",
    "quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n",
    "\n",
    "# Load model\n",
    "model = AutoAWQForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "# Quantize\n",
    "model.quantize(tokenizer, quant_config=quant_config)\n",
    "\n",
    "# Save quantized model\n",
    "model.save_quantized(quant_path)\n",
    "tokenizer.save_pretrained(quant_path)\n",
    "\n",
    "print(f'Model is quantized and saved at \"{quant_path}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36b8515",
   "metadata": {},
   "source": [
    "## Eval Perplexity differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "362ee75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import os\n",
    "class Perplexity:\n",
    "    def __init__(self, model, tokenizer, device=None)->None:\n",
    "        # self.model_id = model_id\n",
    "\n",
    "        # #check device\n",
    "        if device is not None:\n",
    "            assert device in [\"gpu\", \"cpu\", \"cuda\"], \"device should be either gpu or cpu.\"\n",
    "            if device == \"gpu\":\n",
    "                device = \"cuda\"\n",
    "        else:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        # # #load model and move to desired device\n",
    "        # # if load_in_16bit:\n",
    "        # #     \n",
    "        # # else:\n",
    "        # #     self.model = AutoModelForCausalLM.from_pretrained(self.model_id)\n",
    "\n",
    "        # #self.model = AutoModelForCausalLM.from_pretrained(self.model_id, torch_dtype=torch.bfloat16)\n",
    "        # print(load_in_4bit, \"load_in_4bit\")\n",
    "\n",
    "        # self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
    "        #     model_name = model_id, # YOUR MODEL YOU USED FOR TRAINING\n",
    "        #     max_seq_length = 1024,\n",
    "        #     dtype = torch.bfloat16,\n",
    "        #     load_in_4bit = load_in_4bit,\n",
    "        # )\n",
    "        # FastLanguageModel.for_inference(self.model)\n",
    "\n",
    "\n",
    "        # #self.model.to(device)\n",
    "\n",
    "\n",
    "        # # #load tokenizer\n",
    "        # #self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def compute(self, predictions, batch_size: int = 16, add_start_token: bool = True, max_length=None):\n",
    "        # if batch_size > 1 (which generally leads to padding being required), and\n",
    "        # if there is not an already assigned pad_token, assign an existing\n",
    "        # special token to also be the padding token\n",
    "        if self.tokenizer.pad_token is None and batch_size > 1:\n",
    "            existing_special_tokens = list(self.tokenizer.special_tokens_map_extended.values())\n",
    "            # check that the model already has at least one special token defined\n",
    "            assert (\n",
    "                len(existing_special_tokens) > 0\n",
    "            ), \"If batch_size > 1, model must have at least one special token to use for padding. Please use a different model or set batch_size=1.\"\n",
    "            # assign one of the special tokens to also be the pad token\n",
    "            self.tokenizer.add_special_tokens({\"pad_token\": existing_special_tokens[0]})\n",
    "\n",
    "        if add_start_token and max_length:\n",
    "            # leave room for <BOS> token to be added:\n",
    "            assert (\n",
    "                self.tokenizer.bos_token is not None\n",
    "            ), \"Input model must already have a BOS token if using add_start_token=True. Please use a different model, or set add_start_token=False\"\n",
    "            max_tokenized_len = max_length - 1\n",
    "        else:\n",
    "            max_tokenized_len = max_length\n",
    "\n",
    "        ppls = []\n",
    "        loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "        for start_index in tqdm(range(0, len(predictions), batch_size)):\n",
    "            end_index = min(start_index + batch_size, len(predictions))\n",
    "\n",
    "            #compute encodings\n",
    "            encodings = self.tokenizer(\n",
    "                predictions[start_index:end_index],\n",
    "                add_special_tokens=False,\n",
    "                padding=True,\n",
    "                truncation=True if max_tokenized_len else False,\n",
    "                max_length=max_tokenized_len,\n",
    "                return_tensors=\"pt\",\n",
    "                return_attention_mask=True,\n",
    "            )\n",
    "\n",
    "            encoded_batch = encodings[\"input_ids\"]\n",
    "            attn_mask = encodings[\"attention_mask\"]\n",
    "\n",
    "\n",
    "            # check that each input is long enough:\n",
    "            if add_start_token:\n",
    "                assert torch.all(torch.ge(attn_mask.sum(1), 1)), \"Each input text must be at least one token long.\"\n",
    "            else:\n",
    "                assert torch.all(\n",
    "                    torch.ge(attn_mask.sum(1), 2)\n",
    "                ), \"When add_start_token=False, each input text must be at least two tokens long. Run with add_start_token=True if inputting strings of only one token, and remove all empty input strings.\"\n",
    "\n",
    "            if add_start_token:\n",
    "                bos_tokens_tensor = torch.tensor([[self.tokenizer.bos_token_id]] * encoded_batch.size(dim=0))\n",
    "                encoded_batch = torch.cat([bos_tokens_tensor, encoded_batch], dim=1)\n",
    "                attn_mask = torch.cat([torch.ones(bos_tokens_tensor.size(), dtype=torch.int64), attn_mask], dim=1)\n",
    "\n",
    "\n",
    "            #now move to gpu\n",
    "            encoded_batch = encoded_batch.to(self.device)\n",
    "            attn_mask = attn_mask.to(self.device)\n",
    "\n",
    "            labels = encoded_batch\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out_logits = self.model(encoded_batch, attention_mask=attn_mask).logits\n",
    "\n",
    "            shift_logits = out_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            shift_attention_mask_batch = attn_mask[..., 1:].contiguous()\n",
    "\n",
    "            perplexity_batch = torch.exp(\n",
    "                (loss_fct(shift_logits.transpose(1, 2), shift_labels) * shift_attention_mask_batch).sum(1)\n",
    "                / shift_attention_mask_batch.sum(1)\n",
    "            )\n",
    "\n",
    "            ppls += perplexity_batch.tolist()\n",
    "\n",
    "        return {\"perplexities\": ppls, \"mean_perplexity\": np.mean(ppls)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e976655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed605e3e5c945b6acd841b91fe40a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"HuggingFaceFW/fineweb-2\", \"ces_Latn\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8df14e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f2540ec2284887a7e899dcd8c3dd58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:15<00:00,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'perplexities': [9.3125, 4.875, 10.75, 9.3125, 10.125, 8.25, 8.625, 9.1875, 9.1875, 11.8125, 6.53125, 12.9375, 9.9375, 9.5, 9.8125, 5.9375, 16.875, 9.3125, 17.125, 7.625, 10.9375, 10.5625, 10.9375, 6.125, 6.0625, 14.25, 6.625, 11.625, 11.0625, 12.5625, 24.625, 7.5, 27.5, 23.875, 9.625, 9.9375, 7.375, 7.0625, 7.5, 7.15625, 13.1875, 9.9375, 8.5, 16.625, 5.09375, 9.0625, 11.4375, 17.5, 10.9375, 135.0, 13.8125, 6.84375, 9.1875, 7.34375, 24.25, 11.4375, 8.625, 14.4375, 18.25, 8.75, 6.53125, 12.0, 15.1875, 7.0625, 8.25, 9.1875, 11.4375, 9.9375, 9.5, 11.625, 15.625, 11.625, 6.21875, 12.1875, 14.0, 47.5, 20.125, 12.0, 24.625, 10.75, 4.71875, 10.25, 11.625, 6.9375, 10.75, 7.75, 8.25, 11.8125, 17.5, 7.625, 13.1875, 11.25, 9.3125, 8.9375, 14.0, 8.125, 9.5, 17.5, 6.1875, 7.28125], 'mean_perplexity': 12.6803125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(f\"{model_path}/merge_16bit_v3\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"{model_path}/merge_16bit_v3\", trust_remote_code=True)\n",
    "\n",
    "perplexity_evaluator = Perplexity(model, tokenizer)\n",
    "result = perplexity_evaluator.compute(dataset[\"text\"][:100], 4, True, 1024)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deb482ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "503af41f6d9849feb983dd45224b7b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:15<00:00,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'perplexities': [9.3125, 4.875, 10.75, 9.3125, 10.125, 8.25, 8.625, 9.1875, 9.1875, 11.8125, 6.53125, 12.9375, 9.9375, 9.5, 9.8125, 5.9375, 16.875, 9.3125, 17.125, 7.625, 10.9375, 10.5625, 10.9375, 6.125, 6.0625, 14.25, 6.625, 11.625, 11.0625, 12.5625, 24.625, 7.5, 27.5, 23.875, 9.625, 9.9375, 7.375, 7.0625, 7.5, 7.15625, 13.1875, 9.9375, 8.5, 16.625, 5.09375, 9.0625, 11.4375, 17.5, 10.9375, 135.0, 13.8125, 6.84375, 9.1875, 7.34375, 24.25, 11.4375, 8.625, 14.4375, 18.25, 8.75, 6.53125, 12.0, 15.1875, 7.0625, 8.25, 9.1875, 11.4375, 9.9375, 9.5, 11.625, 15.625, 11.625, 6.21875, 12.1875, 14.0, 47.5, 20.125, 12.0, 24.625, 10.75, 4.71875, 10.25, 11.625, 6.9375, 10.75, 7.75, 8.25, 11.8125, 17.5, 7.625, 13.1875, 11.25, 9.3125, 8.9375, 14.0, 8.125, 9.5, 17.5, 6.1875, 7.28125], 'mean_perplexity': 12.6803125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(f\"{model_path}/merge_16bit_v2\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"{model_path}/merge_16bit_v2\")\n",
    "perplexity_evaluator = Perplexity(model, tokenizer)\n",
    "result = perplexity_evaluator.compute(dataset[\"text\"][:100], 4, True, 1024)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d1ad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(f\"{model_path}/merge_16bit\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"{model_path}/merge_16bit\")\n",
    "perplexity_evaluator = Perplexity(model, tokenizer)\n",
    "result = perplexity_evaluator.compute(dataset[\"text\"][:100], 4, True, 1024)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57f006d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.7: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.254 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2ff8be5d8844cdbc8efef60d669341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.4.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:20<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'perplexities': [9.3125, 4.8125, 10.9375, 9.3125, 10.125, 8.125, 8.5, 8.375, 9.1875, 11.625, 6.3125, 12.9375, 9.5, 9.3125, 9.8125, 5.96875, 16.125, 9.8125, 16.875, 7.625, 11.0625, 9.9375, 10.4375, 6.1875, 6.0625, 14.25, 6.5625, 11.25, 11.0625, 12.375, 23.875, 7.5, 26.625, 23.125, 9.625, 9.9375, 7.375, 7.0625, 7.28125, 7.0, 13.1875, 9.9375, 8.25, 16.125, 5.15625, 8.0, 10.9375, 17.125, 10.9375, 123.0, 13.1875, 6.28125, 9.0625, 7.21875, 22.75, 11.625, 8.625, 13.8125, 18.0, 8.5, 6.71875, 11.8125, 15.1875, 7.0625, 8.375, 9.0625, 11.25, 9.3125, 9.625, 11.8125, 14.6875, 11.625, 6.40625, 12.1875, 14.0, 46.75, 20.375, 12.375, 22.75, 10.5625, 4.71875, 9.9375, 11.4375, 6.125, 10.25, 7.5, 8.25, 11.8125, 16.875, 7.75, 13.1875, 10.4375, 9.3125, 8.625, 14.0, 8.125, 9.3125, 17.5, 6.1875, 7.21875], 'mean_perplexity': 12.3540625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name = f\"{model_path}/final\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "            max_seq_length = 1024,\n",
    "            dtype = torch.bfloat16,\n",
    "            load_in_4bit = False,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "perplexity_evaluator = Perplexity(model, tokenizer)\n",
    "result = perplexity_evaluator.compute(dataset[\"text\"][:100], 4, True, 1024)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07c179d",
   "metadata": {},
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b6e75ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b7525718014e409dbfc4b6610788bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299803102e164d9ca51c35a7d879f544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:14<00:00,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'perplexities': [8.25, 4.4375, 9.8125, 8.5, 8.5, 7.375, 7.75, 6.6875, 7.625, 9.8125, 5.75, 12.5625, 8.0, 8.5, 8.25, 5.5, 14.25, 7.625, 14.9375, 6.84375, 9.5, 5.25, 9.5, 5.53125, 5.5625, 12.1875, 5.84375, 9.8125, 9.9375, 10.9375, 20.375, 6.78125, 25.0, 18.25, 8.25, 9.1875, 6.71875, 6.5625, 6.3125, 6.0625, 10.9375, 9.1875, 6.78125, 13.8125, 4.625, 7.375, 9.625, 14.9375, 9.5, 105.0, 10.5625, 5.40625, 8.125, 6.53125, 18.25, 10.25, 7.625, 11.4375, 16.625, 6.03125, 5.84375, 11.0625, 13.375, 6.0625, 7.625, 7.875, 9.3125, 8.5, 8.5, 10.75, 12.375, 10.5625, 5.53125, 10.9375, 12.0, 18.0, 16.375, 10.75, 18.25, 9.8125, 4.09375, 9.0625, 10.4375, 5.5625, 7.5, 6.875, 7.34375, 10.25, 13.5625, 6.46875, 10.4375, 8.25, 8.25, 7.625, 12.375, 7.375, 8.0, 12.9375, 5.40625, 6.28125], 'mean_perplexity': 10.4275}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"HuggingFaceFW/fineweb-2\", \"ces_Latn\", split=\"test\")\n",
    "\n",
    "perplexity_evaluator = Perplexity(model_id=\"/mnt/personal/mlynatom/thesis_models/cp_Llama-3.1-8B-full_fineweb2-cs_finewebedu-en_31_500k_seed42_samples500000/merge_16bit\", load_in_4bit=False)\n",
    "result = perplexity_evaluator.compute(dataset[\"text\"][:100], 4, True, 1024)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ff4bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d246880224764ab981c0fff65105feec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False load_in_4bit\n",
      "==((====))==  Unsloth 2025.4.7: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.254 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc495d2a1ee42ba88035e4c1b4d9dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.4.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:21<00:00,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'perplexities': [8.25, 4.4375, 9.9375, 8.5, 8.5, 7.375, 7.75, 6.5625, 7.625, 9.9375, 5.75, 12.5625, 8.0, 8.5, 8.25, 5.53125, 14.25, 8.0, 15.375, 6.84375, 9.625, 5.25, 9.5, 5.53125, 5.53125, 12.375, 5.84375, 9.8125, 9.9375, 10.9375, 20.375, 6.78125, 25.0, 18.625, 8.25, 9.1875, 6.6875, 6.5625, 6.28125, 6.125, 10.9375, 9.1875, 6.78125, 13.8125, 4.59375, 7.375, 9.625, 14.9375, 9.5, 112.0, 10.9375, 5.4375, 8.25, 6.40625, 18.875, 10.125, 7.625, 11.4375, 16.625, 6.0625, 5.8125, 11.0625, 13.1875, 6.0625, 7.625, 7.875, 9.3125, 8.5, 8.625, 10.75, 12.5625, 10.5625, 5.53125, 10.9375, 12.375, 18.0, 16.375, 11.0625, 18.25, 9.8125, 4.09375, 9.1875, 10.4375, 5.5625, 7.5, 7.0625, 7.34375, 10.25, 13.5625, 6.46875, 10.5625, 8.375, 8.25, 7.625, 12.375, 7.34375, 8.0, 13.8125, 5.40625, 6.28125], 'mean_perplexity': 10.5446875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"HuggingFaceFW/fineweb-2\", \"ces_Latn\", split=\"test\")\n",
    "\n",
    "perplexity_evaluator = Perplexity(model_id=\"/mnt/personal/mlynatom/thesis_models/cp_Llama-3.1-8B-full_fineweb2-cs_finewebedu-en_31_500k_seed42_samples500000/final\", load_in_4bit=False)\n",
    "result = perplexity_evaluator.compute(dataset[\"text\"][:100], 4, True, 1024)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5007b3d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f888fbddb9646249d6868d995e9c313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "model = PeftModel.from_pretrained(base_model, \"/mnt/personal/mlynatom/thesis_models/cp_Llama-3.1-8B-full_fineweb2-cs_finewebedu-en_31_500k_seed42_samples500000/final\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/mnt/personal/mlynatom/thesis_models/cp_Llama-3.1-8B-full_fineweb2-cs_finewebedu-en_31_500k_seed42_samples500000/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbc69659",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6c3bbf4ca74d70b891196b52f5b144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:26<00:00,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'perplexities': [8.25, 4.375, 9.625, 8.5, 8.5, 7.15625, 7.625, 6.125, 7.625, 9.0625, 5.65625, 12.1875, 7.75, 8.375, 8.25, 5.5, 13.8125, 7.0, 14.6875, 6.84375, 9.1875, 5.125, 9.3125, 5.53125, 5.5625, 12.1875, 5.75, 9.625, 9.625, 10.4375, 19.5, 6.78125, 23.875, 17.125, 8.25, 9.1875, 6.71875, 6.46875, 6.125, 5.96875, 10.4375, 9.0625, 6.625, 13.375, 4.53125, 6.84375, 9.5, 14.9375, 9.5, 96.0, 9.9375, 4.71875, 7.875, 6.28125, 17.75, 10.25, 7.625, 10.75, 15.875, 5.65625, 5.3125, 11.0625, 13.375, 5.5625, 7.375, 7.875, 9.1875, 8.375, 8.5, 10.25, 12.1875, 10.4375, 5.53125, 10.9375, 11.0625, 18.0, 16.375, 10.4375, 16.875, 9.5, 4.0625, 8.75, 10.125, 5.5, 7.09375, 6.71875, 7.34375, 10.25, 12.75, 5.96875, 10.4375, 7.625, 8.25, 7.375, 12.375, 7.375, 7.75, 11.8125, 5.40625, 6.1875], 'mean_perplexity': 10.0815625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"HuggingFaceFW/fineweb-2\", \"ces_Latn\", split=\"test\")\n",
    "\n",
    "perplexity_evaluator = Perplexity(model, tokenizer)\n",
    "result = perplexity_evaluator.compute(dataset[\"text\"][:100], 4, True, 1024)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc307186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3472a8ef3cc49baa64373542bb88b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:15<00:00,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'perplexities': [8.25, 4.4375, 9.625, 8.5, 8.5, 7.15625, 7.625, 6.125, 7.625, 8.9375, 5.65625, 12.1875, 7.75, 8.375, 8.25, 5.5, 13.5625, 7.0, 14.6875, 6.84375, 9.1875, 5.125, 9.3125, 5.53125, 5.5625, 12.1875, 5.75, 9.625, 9.5, 10.4375, 19.5, 6.78125, 23.875, 17.125, 8.25, 9.1875, 6.71875, 6.46875, 6.125, 5.9375, 10.4375, 9.1875, 6.625, 13.375, 4.53125, 6.9375, 9.5, 14.9375, 9.5, 96.0, 9.9375, 4.6875, 7.875, 6.28125, 17.125, 10.25, 7.625, 10.75, 15.875, 5.65625, 5.3125, 11.0625, 13.375, 5.65625, 7.375, 7.875, 9.1875, 8.25, 8.5, 10.25, 12.1875, 10.4375, 5.53125, 10.9375, 10.9375, 18.0, 16.375, 10.4375, 16.875, 9.5, 4.09375, 8.75, 10.125, 5.4375, 7.15625, 6.71875, 7.34375, 10.25, 12.75, 5.96875, 10.4375, 7.625, 8.25, 7.375, 12.375, 7.375, 7.75, 12.0, 5.40625, 6.0625], 'mean_perplexity': 10.071875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "merged_model =  model.merge_and_unload()\n",
    "dataset = load_dataset(\"HuggingFaceFW/fineweb-2\", \"ces_Latn\", split=\"test\")\n",
    "\n",
    "perplexity_evaluator = Perplexity(merged_model, tokenizer)\n",
    "result = perplexity_evaluator.compute(dataset[\"text\"][:100], 4, True, 1024)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de8234e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca1c133337a4916afbfefe562fab2c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6977edf7374ce3bf22acb932ed24fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:15<00:00,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'perplexities': [8.25, 4.4375, 9.625, 8.5, 8.5, 7.15625, 7.625, 6.125, 7.625, 8.9375, 5.65625, 12.1875, 7.75, 8.375, 8.25, 5.5, 13.5625, 7.0, 14.6875, 6.84375, 9.1875, 5.125, 9.3125, 5.53125, 5.5625, 12.1875, 5.75, 9.625, 9.5, 10.4375, 19.5, 6.78125, 23.875, 17.125, 8.25, 9.1875, 6.71875, 6.46875, 6.125, 5.9375, 10.4375, 9.1875, 6.625, 13.375, 4.53125, 6.9375, 9.5, 14.9375, 9.5, 96.0, 9.9375, 4.6875, 7.875, 6.28125, 17.125, 10.25, 7.625, 10.75, 15.875, 5.65625, 5.3125, 11.0625, 13.375, 5.65625, 7.375, 7.875, 9.1875, 8.25, 8.5, 10.25, 12.1875, 10.4375, 5.53125, 10.9375, 10.9375, 18.0, 16.375, 10.4375, 16.875, 9.5, 4.09375, 8.75, 10.125, 5.4375, 7.15625, 6.71875, 7.34375, 10.25, 12.75, 5.96875, 10.4375, 7.625, 8.25, 7.375, 12.375, 7.375, 7.75, 12.0, 5.40625, 6.0625], 'mean_perplexity': 10.071875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
